{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from random import choice, seed, randint, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, CuDNNGRU as GRU, CuDNNLSTM as LSTM, Dropout, BatchNormalization\n",
    "from keras.layers import Dense, Concatenate, Activation, Embedding, SpatialDropout1D, Bidirectional, Lambda, Conv1D\n",
    "from keras.layers import Add, Average\n",
    "from keras.optimizers import Nadam, Adam, Adamax\n",
    "from keras.activations import absolute_import\n",
    "from keras.legacy import interfaces\n",
    "# from keras.preprocessing.sequence import pad_sequaencesget_\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold as KF\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from keras_bert.loader import load_trained_model_from_checkpoint\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.objectives import sparse_categorical_crossentropy\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from special_tokens import CHINESE_MAP\n",
    "from metric_utils import compute_f1, compute_exact\n",
    "from collections import OrderedDict, Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = \"../../../chinese_bert/NEZHA_large/NEZHA-Large/\"\n",
    "TRN_FILENAME = \"../data/train_20200228.csv\"\n",
    "DEV_FILENAME = \"../data/dev_20200228.csv\"\n",
    "SAVE_DIR = \"../save_models/\"\n",
    "PREFIX = \"nezha_v13_augm\"\n",
    "if \"large-clue\" in BERT_PRETRAINED_DIR or \"large-pair\" in BERT_PRETRAINED_DIR:\n",
    "    W2V_FILE = \"./word_embedding_matrix_v2\"\n",
    "else:\n",
    "    W2V_FILE = \"./word_embedding_matrix\"\n",
    "MAX_EPOCH = 15\n",
    "RUN_EPOCH = 10\n",
    "MAX_LEN = 60\n",
    "MAX_DOC_LEN = MAX_LEN // 2\n",
    "THRE = 0.5\n",
    "B_SIZE = 32\n",
    "ACCUM_STEP = int(32 // B_SIZE)\n",
    "FOLD_ID = list(range(5, 10))\n",
    "FOLD_NUM = 25\n",
    "SEED = 2020\n",
    "\n",
    "SHUFFLE = True\n",
    "DOC_STRIDE = 128\n",
    "cfg = {}\n",
    "\n",
    "cfg[\"base_dir\"] = BERT_PRETRAINED_DIR\n",
    "cfg[\"span_mode\"] = True\n",
    "cfg[\"lr\"] = 6e-6\n",
    "cfg['min_lr'] = 6e-8 \n",
    "cfg[\"ch_type\"] = \"tx_ft\"\n",
    "cfg[\"trainable\"] = True\n",
    "cfg[\"bert_trainable\"] = True\n",
    "cfg[\"accum_step\"] = ACCUM_STEP\n",
    "cfg[\"cls_num\"] = 4\n",
    "cfg[\"unit1\"] = 128\n",
    "cfg[\"unit2\"] = 128\n",
    "cfg[\"unit3\"] = 512\n",
    "cfg[\"conv_num\"] = 128\n",
    "cfg['maxlen'] = MAX_LEN\n",
    "cfg[\"adv_training\"] = True\n",
    "cfg[\"W2V_FILE\"] = W2V_FILE\n",
    "cfg[\"use_embed\"] = True\n",
    "cfg[\"use_embed_v2\"] = True\n",
    "PREFIX += \"_seed\" + str(SEED)\n",
    "cfg[\"verbose\"] = PREFIX\n",
    "PREFIX = PREFIX + \"_embed_v2\" if cfg[\"use_embed_v2\"] else PREFIX\n",
    "\n",
    "train_data = pd.read_csv(TRN_FILENAME)\n",
    "train_data.fillna(\"\", inplace=True)\n",
    "dev_data = pd.read_csv(DEV_FILENAME)\n",
    "dev_data.fillna(\"\", inplace=True)\n",
    "all_data = pd.concat([train_data, dev_data], axis=0, ignore_index=True)\n",
    "\n",
    "def get_data(df_data):\n",
    "\n",
    "    df_gb = df_data.groupby('query1')\n",
    "    res = {}\n",
    "    for index, data in df_gb:\n",
    "        query2s = data[\"query2\"]\n",
    "        lables = data[\"label\"]\n",
    "        ele = {}\n",
    "        pos_qs = []\n",
    "        neg_qs = []\n",
    "        for q, lable in zip(query2s, lables):\n",
    "            if lable == 1:\n",
    "                pos_qs.append(q)\n",
    "            elif lable == 0:\n",
    "                neg_qs.append(q)\n",
    "            else:\n",
    "                print(\"wrong data\", index, q, lable)\n",
    "        ele[\"pos\"] = pos_qs\n",
    "        ele[\"neg\"] = neg_qs\n",
    "        res[index] = ele\n",
    "    return res\n",
    "\n",
    "# train_data_dict = get_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1} {0: '0', 1: '1'} ['0', '1'] 21059 nezha_v13_augm_seed2020_embed_v2_tx_ft_25_6e-06_adv_training_embed_v2\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(base_dir=BERT_PRETRAINED_DIR, albert=False):\n",
    "    if albert or \"albert\"in cfg[\"verbose\"].lower():\n",
    "        dict_path = os.path.join(base_dir, 'vocab_chinese.txt')\n",
    "    else:\n",
    "        dict_path = os.path.join(base_dir, 'vocab.txt')\n",
    "    with open(dict_path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "\n",
    "    word_index = {v: k  for k, v in enumerate(lines)}\n",
    "    for k, v in CHINESE_MAP.items():\n",
    "        assert v in word_index\n",
    "        if k in word_index:\n",
    "            print(\"[!] CHINESE_MAP k = {} is in word_index, DON'T using `{}` to replace\".format(k, v))\n",
    "            continue\n",
    "        # word_index[k] = word_index[v]\n",
    "        del word_index[v]\n",
    "    return word_index\n",
    "\n",
    "\n",
    "def get_label():\n",
    "    labels = [\"0\", \"1\"]\n",
    "    label2id = {k: v for v, k in enumerate(labels)}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    return label2id, id2label, labels\n",
    "    \n",
    "    \n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=np.float16)\n",
    "\n",
    "\n",
    "def load_embed(path, dim=300, word_index=None):\n",
    "    embedding_index = {}\n",
    "    with open(path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.strip().split()\n",
    "            word, arr = l[0], l[1:]\n",
    "            if len(arr) != dim:\n",
    "                print(\"[!] l = {}\".format(l))\n",
    "                continue\n",
    "            if word_index and word not in word_index:\n",
    "                continue\n",
    "            word, arr = get_coefs(word, arr)\n",
    "            embedding_index[word] = arr\n",
    "    return embedding_index\n",
    "\n",
    "\n",
    "def build_matrix(path, word_index=None, max_features=None, dim=300):\n",
    "    embedding_index = load_embed(path, dim=dim, word_index=word_index)\n",
    "    max_features = len(word_index) + 1 if max_features is None else max_features \n",
    "    embedding_matrix = np.zeros((max_features + 1, dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_word_embed(word_embed_f1=\"../../../chinese_embedding/Tencent_AILab_ChineseEmbedding.txt\", \n",
    "               word_embed_f2=\"../../../chinese_embedding/cc.zh.300.vec\", \n",
    "               save_filename=W2V_FILE,\n",
    "               word_index=None):\n",
    "    if os.path.exists(save_filename + \".npy\"):\n",
    "        word_embedding_matrix = np.load(save_filename + \".npy\").astype(\"float32\")\n",
    "    else:\n",
    "        if \"tx\" in cfg[\"ch_type\"]:\n",
    "            tx_embed, tx_unk = build_matrix(word_embed_f1, word_index=word_index, dim=200)\n",
    "        else:\n",
    "            tx_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            tx_unk = []\n",
    "        if \"ft\" in cfg[\"ch_type\"]:\n",
    "            ft_embed, ft_unk = build_matrix(word_embed_f2, word_index=word_index, dim=300)\n",
    "        else:\n",
    "            ft_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            ft_unk = []    \n",
    "\n",
    "        word_embedding_matrix = np.concatenate([tx_embed, ft_embed], axis=-1).astype(\"float32\")\n",
    "        print(word_embedding_matrix.shape, len(tx_unk), len(ft_unk))\n",
    "        np.save(save_filename, word_embedding_matrix )\n",
    "    return word_embedding_matrix\n",
    "    \n",
    "    \n",
    "word_index = get_vocab()\n",
    "label2id, id2label, labels = get_label()\n",
    "word_embedding_matrix = load_word_embed(word_index=word_index)\n",
    "\n",
    "NUM_CLASS = len(label2id)\n",
    "cfg[\"x_pad\"] = word_index[\"[PAD]\"]\n",
    "cfg[\"num_class\"] = NUM_CLASS\n",
    "cfg[\"filename\"] = \"{}_{}_{}_{}\".format(PREFIX, cfg[\"ch_type\"], FOLD_NUM, cfg[\"lr\"])\n",
    "cfg[\"filename\"] = cfg[\"filename\"] + \"_adv_training\" if cfg[\"adv_training\"] else cfg[\"filename\"]\n",
    "cfg[\"filename\"] = cfg[\"filename\"] + \"_embed\" if cfg[\"use_embed\"] else cfg[\"filename\"]\n",
    "cfg[\"filename\"] = cfg[\"filename\"] + \"_v2\" if cfg[\"use_embed_v2\"]and cfg[\"use_embed\"] else cfg[\"filename\"]\n",
    "print(label2id, id2label, labels, len(word_index), cfg[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(cfg, summary=False, word_embedding_matrix=None, bert_summary=False):\n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        if \"albert\"in cfg[\"verbose\"].lower():\n",
    "            from bert4keras.bert import build_bert_model\n",
    "            config_file = os.path.join(base_dir, 'albert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'model.ckpt-best')\n",
    "            model = build_bert_model(\n",
    "                    config_path=config_file,\n",
    "                    checkpoint_path=checkpoint_file,\n",
    "                    model='albert',\n",
    "                    return_keras_model=True\n",
    "            )\n",
    "            if cfg_[\"cls_num\"] > 1:\n",
    "                output = Concatenate(axis=-1)([model.get_layer(\"Encoder-1-FeedForward-Norm\").get_output_at(-i) for i in range(1, cfg[\"cls_num\"] + 1)])\n",
    "                model = Model(model.inputs[: 2], outputs=output)\n",
    "            model.trainable = cfg_[\"bert_trainable\"]\n",
    "        elif \"nezha_wwm\"in cfg[\"verbose\"].lower():\n",
    "            from bert4keras.bert import build_bert_model\n",
    "            config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'model.ckpt-346400')\n",
    "            model = build_bert_model(\n",
    "                    config_path=config_file,\n",
    "                    checkpoint_path=checkpoint_file,\n",
    "                    model='nezha',\n",
    "                    return_keras_model=True\n",
    "            )\n",
    "            if bert_summary:\n",
    "                model.summary()            \n",
    "            if cfg_[\"cls_num\"] > 1:\n",
    "                output = Concatenate(axis=-1)([\n",
    "                    model.get_layer(\"Encoder-{}-FeedForward-Norm\".format(24 - i)).output \n",
    "                    for i in range(0, cfg[\"cls_num\"])])\n",
    "        \n",
    "                model = Model(model.inputs[: 2], outputs=output)\n",
    "                model = Model(model.inputs[: 2], outputs=output)\n",
    "            model.trainable = cfg_[\"bert_trainable\"]            \n",
    "        elif \"nezha\"in cfg[\"verbose\"].lower():\n",
    "            from bert4keras.bert import build_bert_model\n",
    "            config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'model.ckpt-325810')\n",
    "            model = build_bert_model(\n",
    "                    config_path=config_file,\n",
    "                    checkpoint_path=checkpoint_file,\n",
    "                    model='nezha',\n",
    "                    return_keras_model=True, \n",
    "            )\n",
    "            if bert_summary:\n",
    "                model.summary()\n",
    "    \n",
    "            if cfg_[\"cls_num\"] > 1:\n",
    "                output = Concatenate(axis=-1)(\n",
    "                    [model.get_layer(\"Encoder-{}-FeedForward-Norm\".format(24 - i)).output \n",
    "                     for i in range(0, cfg[\"cls_num\"])])\n",
    "                model = Model(model.inputs[: 2], outputs=output)\n",
    "            model.trainable = cfg_[\"bert_trainable\"]             \n",
    "        else:\n",
    "            config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "            if not os.path.exists(config_file):\n",
    "                config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "                checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')            \n",
    "            model = load_trained_model_from_checkpoint(config_file, \n",
    "                                                       checkpoint_file, \n",
    "                                                       training=False, \n",
    "                                                       trainable=cfg_[\"bert_trainable\"], \n",
    "                                                       output_layer_num=cfg_[\"cls_num\"],\n",
    "                                                       seq_len=cfg_['maxlen'])\n",
    "            \n",
    "            # model = Model(inputs=model.inputs[: 2], outputs=model.layers[-7].output)\n",
    "        print(config_file, checkpoint_file)\n",
    "        return model\n",
    "    \n",
    "    def _get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        total_steps, warmup_steps = calc_train_steps(\n",
    "            num_example=num_example,\n",
    "            batch_size=B_SIZE,\n",
    "            epochs=MAX_EPOCH,\n",
    "            warmup_proportion=warmup_proportion,\n",
    "        )\n",
    "        opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "        if cfg.get(\"accum_step\", None) and cfg[\"accum_step\"] > 1:\n",
    "            print(\"[!] using accum_step = {}\".format(cfg[\"accum_step\"]))\n",
    "            from accum_optimizer import AccumOptimizer\n",
    "            opt = AccumOptimizer(opt, steps_per_update=cfg[\"accum_step\"])\n",
    "        \n",
    "        return opt\n",
    "\n",
    "    bert_model = _get_model(cfg[\"base_dir\"], cfg)\n",
    "\n",
    "    if word_embedding_matrix is not None:\n",
    "        embed = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                          output_dim=word_embedding_matrix.shape[1],\n",
    "                          weights=[word_embedding_matrix],\n",
    "                          trainable=cfg[\"trainable\"],\n",
    "                          name=\"char_embed\"\n",
    "                         )\n",
    "        \n",
    "    t1_in = Input(shape=(None, ))\n",
    "    t2_in = Input(shape=(None, ))\n",
    "    o1_in = Input(shape=(1, ))\n",
    "    o2_in = Input(shape=(1, ))\n",
    "\n",
    "    t1, t2, o1, o2 = t1_in, t2_in, o1_in, o2_in\n",
    "    \n",
    "    t = bert_model([t1, t2])\n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'))(t1)\n",
    "    ## Char information\n",
    "    if word_embedding_matrix is not None:\n",
    "        word_embed = embed(t1)\n",
    "        if cfg.get(\"use_embed_v2\", False):\n",
    "            _t2 = Lambda(lambda x: K.expand_dims(x, axis=-1))(t2)\n",
    "            word_embed = Concatenate(axis=-1)([word_embed, _t2])\n",
    "        word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "        word_embed = Bidirectional(LSTM(cfg[\"unit1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "        word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "        t = Concatenate(axis=-1)([t, word_embed])\n",
    "    \n",
    "    t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask])     \n",
    "    t = Bidirectional(LSTM(cfg[\"unit3\"], return_sequences=True), merge_mode=\"concat\")(t)\n",
    "    # t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask]) \n",
    "    # t = Conv1D(cfg[\"conv_num\"], kernel_size=3, padding=\"same\")(t) \n",
    "    t = Lambda(lambda x: x[:, 0, :], name=\"extract_layer\")(t)\n",
    "    if cfg.get(\"num_class\", 1) == 2:\n",
    "        po1_logit = Dense(1, name=\"po1_logit\")(t)\n",
    "        po1 = Activation('sigmoid', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])        \n",
    "        o1_loss = K.binary_crossentropy(o1, po1)\n",
    "        loss = K.mean(o1_loss)\n",
    "    else:\n",
    "        po1_logit = Dense(cfg[\"num_class\"], name=\"po1_logit\")(t)\n",
    "        po1 = Activation('softmax', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])\n",
    "        loss = K.categorical_crossentropy(o1, po1, axis=-1)\n",
    "        loss = K.mean(loss)\n",
    "\n",
    "    train_model.add_loss(loss)\n",
    "    opt = _get_opt(num_example=cfg[\"num_example\"], lr=cfg[\"lr\"], min_lr=cfg['min_lr'])\n",
    "    train_model.compile(optimizer=opt)\n",
    "    if summary:\n",
    "        train_model.summary()\n",
    "    return train_model\n",
    "\n",
    "\n",
    "# print(\"----------------build model ---------------\")\n",
    "# model = build_model(cfg, \n",
    "#                     summary=True,\n",
    "#                     word_embedding_matrix=word_embedding_matrix if cfg[\"use_embed\"] else None, \n",
    "#                     bert_summary=True)\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2id_X(x, x_dict, x2=None, maxlen=None, maxlen1=None):\n",
    "    if x2:\n",
    "        x1 = x\n",
    "        del x\n",
    "        maxlen -= 3\n",
    "        maxlen1 -= 2\n",
    "        assert maxlen > maxlen1\n",
    "        maxlen2 = maxlen - maxlen1 - 1\n",
    "        x1 = [\"[CLS]\"] + list(x1)[: maxlen1] + [\"[SEP]\"] \n",
    "        x1 = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x1]\n",
    "        seg1= [0 for _ in x1]\n",
    "        \n",
    "        x2 = list(x2)[: maxlen2] + [\"[SEP]\"] \n",
    "        x2= [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x2]\n",
    "        seg2 = [1 for _ in x2]\n",
    "        x = x1 + x2\n",
    "        seg = seg1 + seg2\n",
    "        \n",
    "    else:\n",
    "        maxlen -= 2\n",
    "        x = [\"[CLS]\"] + list(x)[: maxlen] + [\"[SEP]\"] \n",
    "        x = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x]\n",
    "        seg = [0 for _ in x]        \n",
    "    return x, seg\n",
    "\n",
    "\n",
    "def seq_padding(X, maxlen=None, padding_value=None, debug=False):\n",
    "    L = [len(x) for x in X]\n",
    "    if maxlen is None:\n",
    "        maxlen = max(L)\n",
    "\n",
    "    pad_X = np.array([\n",
    "        np.concatenate([x, [padding_value] * (maxlen - len(x))]) if len(x) < maxlen else x for x in X\n",
    "    ])\n",
    "    if debug:\n",
    "        print(\"[!] before pading {}\\n\".format(X))\n",
    "        print(\"[!] after pading {}\\n\".format(pad_X))\n",
    "    return pad_X\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    \n",
    "    def __init__(self, data, batch_size=B_SIZE, shuffle=SHUFFLE, augm_frac=0.75):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = cfg[\"num_example\"] // self.batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_dict = get_data(data)\n",
    "        self.augm_frac = augm_frac\n",
    "        if cfg[\"num_example\"] % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "            T1, T2, O1, O2 = [], [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data.iloc[i]\n",
    "                text = d[\"query1\"]\n",
    "                label_text = d[\"query2\"]\n",
    "                o1 = d[\"label\"]\n",
    "                \n",
    "                if random() > self.augm_frac:\n",
    "                    data_d = self.data_dict[text]\n",
    "                    pos_data = data_d[\"pos\"]\n",
    "                    neg_data = data_d[\"neg\"]\n",
    "                    if pos_data and neg_data:\n",
    "                        if random() > 0.5:\n",
    "                            o1 = 1\n",
    "                            label_text = choice(pos_data)\n",
    "                            if len(pos_data) >= 2:\n",
    "                                _pos_data = [e for e in pos_data if e != label_text]\n",
    "                                text = choice(_pos_data)\n",
    "                        else:\n",
    "                            o1 = 0\n",
    "                            text = choice(pos_data)\n",
    "                            label_text = choice(neg_data)   \n",
    "                \n",
    "                if random() > 0.5:\n",
    "                    text, label_text = label_text, text\n",
    "                \n",
    "                if o1 == \"\":\n",
    "                    continue\n",
    "                o1 = float(o1)\n",
    "                assert 0 <= o1 <= 1\n",
    "                \n",
    "                O1.append(o1)                \n",
    "                t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "                assert len(t1) == len(t2)\n",
    "                \n",
    "                T1.append(t1)\n",
    "                T2.append(t2)\n",
    "\n",
    "                if len(T1) == self.batch_size or i == idxs[-1]:\n",
    "                    O1 = np.array(O1).reshape(-1, 1)\n",
    "                    T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "                    T2 = seq_padding(T2, padding_value=0)\n",
    "                    assert T1.shape == T2.shape and T1.shape[0] == O1.shape[0]\n",
    "\n",
    "                    yield [T1, T2, O1], None\n",
    "                    T1, T2, O1, = [], [], []\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_):\n",
    "    model_inp_ind = [0, 1]\n",
    "    inputs = [model_.inputs[e] for e in model_inp_ind]\n",
    "    sub_model = Model(inputs=inputs, outputs=[model_.get_layer(\"po1\").output])\n",
    "    return sub_model\n",
    "\n",
    "\n",
    "def find_best_acc_score(y_pred, y_true, use_plt=True, bins=1000):\n",
    "    thres = [i / bins for i in range(1, bins)]\n",
    "    scores = [accuracy_score(y_true, np.array(y_pred > thre, \"int32\")) for thre in thres]\n",
    "#     if use_plt:\n",
    "#         import matplotlib\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         %matplotlib inline\n",
    "#         plt.plot(scores)\n",
    "#         plt.show()\n",
    "    ind = np.argmax(scores)\n",
    "    max_score = np.max(scores)\n",
    "    assert abs(scores[ind] - max_score) < 1e-15\n",
    "    return max_score, thres[ind]\n",
    "            \n",
    "\n",
    "def evaluate(sub_model, data, bs=32):\n",
    "    idxs = list(range(len(data)))\n",
    "    T1, T2, O1, O2 = [], [], [], []\n",
    "    preds = []\n",
    "    for i in idxs:\n",
    "        d = data.iloc[i]\n",
    "        text = d[\"query1\"]\n",
    "        label_text = d[\"query2\"]\n",
    "\n",
    "        t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "        assert len(t1) == len(t2)\n",
    "\n",
    "        T1.append(t1)\n",
    "        T2.append(t2)\n",
    "\n",
    "        o1 = float(d[\"label\"])\n",
    "        O1.append(o1)\n",
    "        if len(T1) == bs or i == idxs[-1]:\n",
    "            T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "            T2 = seq_padding(T2, padding_value=0)\n",
    "            assert T1.shape == T2.shape\n",
    "            pred = sub_model.predict([T1, T2])\n",
    "            preds.append(pred)\n",
    "            T1, T2 = [], []\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    O1 = np.array(O1).reshape(-1)\n",
    "    O1 = O1.astype(\"int32\")\n",
    "    auc = roc_auc_score(O1, preds)\n",
    "    best_res = find_best_acc_score(preds, O1)\n",
    "    print(\"[!] best accurary&threshold = {}\".format(best_res))\n",
    "    print(\"[!] best threshold classification_report\")\n",
    "    print(classification_report(O1,  np.array(preds > best_res[1], \"int32\"), digits=6))    \n",
    "    print(\"-\" * 80)\n",
    "    print(\"[!] np.mean(preds) = {}\".format(np.mean(preds)))\n",
    "    print(\"[!] classification_report\")\n",
    "    print(classification_report(O1,  np.array(preds > 0.5, \"int32\"), digits=6))\n",
    "    acc = accuracy_score(O1, np.array(preds > 0.5, \"int32\"))\n",
    "    return auc, acc\n",
    "    \n",
    "\n",
    "class Evaluate(Callback):\n",
    "    def __init__(self, data, filename=None):\n",
    "        self.F1 = []\n",
    "        self.best = 0.\n",
    "        self.filename = filename\n",
    "        self.data = data\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch ==  0:\n",
    "            print(\"[!] test load&save model\")\n",
    "            f = self.filename + \".h5\"\n",
    "            f = os.path.join(SAVE_DIR, f)\n",
    "            self.model.save(f, include_optimizer=False, overwrite=False)\n",
    "            if \"albert\" in cfg[\"verbose\"]:\n",
    "                model_ = load_model(f) \n",
    "            elif \"nezha\" in cfg[\"verbose\"]:\n",
    "                model_ = load_model(f) \n",
    "            else:\n",
    "                model_ = load_model(f, custom_objects=get_custom_objects()) \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch + 1 < 1:\n",
    "            return\n",
    "#         if epoch + 1 in [3, 6, 9, 10, 12, 15, 18, 20]:\n",
    "#             f = self.filename + \"_{}.h5\".format(epoch + 1)\n",
    "#             f = os.path.join(SAVE_DIR, f)\n",
    "#             self.model.save(f, include_optimizer=False)\n",
    "            \n",
    "        sub_model = get_model(self.model)\n",
    "        f1, class_f1 = evaluate(sub_model, data=self.data)\n",
    "        self.F1.append(f1)\n",
    "        if f1 > self.best:\n",
    "            f = self.filename + \".h5\"\n",
    "            f = os.path.join(SAVE_DIR, f)\n",
    "            self.model.save(f, include_optimizer=False)\n",
    "            \n",
    "        if f1 > self.best:\n",
    "            self.best = f1\n",
    "            print(\"[!] epoch = {}, new best_auc = {}\".format(epoch + 1,  f1))\n",
    "        print('[!] epoch = {}, auc = {}, best auc {}'.format(epoch + 1, f1, self.best))\n",
    "        print('[!] epoch = {}, acc = {}\\n'.format(epoch + 1, class_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_layer(inputs, name, exclude_from=None):\n",
    "    \"\"\"根据inputs和name来搜索层\n",
    "    说明：inputs为某个层或某个层的输出；name为目标层的名字。\n",
    "    实现：根据inputs一直往上递归搜索，直到发现名字为name的层为止；\n",
    "         如果找不到，那就返回None。\n",
    "    \"\"\"\n",
    "    if exclude_from is None:\n",
    "        exclude_from = set()\n",
    "\n",
    "    if isinstance(inputs, keras.layers.Layer):\n",
    "        layer = inputs\n",
    "    else:\n",
    "        layer = inputs._keras_history[0]\n",
    "\n",
    "    if layer.name == name:\n",
    "        return layer\n",
    "    elif layer in exclude_from:\n",
    "        return None\n",
    "    else:\n",
    "        exclude_from.add(layer)\n",
    "        if isinstance(layer, keras.models.Model):\n",
    "            model = layer\n",
    "            for layer in model.layers:\n",
    "                if layer.name == name:\n",
    "                    return layer\n",
    "        inbound_layers = layer._inbound_nodes[0].inbound_layers\n",
    "        if not isinstance(inbound_layers, list):\n",
    "            inbound_layers = [inbound_layers]\n",
    "        if len(inbound_layers) > 0:\n",
    "            for layer in inbound_layers:\n",
    "                layer = search_layer(layer, name, exclude_from)\n",
    "                if layer is not None:\n",
    "                    return layer\n",
    "                \n",
    "def adversarial_training(model, embedding_names, epsilon=1):\n",
    "    \"\"\"给模型添加对抗训练\n",
    "    其中model是需要添加对抗训练的keras模型，embedding_names\n",
    "    则是model里边Embedding层的名字。要在模型compile之后使用。\n",
    "    \"\"\"\n",
    "    if model.train_function is None:  # 如果还没有训练函数\n",
    "        model._make_train_function()  # 手动make\n",
    "    old_train_function = model.train_function  # 备份旧的训练函数\n",
    "\n",
    "    # 查找Embedding层\n",
    "    embedding_layers = []\n",
    "    for embedding_name in embedding_names:\n",
    "        for output in model.outputs:\n",
    "            embedding_layer = search_layer(output, embedding_name)\n",
    "            if embedding_layer is not None:\n",
    "                embedding_layers.append(embedding_layer)\n",
    "                break\n",
    "    for embedding_layer in embedding_layers:\n",
    "        if embedding_layer is None:\n",
    "            raise Exception('Embedding layer not found')\n",
    "\n",
    "    # 求Embedding梯度\n",
    "    embeddings = [embedding_layer.embeddings for embedding_layer in embedding_layers] # Embedding矩阵\n",
    "    gradients = K.gradients(model.total_loss, embeddings)  # Embedding梯度\n",
    "    # gradients = K.zeros_like(embeddings) + gradients[0]  # 转为dense tensor\n",
    "    gradients = [K.zeros_like(embedding) + gradient for embedding, gradient in zip(embeddings, gradients)]\n",
    "\n",
    "    # 封装为函数\n",
    "    inputs = (model._feed_inputs +\n",
    "              model._feed_targets +\n",
    "              model._feed_sample_weights)  # 所有输入层\n",
    "    embedding_gradients = K.function(\n",
    "        inputs=inputs,\n",
    "        outputs=gradients,\n",
    "        name='embedding_gradients',\n",
    "    )  # 封装为函数\n",
    "\n",
    "    def train_function(inputs):  # 重新定义训练函数\n",
    "#         grads = embedding_gradients(inputs)[0]  # Embedding梯度\n",
    "#         delta = epsilon * grads / (np.sqrt((grads**2).sum()) + 1e-8)  # 计算扰动\n",
    "        grads = embedding_gradients(inputs)  # Embedding梯度\n",
    "        deltas = [epsilon * grad / (np.sqrt((grad**2).sum()) + 1e-8) for grad in grads]  # 计算扰动\n",
    "        # 注入扰动\n",
    "        # K.set_value(embeddings, K.eval(embeddings) + delta)  \n",
    "        for embedding, delta in zip(embeddings, deltas):\n",
    "            K.set_value(embedding, K.eval(embedding) + delta)\n",
    "            \n",
    "        outputs = old_train_function(inputs)  # 梯度下降\n",
    "        # 删除扰动\n",
    "        # K.set_value(embeddings, K.eval(embeddings) - delta)  # 删除扰动\n",
    "        for embedding, delta in zip(embeddings, deltas):\n",
    "            K.set_value(embedding, K.eval(embedding) - delta)       \n",
    "        return outputs\n",
    "\n",
    "    model.train_function = train_function  # 覆盖原训练函数\n",
    "\n",
    "\n",
    "# 写好函数后，启用对抗训练只需要一行代码\n",
    "# adversarial_training(model, 'Embedding-Token', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10749, 5)\n",
      "---------------------------------------------------------------------------------\n",
      "[!] start fold_id = 5 (10319, 5) (430, 5)\n",
      "{'base_dir': '../../../chinese_bert/NEZHA_large/NEZHA-Large/', 'span_mode': True, 'lr': 6e-06, 'min_lr': 6e-08, 'ch_type': 'tx_ft', 'trainable': True, 'bert_trainable': True, 'accum_step': 1, 'cls_num': 4, 'unit1': 128, 'unit2': 128, 'unit3': 512, 'conv_num': 128, 'maxlen': 60, 'adv_training': True, 'W2V_FILE': './word_embedding_matrix', 'use_embed': True, 'use_embed_v2': True, 'verbose': 'nezha_v13_augm_seed2020', 'x_pad': 0, 'num_class': 2, 'filename': 'nezha_v13_augm_seed2020_embed_v2_tx_ft_25_6e-06_adv_training_embed_v2', 'num_example': 10319}\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "../../../chinese_bert/NEZHA_large/NEZHA-Large/bert_config.json ../../../chinese_bert/NEZHA_large/NEZHA-Large/model.ckpt-325810\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, None, 500)    10530500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 1)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 501)    0           char_embed[0][0]                 \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 501)    0           concatenate_2[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 128)    646144      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, None, 4096)   324146688   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 128)    0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 4224)   0           model_2[1][0]                    \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 4224)   0           concatenate_3[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 1024)   19406848    lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "extract_layer (Lambda)          (None, 1024)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "po1_logit (Dense)               (None, 1)            1025        extract_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "po1 (Activation)                (None, 1)            0           po1_logit[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 354,731,205\n",
      "Trainable params: 354,533,061\n",
      "Non-trainable params: 198,144\n",
      "__________________________________________________________________________________________________\n",
      "[!] using adv_training\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "[!] test load&save model\n",
      "[WARNING] ../save_models/nezha_v13_augm_seed2020_embed_v2_tx_ft_25_6e-06_adv_training_embed_v2_fold5.h5 already exists - overwrite? [y/n]y\n",
      "[TIP] Next time specify overwrite=True!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 587s 2s/step - loss: 0.5267\n",
      "[!] best accurary&threshold = (0.9488372093023256, 0.201)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.943609  0.972868  0.958015       258\n",
      "           1   0.957317  0.912791  0.934524       172\n",
      "\n",
      "    accuracy                       0.948837       430\n",
      "   macro avg   0.950463  0.942829  0.946270       430\n",
      "weighted avg   0.949092  0.948837  0.948619       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.29919928312301636\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.849498  0.984496  0.912029       258\n",
      "           1   0.969466  0.738372  0.838284       172\n",
      "\n",
      "    accuracy                       0.886047       430\n",
      "   macro avg   0.909482  0.861434  0.875156       430\n",
      "weighted avg   0.897485  0.886047  0.882531       430\n",
      "\n",
      "[!] epoch = 1, new best_auc = 0.9842257075896881\n",
      "[!] epoch = 1, auc = 0.9842257075896881, best auc 0.9842257075896881\n",
      "[!] epoch = 1, acc = 0.8860465116279069\n",
      "\n",
      "Epoch 2/10\n",
      "323/323 [==============================] - 472s 1s/step - loss: 0.2533\n",
      "[!] best accurary&threshold = (0.9558139534883721, 0.317)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.950943  0.976744  0.963671       258\n",
      "           1   0.963636  0.924419  0.943620       172\n",
      "\n",
      "    accuracy                       0.955814       430\n",
      "   macro avg   0.957290  0.950581  0.953646       430\n",
      "weighted avg   0.956021  0.955814  0.955651       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.35242870450019836\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.926471  0.976744  0.950943       258\n",
      "           1   0.962025  0.883721  0.921212       172\n",
      "\n",
      "    accuracy                       0.939535       430\n",
      "   macro avg   0.944248  0.930233  0.936078       430\n",
      "weighted avg   0.940692  0.939535  0.939051       430\n",
      "\n",
      "[!] epoch = 2, new best_auc = 0.9902199387056067\n",
      "[!] epoch = 2, auc = 0.9902199387056067, best auc 0.9902199387056067\n",
      "[!] epoch = 2, acc = 0.9395348837209302\n",
      "\n",
      "Epoch 3/10\n",
      "323/323 [==============================] - 474s 1s/step - loss: 0.1720\n",
      "[!] best accurary&threshold = (0.9627906976744186, 0.176)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.961832  0.976744  0.969231       258\n",
      "           1   0.964286  0.941860  0.952941       172\n",
      "\n",
      "    accuracy                       0.962791       430\n",
      "   macro avg   0.963059  0.959302  0.961086       430\n",
      "weighted avg   0.962814  0.962791  0.962715       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3611101806163788\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.930403  0.984496  0.956685       258\n",
      "           1   0.974522  0.889535  0.930091       172\n",
      "\n",
      "    accuracy                       0.946512       430\n",
      "   macro avg   0.952463  0.937016  0.943388       430\n",
      "weighted avg   0.948051  0.946512  0.946048       430\n",
      "\n",
      "[!] epoch = 3, new best_auc = 0.9911663962502254\n",
      "[!] epoch = 3, auc = 0.9911663962502254, best auc 0.9911663962502254\n",
      "[!] epoch = 3, acc = 0.9465116279069767\n",
      "\n",
      "Epoch 4/10\n",
      "323/323 [==============================] - 470s 1s/step - loss: 0.1307\n",
      "[!] best accurary&threshold = (0.9604651162790697, 0.317)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.954717  0.980620  0.967495       258\n",
      "           1   0.969697  0.930233  0.949555       172\n",
      "\n",
      "    accuracy                       0.960465       430\n",
      "   macro avg   0.962207  0.955426  0.958525       430\n",
      "weighted avg   0.960709  0.960465  0.960319       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.375064879655838\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.944030  0.980620  0.961977       258\n",
      "           1   0.969136  0.912791  0.940120       172\n",
      "\n",
      "    accuracy                       0.953488       430\n",
      "   macro avg   0.956583  0.946705  0.951048       430\n",
      "weighted avg   0.954072  0.953488  0.953234       430\n",
      "\n",
      "[!] epoch = 4, new best_auc = 0.9918424373535244\n",
      "[!] epoch = 4, auc = 0.9918424373535244, best auc 0.9918424373535244\n",
      "[!] epoch = 4, acc = 0.9534883720930233\n",
      "\n",
      "Epoch 5/10\n",
      "323/323 [==============================] - 470s 1s/step - loss: 0.1075\n",
      "[!] best accurary&threshold = (0.958139534883721, 0.357)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.951128  0.980620  0.965649       258\n",
      "           1   0.969512  0.924419  0.946429       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.960320  0.952519  0.956039       430\n",
      "weighted avg   0.958482  0.958140  0.957961       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3726509213447571\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.944030  0.980620  0.961977       258\n",
      "           1   0.969136  0.912791  0.940120       172\n",
      "\n",
      "    accuracy                       0.953488       430\n",
      "   macro avg   0.956583  0.946705  0.951048       430\n",
      "weighted avg   0.954072  0.953488  0.953234       430\n",
      "\n",
      "[!] epoch = 5, auc = 0.9906030286641427, best auc 0.9918424373535244\n",
      "[!] epoch = 5, acc = 0.9534883720930233\n",
      "\n",
      "Epoch 6/10\n",
      "323/323 [==============================] - 465s 1s/step - loss: 0.0831\n",
      "[!] best accurary&threshold = (0.958139534883721, 0.438)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.954545  0.976744  0.965517       258\n",
      "           1   0.963855  0.930233  0.946746       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.959200  0.953488  0.956131       430\n",
      "weighted avg   0.958269  0.958140  0.958009       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3843204975128174\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.950943  0.976744  0.963671       258\n",
      "           1   0.963636  0.924419  0.943620       172\n",
      "\n",
      "    accuracy                       0.955814       430\n",
      "   macro avg   0.957290  0.950581  0.953646       430\n",
      "weighted avg   0.956021  0.955814  0.955651       430\n",
      "\n",
      "[!] epoch = 6, auc = 0.99017486929872, best auc 0.9918424373535244\n",
      "[!] epoch = 6, acc = 0.9558139534883721\n",
      "\n",
      "Epoch 7/10\n",
      "323/323 [==============================] - 470s 1s/step - loss: 0.0682\n",
      "[!] best accurary&threshold = (0.9558139534883721, 0.644)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.947566  0.980620  0.963810       258\n",
      "           1   0.969325  0.918605  0.943284       172\n",
      "\n",
      "    accuracy                       0.955814       430\n",
      "   macro avg   0.958445  0.949612  0.953547       430\n",
      "weighted avg   0.956269  0.955814  0.955599       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3863186836242676\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.950382  0.965116  0.957692       258\n",
      "           1   0.946429  0.924419  0.935294       172\n",
      "\n",
      "    accuracy                       0.948837       430\n",
      "   macro avg   0.948405  0.944767  0.946493       430\n",
      "weighted avg   0.948800  0.948837  0.948733       430\n",
      "\n",
      "[!] epoch = 7, auc = 0.9898368487470705, best auc 0.9918424373535244\n",
      "[!] epoch = 7, acc = 0.9488372093023256\n",
      "\n",
      "Epoch 8/10\n",
      "323/323 [==============================] - 470s 1s/step - loss: 0.0528\n",
      "[!] best accurary&threshold = (0.9558139534883721, 0.29)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.961390  0.965116  0.963250       258\n",
      "           1   0.947368  0.941860  0.944606       172\n",
      "\n",
      "    accuracy                       0.955814       430\n",
      "   macro avg   0.954379  0.953488  0.953928       430\n",
      "weighted avg   0.955781  0.955814  0.955792       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.38551706075668335\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.943609  0.972868  0.958015       258\n",
      "           1   0.957317  0.912791  0.934524       172\n",
      "\n",
      "    accuracy                       0.948837       430\n",
      "   macro avg   0.950463  0.942829  0.946270       430\n",
      "weighted avg   0.949092  0.948837  0.948619       430\n",
      "\n",
      "[!] epoch = 8, auc = 0.9885749053542455, best auc 0.9918424373535244\n",
      "[!] epoch = 8, acc = 0.9488372093023256\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 470s 1s/step - loss: 0.0454\n",
      "[!] best accurary&threshold = (0.9534883720930233, 0.165)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.961240  0.961240  0.961240       258\n",
      "           1   0.941860  0.941860  0.941860       172\n",
      "\n",
      "    accuracy                       0.953488       430\n",
      "   macro avg   0.951550  0.951550  0.951550       430\n",
      "weighted avg   0.953488  0.953488  0.953488       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3837631344795227\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.947368  0.976744  0.961832       258\n",
      "           1   0.963415  0.918605  0.940476       172\n",
      "\n",
      "    accuracy                       0.953488       430\n",
      "   macro avg   0.955392  0.947674  0.951154       430\n",
      "weighted avg   0.953787  0.953488  0.953290       430\n",
      "\n",
      "[!] epoch = 9, auc = 0.988755182981792, best auc 0.9918424373535244\n",
      "[!] epoch = 9, acc = 0.9534883720930233\n",
      "\n",
      "Epoch 10/10\n",
      "323/323 [==============================] - 476s 1s/step - loss: 0.0387\n",
      "[!] best accurary&threshold = (0.9534883720930233, 0.129)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.964844  0.957364  0.961089       258\n",
      "           1   0.936782  0.947674  0.942197       172\n",
      "\n",
      "    accuracy                       0.953488       430\n",
      "   macro avg   0.950813  0.952519  0.951643       430\n",
      "weighted avg   0.953619  0.953488  0.953532       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3797896206378937\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.943609  0.972868  0.958015       258\n",
      "           1   0.957317  0.912791  0.934524       172\n",
      "\n",
      "    accuracy                       0.948837       430\n",
      "   macro avg   0.950463  0.942829  0.946270       430\n",
      "weighted avg   0.949092  0.948837  0.948619       430\n",
      "\n",
      "[!] epoch = 10, auc = 0.9893185505678744, best auc 0.9918424373535244\n",
      "[!] epoch = 10, acc = 0.9488372093023256\n",
      "\n",
      "[0.9842257075896881, 0.9902199387056067, 0.9911663962502254, 0.9918424373535244, 0.9906030286641427, 0.99017486929872, 0.9898368487470705, 0.9885749053542455, 0.988755182981792, 0.9893185505678744] 0.9918424373535244\n",
      "[5020.761966943741] finish fold_id = 5\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "[!] start fold_id = 6 (10319, 5) (430, 5)\n",
      "{'base_dir': '../../../chinese_bert/NEZHA_large/NEZHA-Large/', 'span_mode': True, 'lr': 6e-06, 'min_lr': 6e-08, 'ch_type': 'tx_ft', 'trainable': True, 'bert_trainable': True, 'accum_step': 1, 'cls_num': 4, 'unit1': 128, 'unit2': 128, 'unit3': 512, 'conv_num': 128, 'maxlen': 60, 'adv_training': True, 'W2V_FILE': './word_embedding_matrix', 'use_embed': True, 'use_embed_v2': True, 'verbose': 'nezha_v13_augm_seed2020', 'x_pad': 0, 'num_class': 2, 'filename': 'nezha_v13_augm_seed2020_embed_v2_tx_ft_25_6e-06_adv_training_embed_v2', 'num_example': 10319}\n",
      "../../../chinese_bert/NEZHA_large/NEZHA-Large/bert_config.json ../../../chinese_bert/NEZHA_large/NEZHA-Large/model.ckpt-325810\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, None, 500)    10530500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 1)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 501)    0           char_embed[0][0]                 \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 501)    0           concatenate_2[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 128)    646144      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, None, 4096)   324146688   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 128)    0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 4224)   0           model_2[1][0]                    \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 4224)   0           concatenate_3[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 1024)   19406848    lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "extract_layer (Lambda)          (None, 1024)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "po1_logit (Dense)               (None, 1)            1025        extract_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "po1 (Activation)                (None, 1)            0           po1_logit[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 354,731,205\n",
      "Trainable params: 354,533,061\n",
      "Non-trainable params: 198,144\n",
      "__________________________________________________________________________________________________\n",
      "[!] using adv_training\n",
      "Epoch 1/10\n",
      "[!] test load&save model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17/323 [>.............................] - ETA: 36:19 - loss: 0.7256"
     ]
    }
   ],
   "source": [
    "adv_layer_names = ['Embedding-Token', 'char_embed']\n",
    "\n",
    "if -1 in FOLD_ID:\n",
    "    fold_id = -1\n",
    "    cfg[\"num_example\"] = len(train_data)\n",
    "    print(\"-\" * 81)\n",
    "    print(\"[!] start fold_id =\", fold_id, train_data.shape, dev_data.shape)\n",
    "    print(cfg)\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    train_D = data_generator(train_data)\n",
    "    seed(SEED + fold_id)\n",
    "    np.random.seed(SEED + fold_id)\n",
    "    tf.random.set_random_seed(SEED + fold_id)\n",
    "    model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix if cfg[\"use_embed\"] else None)\n",
    "    if cfg[\"adv_training\"]:\n",
    "        print(\"[!] using adv_training\")\n",
    "        adversarial_training(model, adv_layer_names, 0.5)\n",
    "    evaluator = Evaluate(filename=cfg[\"filename\"] + \"_fold{}\".format(fold_id), data=dev_data)\n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                              steps_per_epoch=len(train_D),\n",
    "                              epochs=RUN_EPOCH,\n",
    "                              callbacks=[evaluator],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    del model, train_data, dev_data\n",
    "    gc.collect()\n",
    "    print(\"[!] finish fold_id =\", fold_id)\n",
    "    print(\"-\" * 81)\n",
    "    \n",
    "\n",
    "skf = SKF(FOLD_NUM, shuffle=False, random_state=SEED)\n",
    "\n",
    "print(all_data.shape)\n",
    "_t0 = time()\n",
    "for fold_id, (trn_ind, val_ind) in enumerate(skf.split(range(len(all_data)), all_data[\"label\"])):\n",
    "    if fold_id not in FOLD_ID:\n",
    "        continue\n",
    "    t0 = time()\n",
    "    dev_data = all_data.iloc[val_ind].reset_index(drop=True)\n",
    "    train_data = all_data.iloc[trn_ind].reset_index(drop=True)\n",
    "    cfg[\"num_example\"] = len(train_data)\n",
    "    print(\"-\" * 81)\n",
    "    print(\"[!] start fold_id =\", fold_id, train_data.shape, dev_data.shape)\n",
    "    print(cfg)\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    train_D = data_generator(train_data)\n",
    "    seed(SEED + fold_id)\n",
    "    np.random.seed(SEED + fold_id)\n",
    "    tf.random.set_random_seed(SEED + fold_id)\n",
    "    model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix if cfg[\"use_embed\"] else None)\n",
    "    if cfg[\"adv_training\"]:\n",
    "        print(\"[!] using adv_training\")\n",
    "        adversarial_training(model, adv_layer_names, 0.5)\n",
    "    evaluator = Evaluate(filename=cfg[\"filename\"] + \"_fold{}\".format(fold_id), data=dev_data)\n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                              steps_per_epoch=len(train_D),\n",
    "                              epochs=RUN_EPOCH,\n",
    "                              callbacks=[evaluator],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    print(evaluator.F1, max(evaluator.F1))    \n",
    "    print(\"[{}] finish fold_id =\".format(time() - t0), fold_id)\n",
    "    print(\"-\" * 81)\n",
    "    del model, train_data, dev_data, evaluator\n",
    "    gc.collect()    \n",
    "print(\"[{}] finish =\".format(time() - _t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "def get_cnt(data, col1, col2, cate):\n",
    "    data = data[data[\"category\"] == cate]\n",
    "    if col2:\n",
    "        data = data[col1].tolist() + data[col2].tolist() \n",
    "    else:\n",
    "        data = data[col1].tolist()\n",
    "    data = [list(jieba.cut(e)) for e in data]\n",
    "    cnt1 = Counter([w for sent in data for w in sent])\n",
    "    return cnt1\n",
    "    \n",
    "    \n",
    "stop_words = ['？', '吗', '了', '，', '的', '?', '有', '得', '地', '是', '什么',\n",
    "              '怎么办', '哪些', '怎么回事', '怎么', '要', '能', '呢', '会']\n",
    "for cate in train_data[\"category\"].value_counts().index:\n",
    "    print(\"-\" * 40, cate, \"-\" * 40)\n",
    "    cnt1 = get_cnt(train_data, col1=\"query1\", col2=\"query2\", cate=cate)\n",
    "    cnt1 = [(k, cnt) for k, cnt in cnt1.most_common() if k not in stop_words]\n",
    "    print(cnt1[: 20])\n",
    "    print(\"-\" * 81)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in dev_data[\"category\"].value_counts().index:\n",
    "    print(\"------------------------\")\n",
    "    print(cate)\n",
    "    cnt1 = get_cnt(dev_data, col=\"query1\", cate=cate)\n",
    "    cnt2 = get_cnt(dev_data, col=\"query2\", cate=cate)\n",
    "    print(cnt1.most_common(10))\n",
    "    print(cnt2.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _foo(x, col):\n",
    "    cate = x[\"category\"]\n",
    "    q = x[col]\n",
    "    if cate == \"咳血\":\n",
    "        return q.count(\"咯血\") + q.count(\"咳血\")\n",
    "    \n",
    "    \n",
    "    return q.count(cate)\n",
    "\n",
    "    \n",
    "train_data[\"q1_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query1\"), axis=1)\n",
    "train_data[\"q2_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query2\"), axis=1)\n",
    "train_data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
