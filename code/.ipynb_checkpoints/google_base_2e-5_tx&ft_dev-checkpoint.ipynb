{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5fea8ff6ab74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/sparse/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/sparse/frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# pylint: disable=E1101,E1103,W0231,E0202\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_new_module\u001b[0;34m(name)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from random import choice, seed, randint, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, CuDNNGRU as GRU, CuDNNLSTM as LSTM, Dropout, BatchNormalization\n",
    "from keras.layers import Dense, Concatenate, Activation, Embedding, SpatialDropout1D, Bidirectional, Lambda, Conv1D\n",
    "from keras.layers import Add, Average\n",
    "from keras.optimizers import Nadam, Adam, Adamax\n",
    "from keras.activations import absolute_import\n",
    "from keras.legacy import interfaces\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "from keras_bert.loader import load_trained_model_from_checkpoint\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.objectives import sparse_categorical_crossentropy\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from special_tokens import CHINESE_MAP\n",
    "from metric_utils import compute_f1, compute_exact\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = \"../../../chinese_bert/chinese_L-12_H-768_A-12/\"\n",
    "TRN_FILENAME = \"../data/train_20200228.csv\"\n",
    "DEV_FILENAME = \"../data/dev_20200228.csv\"\n",
    "PREFIX = \"google_bert_base\"\n",
    "MAX_EPOCH = 20\n",
    "MAX_LEN = 60\n",
    "MAX_DOC_LEN = MAX_LEN // 2\n",
    "THRE = 0.5\n",
    "B_SIZE = 32\n",
    "ACCUM_STEP = int(32 // B_SIZE)\n",
    "FOLD_ID = -1\n",
    "FOLD_NUM = 5\n",
    "SEED = 2020 + FOLD_ID\n",
    "SHUFFLE = True\n",
    "DOC_STRIDE = 128\n",
    "cfg = {}\n",
    "cfg[\"verbose\"] = PREFIX\n",
    "cfg[\"base_dir\"] = BERT_PRETRAINED_DIR\n",
    "cfg[\"span_mode\"] = True\n",
    "cfg[\"lr\"] = 2e-5\n",
    "cfg['min_lr'] = 6e-8 \n",
    "cfg[\"ch_type\"] = \"tx_ft\"\n",
    "cfg[\"trainable\"] = True\n",
    "cfg[\"bert_trainable\"] = True\n",
    "cfg[\"accum_step\"] = ACCUM_STEP\n",
    "cfg[\"cls_num\"] = 4\n",
    "cfg[\"unit1\"] = 128\n",
    "cfg[\"unit2\"] = 128\n",
    "cfg[\"unit3\"] = 512\n",
    "cfg[\"conv_num\"] = 128\n",
    "cfg['maxlen'] = MAX_LEN\n",
    "\n",
    "train_data = pd.read_csv(TRN_FILENAME)\n",
    "train_data.fillna(\"\", inplace=True)\n",
    "dev_data = pd.read_csv(DEV_FILENAME)\n",
    "dev_data.fillna(\"\", inplace=True)\n",
    "all_data = pd.concat([train_data, dev_data], axis=0, ignore_index=True)\n",
    "\n",
    "if FOLD_ID > 0 and FOLD_ID < FOLD_NUM:\n",
    "    print(\"-------------\")\n",
    "    dev_data = all_data.iloc[[i for i, _ in enumerate(range(len(all_data))) if i % FOLD_NUM == FOLD_ID]]\n",
    "    dev_data = dev_data.reset_index(drop=True)\n",
    "    train_data = all_data.iloc[[i for i, _ in enumerate(range(len(all_data))) if i % FOLD_NUM != FOLD_ID]]\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(all_data.shape, train_data.shape, dev_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "    if \"albert\"in cfg[\"verbose\"].lower():\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab_chinese.txt')\n",
    "    else:\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "    with open(dict_path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "\n",
    "    word_index = {v: k  for k, v in enumerate(lines)}\n",
    "    for k, v in CHINESE_MAP.items():\n",
    "        assert v in word_index\n",
    "        del word_index[v]\n",
    "    return word_index\n",
    "\n",
    "\n",
    "def get_label():\n",
    "    labels = [\"0\", \"1\"]\n",
    "    label2id = {k: v for v, k in enumerate(labels)}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    return label2id, id2label, labels\n",
    "    \n",
    "    \n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=np.float16)\n",
    "\n",
    "\n",
    "def load_embed(path, dim=300, word_index=None):\n",
    "    embedding_index = {}\n",
    "    with open(path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.strip().split()\n",
    "            word, arr = l[0], l[1:]\n",
    "            if len(arr) != dim:\n",
    "                print(\"[!] l = {}\".format(l))\n",
    "                continue\n",
    "            if word_index and word not in word_index:\n",
    "                continue\n",
    "            word, arr = get_coefs(word, arr)\n",
    "            embedding_index[word] = arr\n",
    "    return embedding_index\n",
    "\n",
    "\n",
    "def build_matrix(path, word_index=None, max_features=None, dim=300):\n",
    "    embedding_index = load_embed(path, dim=dim, word_index=word_index)\n",
    "    max_features = len(word_index) + 1 if max_features is None else max_features \n",
    "    embedding_matrix = np.zeros((max_features + 1, dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_word_embed(word_embed_f1=\"../../../chinese_embedding/Tencent_AILab_ChineseEmbedding.txt\", \n",
    "               word_embed_f2=\"../../../chinese_embedding/cc.zh.300.vec\", \n",
    "               save_filename=\"./word_embedding_matrix\",\n",
    "               word_index=None):\n",
    "    if os.path.exists(save_filename + \".npy\"):\n",
    "        word_embedding_matrix = np.load(save_filename + \".npy\").astype(\"float32\")\n",
    "    else:\n",
    "        if \"tx\" in cfg[\"ch_type\"]:\n",
    "            tx_embed, tx_unk = build_matrix(word_embed_f1, word_index=word_index, dim=200)\n",
    "        else:\n",
    "            tx_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            tx_unk = []\n",
    "        if \"ft\" in cfg[\"ch_type\"]:\n",
    "            ft_embed, ft_unk = build_matrix(word_embed_f2, word_index=word_index, dim=300)\n",
    "        else:\n",
    "            ft_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            ft_unk = []    \n",
    "\n",
    "        word_embedding_matrix = np.concatenate([tx_embed, ft_embed], axis=-1).astype(\"float32\")\n",
    "        print(word_embedding_matrix.shape, len(tx_unk), len(ft_unk))\n",
    "        np.save(save_filename, word_embedding_matrix )\n",
    "    return word_embedding_matrix\n",
    "    \n",
    "    \n",
    "word_index = get_vocab()\n",
    "label2id, id2label, labels = get_label()\n",
    "word_embedding_matrix = load_word_embed(word_index=word_index)\n",
    "\n",
    "NUM_CLASS = len(label2id)\n",
    "cfg[\"x_pad\"] = word_index[\"[PAD]\"]\n",
    "cfg[\"num_example\"] = len(train_data)\n",
    "cfg[\"num_class\"] = NUM_CLASS\n",
    "cfg[\"filename\"] = \"{}_{}_{}fold{}_{}\".format(PREFIX, cfg[\"ch_type\"], FOLD_NUM, FOLD_ID, cfg[\"lr\"])\n",
    "print(label2id, id2label, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(cfg, summary=False, word_embedding_matrix=None):\n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        if \"albert\"in cfg[\"verbose\"].lower():\n",
    "            from bert4keras.bert import build_bert_model\n",
    "            config_file = os.path.join(base_dir, 'albert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'model.ckpt-best')\n",
    "            model = build_bert_model(\n",
    "                    config_path=config_file,\n",
    "                    checkpoint_path=checkpoint_file,\n",
    "                    model='albert',\n",
    "                    return_keras_model=True\n",
    "            )\n",
    "            if cfg_[\"cls_num\"] > 1:\n",
    "                output = Concatenate(axis=-1)([model.get_layer(\"Encoder-1-FeedForward-Norm\").get_output_at(-i) for i in range(1, cfg[\"cls_num\"] + 1)])\n",
    "                model = Model(model.inputs[: 2], outputs=output)\n",
    "            model.trainable = cfg_[\"bert_trainable\"]\n",
    "        else:\n",
    "            config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "            if not os.path.exists(config_file):\n",
    "                config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "                checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')            \n",
    "            model = load_trained_model_from_checkpoint(config_file, \n",
    "                                                       checkpoint_file, \n",
    "                                                       training=False, \n",
    "                                                       trainable=cfg_[\"bert_trainable\"], \n",
    "                                                       output_layer_num=cfg_[\"cls_num\"],\n",
    "                                                       seq_len=cfg_['maxlen'])\n",
    "            \n",
    "            # model = Model(inputs=model.inputs[: 2], outputs=model.layers[-7].output)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def _get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        total_steps, warmup_steps = calc_train_steps(\n",
    "            num_example=num_example,\n",
    "            batch_size=B_SIZE,\n",
    "            epochs=MAX_EPOCH,\n",
    "            warmup_proportion=warmup_proportion,\n",
    "        )\n",
    "        opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "        if cfg.get(\"accum_step\", None) and cfg[\"accum_step\"] > 1:\n",
    "            print(\"[!] using accum_step = {}\".format(cfg[\"accum_step\"]))\n",
    "            from accum_optimizer import AccumOptimizer\n",
    "            opt = AccumOptimizer(opt, steps_per_update=cfg[\"accum_step\"])\n",
    "        \n",
    "        return opt\n",
    "\n",
    "    bert_model = _get_model(cfg[\"base_dir\"], cfg)\n",
    "\n",
    "    if word_embedding_matrix is not None:\n",
    "        embed = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                          output_dim=word_embedding_matrix.shape[1],\n",
    "                          weights=[word_embedding_matrix],\n",
    "                          trainable=cfg[\"trainable\"]\n",
    "                         )\n",
    "    \n",
    "    t1_in = Input(shape=(None, ))\n",
    "    t2_in = Input(shape=(None, ))\n",
    "    o1_in = Input(shape=(1, ))\n",
    "    o2_in = Input(shape=(1, ))\n",
    "\n",
    "    t1, t2, o1, o2 = t1_in, t2_in, o1_in, o2_in\n",
    "    \n",
    "    ## Char information\n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'))(t1)\n",
    "    word_embed = embed(t1)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    word_embed = Bidirectional(LSTM(cfg[\"unit1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    \n",
    "    t = bert_model([t1, t2])\n",
    "    t = Concatenate(axis=-1)([t, word_embed])\n",
    "    t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask]) \n",
    "    t = Bidirectional(LSTM(cfg[\"unit3\"], return_sequences=True), merge_mode=\"concat\")(t)\n",
    "    # t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask]) \n",
    "    # t = Conv1D(cfg[\"conv_num\"], kernel_size=3, padding=\"same\")(t) \n",
    "    t = Lambda(lambda x: x[:, 0, :], name=\"extract_layer\")(t)\n",
    "    if cfg.get(\"num_class\", 1) == 2:\n",
    "        po1_logit = Dense(1, name=\"po1_logit\")(t)\n",
    "        po1 = Activation('sigmoid', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])        \n",
    "        o1_loss = K.binary_crossentropy(o1, po1)\n",
    "        loss = K.mean(o1_loss)\n",
    "    else:\n",
    "        po1_logit = Dense(cfg[\"num_class\"], name=\"po1_logit\")(t)\n",
    "        po1 = Activation('softmax', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])\n",
    "        loss = K.categorical_crossentropy(o1, po1, axis=-1)\n",
    "        loss = K.mean(loss)\n",
    "\n",
    "    train_model.add_loss(loss)\n",
    "    opt = _get_opt(num_example=cfg[\"num_example\"], lr=cfg[\"lr\"], min_lr=cfg['min_lr'])\n",
    "    train_model.compile(optimizer=opt)\n",
    "    if summary:\n",
    "        train_model.summary()\n",
    "    return train_model\n",
    "\n",
    "\n",
    "# print(\"----------------build model ---------------\")\n",
    "# model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2id_X(x, x_dict, x2=None, maxlen=None, maxlen1=None):\n",
    "    if x2:\n",
    "        x1 = x\n",
    "        del x\n",
    "        maxlen -= 3\n",
    "        maxlen1 -= 2\n",
    "        assert maxlen > maxlen1\n",
    "        maxlen2 = maxlen - maxlen1 - 1\n",
    "        x1 = [\"[CLS]\"] + list(x1)[: maxlen1] + [\"[SEP]\"] \n",
    "        x1 = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x1]\n",
    "        seg1= [0 for _ in x1]\n",
    "        \n",
    "        x2 = list(x2)[: maxlen2] + [\"[SEP]\"] \n",
    "        x2= [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x2]\n",
    "        seg2 = [1 for _ in x2]\n",
    "        x = x1 + x2\n",
    "        seg = seg1 + seg2\n",
    "        \n",
    "    else:\n",
    "        maxlen -= 2\n",
    "        x = [\"[CLS]\"] + list(x)[: maxlen] + [\"[SEP]\"] \n",
    "        x = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x]\n",
    "        seg = [0 for _ in x]        \n",
    "    return x, seg\n",
    "\n",
    "\n",
    "def seq_padding(X, maxlen=None, padding_value=None, debug=False):\n",
    "    L = [len(x) for x in X]\n",
    "    if maxlen is None:\n",
    "        maxlen = max(L)\n",
    "\n",
    "    pad_X = np.array([\n",
    "        np.concatenate([x, [padding_value] * (maxlen - len(x))]) if len(x) < maxlen else x for x in X\n",
    "    ])\n",
    "    if debug:\n",
    "        print(\"[!] before pading {}\\n\".format(X))\n",
    "        print(\"[!] after pading {}\\n\".format(pad_X))\n",
    "    return pad_X\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    \n",
    "    def __init__(self, data, batch_size=B_SIZE, shuffle=SHUFFLE):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = cfg[\"num_example\"] // self.batch_size\n",
    "        self.shuffle = shuffle\n",
    "        if cfg[\"num_example\"] % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "            T1, T2, O1, O2 = [], [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data.iloc[i]\n",
    "                text = d[\"query1\"]\n",
    "                label_text = d[\"query2\"]\n",
    "                \n",
    "                o1 = d[\"label\"]\n",
    "                if o1 == \"\":\n",
    "                    continue\n",
    "                o1 = float(o1)\n",
    "                assert 0 <= o1 <= 1\n",
    "                \n",
    "                O1.append(o1)                \n",
    "                t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "                assert len(t1) == len(t2)\n",
    "                \n",
    "                T1.append(t1)\n",
    "                T2.append(t2)\n",
    "\n",
    "                if len(T1) == self.batch_size or i == idxs[-1]:\n",
    "                    O1 = np.array(O1).reshape(-1, 1)\n",
    "                    T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "                    T2 = seq_padding(T2, padding_value=0)\n",
    "                    assert T1.shape == T2.shape and T1.shape[0] == O1.shape[0]\n",
    "\n",
    "                    yield [T1, T2, O1], None\n",
    "                    T1, T2, O1, = [], [], []\n",
    "                    \n",
    "                        \n",
    "# gen = data_generator(train_data)\n",
    "# for i, e in enumerate(gen):\n",
    "#     if i > 400:\n",
    "#         break\n",
    "#     print(\"i = {}\".format(i), \"-\" * 81)\n",
    "#     # print(e[0])\n",
    "#     for _e in e[0]:\n",
    "#         print(_e.shape, _e.sum(axis=0).sum(axis=0))\n",
    "# del gen\n",
    "# print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_):\n",
    "    model_inp_ind = [0, 1]\n",
    "    inputs = [model_.inputs[e] for e in model_inp_ind]\n",
    "    sub_model = Model(inputs=inputs, outputs=[model_.get_layer(\"po1\").output])\n",
    "    return sub_model\n",
    "\n",
    "\n",
    "def evaluate(sub_model, data, bs=32):\n",
    "    idxs = list(range(len(data)))\n",
    "    T1, T2, O1, O2 = [], [], [], []\n",
    "    preds = []\n",
    "    for i in idxs:\n",
    "        d = data.iloc[i]\n",
    "        text = d[\"query1\"]\n",
    "        label_text = d[\"query2\"]\n",
    "\n",
    "        t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "        assert len(t1) == len(t2)\n",
    "\n",
    "        T1.append(t1)\n",
    "        T2.append(t2)\n",
    "\n",
    "        o1 = float(d[\"label\"])\n",
    "        O1.append(o1)\n",
    "        if len(T1) == bs or i == idxs[-1]:\n",
    "            T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "            T2 = seq_padding(T2, padding_value=0)\n",
    "            assert T1.shape == T2.shape\n",
    "            pred = sub_model.predict([T1, T2])\n",
    "            preds.append(pred)\n",
    "            T1, T2 = [], []\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    O1 = np.array(O1).reshape(-1)\n",
    "    O1 = O1.astype(\"int32\")\n",
    "    auc = roc_auc_score(O1, preds)\n",
    "    acc = accuracy_score(O1, np.array(preds > 0.5, \"int32\"))\n",
    "    return auc, acc\n",
    "    \n",
    "\n",
    "class Evaluate(Callback):\n",
    "    def __init__(self, data, filename=None):\n",
    "        self.F1 = []\n",
    "        self.best = 0.\n",
    "        self.filename = filename\n",
    "        self.data = data\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch ==  0:\n",
    "            print(\"[!] test load&save model\")\n",
    "            f = self.filename + \".h5\"\n",
    "            self.model.save(f, include_optimizer=False, overwrite=False)\n",
    "            if \"albert\" in cfg[\"verbose\"]:\n",
    "                model_ = load_model(f) \n",
    "            else:\n",
    "                model_ = load_model(f, custom_objects=get_custom_objects()) \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch + 1 < 1:\n",
    "            return\n",
    "        if epoch + 1 in [3, 6, 9, 10, 12, 15, 18, 20]:\n",
    "            self.model.save(self.filename + \"_{}.h5\".format(epoch + 1), include_optimizer=False)\n",
    "            \n",
    "        sub_model = get_model(self.model)\n",
    "        f1, class_f1 = evaluate(sub_model, data=self.data)\n",
    "        self.F1.append(f1)\n",
    "        if f1 > self.best:\n",
    "            self.model.save(self.filename + \".h5\", include_optimizer=False)\n",
    "            \n",
    "        if f1 > self.best:\n",
    "            self.best = f1\n",
    "            print(\"[!] epoch = {}, new best_auc = {}\".format(epoch + 1,  f1))\n",
    "        print('[!] epoch = {}, auc = {}, best auc {}'.format(epoch + 1, f1, self.best))\n",
    "        print('[!] epoch = {}, acc = {}\\n'.format(epoch + 1, class_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gc.collect()\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_random_seed(SEED)\n",
    "train_D = data_generator(train_data)\n",
    "print(cfg)\n",
    "model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "evaluator = Evaluate(filename=cfg[\"filename\"], data=dev_data)\n",
    "model.fit_generator(train_D.__iter__(),\n",
    "                          steps_per_epoch=len(train_D),\n",
    "                          epochs=MAX_EPOCH,\n",
    "                          callbacks=[evaluator],\n",
    "                          shuffle=True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.F1, max(evaluator.F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sub_model, data, bs=32):\n",
    "    idxs = list(range(len(data)))\n",
    "    T1, T2, O1, O2 = [], [], [], []\n",
    "    preds = []\n",
    "    for i in idxs:\n",
    "        d = data.iloc[i]\n",
    "        text = d[\"query1\"]\n",
    "        label_text = d[\"query2\"]\n",
    "\n",
    "        t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "        assert len(t1) == len(t2)\n",
    "\n",
    "        T1.append(t1)\n",
    "        T2.append(t2)\n",
    "\n",
    "        if len(T1) == bs or i == idxs[-1]:\n",
    "            T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "            T2 = seq_padding(T2, padding_value=0)\n",
    "            assert T1.shape == T2.shape\n",
    "            pred = sub_model.predict([T1, T2])\n",
    "            preds.append(pred)\n",
    "            T1, T2 = [], []\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def ensemble_predictions(predictions, weights=None, type_=\"linear\"):\n",
    "    if not weights:\n",
    "        # print(\"[!] AVE_WGT\")\n",
    "        weights = [1./ len(predictions) for _ in range(len(predictions))]\n",
    "    assert len(predictions) == len(weights)\n",
    "    if np.sum(weights) != 1.0:\n",
    "        weights = [w / np.sum(weights) for w in weights]\n",
    "    # print(\"[!] weights = {}\".format(weights))\n",
    "    assert np.isclose(np.sum(weights), 1.0)\n",
    "    if type_ == \"linear\":\n",
    "        res = np.average(predictions, weights=weights, axis=0)\n",
    "    elif type_ == \"harmonic\":\n",
    "        res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n",
    "        return 1 / res\n",
    "    elif type_ == \"geometric\":\n",
    "        numerator = np.average(\n",
    "            [np.log(p) for p in predictions], weights=weights, axis=0\n",
    "        )\n",
    "        res = np.exp(numerator / sum(weights))\n",
    "        return res\n",
    "    elif type_ == \"rank\":\n",
    "        from scipy.stats import rankdata\n",
    "        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n",
    "        return res / (len(res) + 1)\n",
    "    return res\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "\n",
    "model_files =[\n",
    "            \"UER_base_tx_ft_5fold-1_2e-05.h5\",\n",
    "            \"UER_base_tx_ft_5fold-1_6e-06.h5\", \n",
    "            \"roberta_base_tx_ft_5fold-1_2e-05.h5\", \n",
    "            \"roberta_large_tx_ft_5fold-1_2e-05.h5\",\n",
    "            \"UER_large_tx_ft_5fold-1_6e-06.h5\",\n",
    "            \"roberta_large_v2_tx_ft_5fold-1_2e-05.h5\",     \n",
    "             ]\n",
    "\n",
    "assert len(model_files) == len(set(model_files)) \n",
    "assert all([os.path.exists(f) for f in model_files]) \n",
    "preds = []\n",
    "O1 = dev_data[\"label\"].values.reshape(-1)\n",
    "for f in model_files:\n",
    "    print(\"-\" * 80)\n",
    "    K.clear_session()\n",
    "    t0 = time()\n",
    "    print(\"[!]\", f)\n",
    "    model = load_model(f, custom_objects=get_custom_objects())\n",
    "    sub_model = get_model(model)\n",
    "    pred = test(sub_model, dev_data)\n",
    "    auc = roc_auc_score(O1, pred)\n",
    "    acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "    print(\"[{}]\".format(time() - t0), auc, acc)\n",
    "    print(\"-\" * 80)\n",
    "    preds.append(pred)\n",
    "    del model\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ensemble_predictions(preds[: 1]  + preds[2: ])\n",
    "print(pred.shape)\n",
    "auc = roc_auc_score(O1, pred)\n",
    "acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "print(auc, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "def get_cnt(data, col1, col2, cate):\n",
    "    data = data[data[\"category\"] == cate]\n",
    "    if col2:\n",
    "        data = data[col1].tolist() + data[col2].tolist() \n",
    "    else:\n",
    "        data = data[col1].tolist()\n",
    "    data = [list(jieba.cut(e)) for e in data]\n",
    "    cnt1 = Counter([w for sent in data for w in sent])\n",
    "    return cnt1\n",
    "    \n",
    "    \n",
    "stop_words = ['？', '吗', '了', '，', '的', '?', '有', '得', '地', '是', '什么',\n",
    "              '怎么办', '哪些', '怎么回事', '怎么', '要', '能', '呢', '会']\n",
    "for cate in train_data[\"category\"].value_counts().index:\n",
    "    print(\"-\" * 40, cate, \"-\" * 40)\n",
    "    cnt1 = get_cnt(train_data, col1=\"query1\", col2=\"query2\", cate=cate)\n",
    "    cnt1 = [(k, cnt) for k, cnt in cnt1.most_common() if k not in stop_words]\n",
    "    print(cnt1[: 20])\n",
    "    print(\"-\" * 81)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in dev_data[\"category\"].value_counts().index:\n",
    "    print(\"------------------------\")\n",
    "    print(cate)\n",
    "    cnt1 = get_cnt(dev_data, col=\"query1\", cate=cate)\n",
    "    cnt2 = get_cnt(dev_data, col=\"query2\", cate=cate)\n",
    "    print(cnt1.most_common(10))\n",
    "    print(cnt2.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _foo(x, col):\n",
    "    cate = x[\"category\"]\n",
    "    q = x[col]\n",
    "    if cate == \"咳血\":\n",
    "        return q.count(\"咯血\") + q.count(\"咳血\")\n",
    "    \n",
    "    \n",
    "    return q.count(cate)\n",
    "\n",
    "\n",
    "    \n",
    "train_data[\"q1_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query1\"), axis=1)\n",
    "train_data[\"q2_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query2\"), axis=1)\n",
    "train_data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
