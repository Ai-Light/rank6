{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from random import choice, seed, randint, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, CuDNNGRU as GRU, CuDNNLSTM as LSTM, Dropout, BatchNormalization\n",
    "from keras.layers import Dense, Concatenate, Activation, Embedding, SpatialDropout1D, Bidirectional, Lambda, Conv1D\n",
    "from keras.layers import Add, Average\n",
    "from keras.optimizers import Nadam, Adam, Adamax\n",
    "from keras.activations import absolute_import\n",
    "from keras.legacy import interfaces\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold as KF\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from keras_bert.loader import load_trained_model_from_checkpoint\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.objectives import sparse_categorical_crossentropy\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from special_tokens import CHINESE_MAP\n",
    "from metric_utils import compute_f1, compute_exact\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = \"../../../chinese_bert/RoBERTa-large-pair/\"\n",
    "TRN_FILENAME = \"../data/train_20200228.csv\"\n",
    "DEV_FILENAME = \"../data/dev_20200228.csv\"\n",
    "PREFIX = \"roberta_large_v2_augm\"\n",
    "MAX_EPOCH = 20\n",
    "MAX_LEN = 60\n",
    "MAX_DOC_LEN = MAX_LEN // 2\n",
    "THRE = 0.5\n",
    "B_SIZE = 32\n",
    "ACCUM_STEP = int(32 // B_SIZE)\n",
    "FOLD_ID = [-1]\n",
    "FOLD_NUM = 25\n",
    "SEED = 2020\n",
    "PREFIX += \"_seed\" + str(SEED)\n",
    "SHUFFLE = True\n",
    "DOC_STRIDE = 128\n",
    "cfg = {}\n",
    "cfg[\"verbose\"] = PREFIX\n",
    "cfg[\"base_dir\"] = BERT_PRETRAINED_DIR\n",
    "cfg[\"span_mode\"] = True\n",
    "cfg[\"lr\"] = 6e-6\n",
    "cfg['min_lr'] = 6e-8 \n",
    "cfg[\"ch_type\"] = \"tx_ft\"\n",
    "cfg[\"trainable\"] = True\n",
    "cfg[\"bert_trainable\"] = True\n",
    "cfg[\"accum_step\"] = ACCUM_STEP\n",
    "cfg[\"cls_num\"] = 4\n",
    "cfg[\"unit1\"] = 128\n",
    "cfg[\"unit2\"] = 128\n",
    "cfg[\"unit3\"] = 512\n",
    "cfg[\"conv_num\"] = 128\n",
    "cfg['maxlen'] = MAX_LEN\n",
    "cfg[\"adv_training\"] = False\n",
    "\n",
    "train_data = pd.read_csv(TRN_FILENAME)\n",
    "train_data.fillna(\"\", inplace=True)\n",
    "dev_data = pd.read_csv(DEV_FILENAME)\n",
    "dev_data.fillna(\"\", inplace=True)\n",
    "all_data = pd.concat([train_data, dev_data], axis=0, ignore_index=True)\n",
    "\n",
    "def get_data(df_data):\n",
    "\n",
    "    df_gb = df_data.groupby('query1')\n",
    "    res = {}\n",
    "    for index, data in df_gb:\n",
    "        query2s = data[\"query2\"]\n",
    "        lables = data[\"label\"]\n",
    "        ele = {}\n",
    "        pos_qs = []\n",
    "        neg_qs = []\n",
    "        for q, lable in zip(query2s, lables):\n",
    "            if lable == 1:\n",
    "                pos_qs.append(q)\n",
    "            elif lable == 0:\n",
    "                neg_qs.append(q)\n",
    "            else:\n",
    "                print(\"wrong data\", index, q, lable)\n",
    "        ele[\"pos\"] = pos_qs\n",
    "        ele[\"neg\"] = neg_qs\n",
    "        res[index] = ele\n",
    "    return res\n",
    "\n",
    "# train_data_dict = get_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1} {0: '0', 1: '1'} ['0', '1']\n"
     ]
    }
   ],
   "source": [
    "def get_vocab():\n",
    "    if \"albert\"in cfg[\"verbose\"].lower():\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab_chinese.txt')\n",
    "    else:\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "    with open(dict_path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "\n",
    "    word_index = {v: k  for k, v in enumerate(lines)}\n",
    "    for k, v in CHINESE_MAP.items():\n",
    "        assert v in word_index\n",
    "        del word_index[v]\n",
    "    return word_index\n",
    "\n",
    "\n",
    "def get_label():\n",
    "    labels = [\"0\", \"1\"]\n",
    "    label2id = {k: v for v, k in enumerate(labels)}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    return label2id, id2label, labels\n",
    "    \n",
    "    \n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=np.float16)\n",
    "\n",
    "\n",
    "def load_embed(path, dim=300, word_index=None):\n",
    "    embedding_index = {}\n",
    "    with open(path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.strip().split()\n",
    "            word, arr = l[0], l[1:]\n",
    "            if len(arr) != dim:\n",
    "                print(\"[!] l = {}\".format(l))\n",
    "                continue\n",
    "            if word_index and word not in word_index:\n",
    "                continue\n",
    "            word, arr = get_coefs(word, arr)\n",
    "            embedding_index[word] = arr\n",
    "    return embedding_index\n",
    "\n",
    "\n",
    "def build_matrix(path, word_index=None, max_features=None, dim=300):\n",
    "    embedding_index = load_embed(path, dim=dim, word_index=word_index)\n",
    "    max_features = len(word_index) + 1 if max_features is None else max_features \n",
    "    embedding_matrix = np.zeros((max_features + 1, dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_word_embed(word_embed_f1=\"../../../chinese_embedding/Tencent_AILab_ChineseEmbedding.txt\", \n",
    "               word_embed_f2=\"../../../chinese_embedding/cc.zh.300.vec\", \n",
    "               save_filename=\"./word_embedding_matrix\",\n",
    "               word_index=None):\n",
    "    if os.path.exists(save_filename + \".npy\"):\n",
    "        word_embedding_matrix = np.load(save_filename + \".npy\").astype(\"float32\")\n",
    "    else:\n",
    "        if \"tx\" in cfg[\"ch_type\"]:\n",
    "            tx_embed, tx_unk = build_matrix(word_embed_f1, word_index=word_index, dim=200)\n",
    "        else:\n",
    "            tx_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            tx_unk = []\n",
    "        if \"ft\" in cfg[\"ch_type\"]:\n",
    "            ft_embed, ft_unk = build_matrix(word_embed_f2, word_index=word_index, dim=300)\n",
    "        else:\n",
    "            ft_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            ft_unk = []    \n",
    "\n",
    "        word_embedding_matrix = np.concatenate([tx_embed, ft_embed], axis=-1).astype(\"float32\")\n",
    "        print(word_embedding_matrix.shape, len(tx_unk), len(ft_unk))\n",
    "        np.save(save_filename, word_embedding_matrix )\n",
    "    return word_embedding_matrix\n",
    "    \n",
    "    \n",
    "word_index = get_vocab()\n",
    "label2id, id2label, labels = get_label()\n",
    "word_embedding_matrix = load_word_embed(word_index=word_index)\n",
    "\n",
    "NUM_CLASS = len(label2id)\n",
    "cfg[\"x_pad\"] = word_index[\"[PAD]\"]\n",
    "cfg[\"num_class\"] = NUM_CLASS\n",
    "cfg[\"filename\"] = \"{}_{}_{}_{}\".format(PREFIX, cfg[\"ch_type\"], FOLD_NUM, cfg[\"lr\"])\n",
    "cfg[\"filename\"] = cfg[\"filename\"] + \"_adv_training\" if cfg[\"adv_training\"] else cfg[\"filename\"]\n",
    "print(label2id, id2label, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(cfg, summary=False, word_embedding_matrix=None):\n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        if \"albert\"in cfg[\"verbose\"].lower():\n",
    "            from bert4keras.bert import build_bert_model\n",
    "            config_file = os.path.join(base_dir, 'albert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'model.ckpt-best')\n",
    "            model = build_bert_model(\n",
    "                    config_path=config_file,\n",
    "                    checkpoint_path=checkpoint_file,\n",
    "                    model='albert',\n",
    "                    return_keras_model=True\n",
    "            )\n",
    "            if cfg_[\"cls_num\"] > 1:\n",
    "                output = Concatenate(axis=-1)([model.get_layer(\"Encoder-1-FeedForward-Norm\").get_output_at(-i) for i in range(1, cfg[\"cls_num\"] + 1)])\n",
    "                model = Model(model.inputs[: 2], outputs=output)\n",
    "            model.trainable = cfg_[\"bert_trainable\"]\n",
    "        else:\n",
    "            config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "            if not os.path.exists(config_file):\n",
    "                config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "                checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')            \n",
    "            model = load_trained_model_from_checkpoint(config_file, \n",
    "                                                       checkpoint_file, \n",
    "                                                       training=False, \n",
    "                                                       trainable=cfg_[\"bert_trainable\"], \n",
    "                                                       output_layer_num=cfg_[\"cls_num\"],\n",
    "                                                       seq_len=cfg_['maxlen'])\n",
    "            \n",
    "            # model = Model(inputs=model.inputs[: 2], outputs=model.layers[-7].output)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def _get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        total_steps, warmup_steps = calc_train_steps(\n",
    "            num_example=num_example,\n",
    "            batch_size=B_SIZE,\n",
    "            epochs=MAX_EPOCH,\n",
    "            warmup_proportion=warmup_proportion,\n",
    "        )\n",
    "        opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "        if cfg.get(\"accum_step\", None) and cfg[\"accum_step\"] > 1:\n",
    "            print(\"[!] using accum_step = {}\".format(cfg[\"accum_step\"]))\n",
    "            from accum_optimizer import AccumOptimizer\n",
    "            opt = AccumOptimizer(opt, steps_per_update=cfg[\"accum_step\"])\n",
    "        \n",
    "        return opt\n",
    "\n",
    "    bert_model = _get_model(cfg[\"base_dir\"], cfg)\n",
    "\n",
    "    if word_embedding_matrix is not None:\n",
    "        embed = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                          output_dim=word_embedding_matrix.shape[1],\n",
    "                          weights=[word_embedding_matrix],\n",
    "                          trainable=cfg[\"trainable\"],\n",
    "                          name=\"char_embed\"\n",
    "                         )\n",
    "    \n",
    "    t1_in = Input(shape=(None, ))\n",
    "    t2_in = Input(shape=(None, ))\n",
    "    o1_in = Input(shape=(1, ))\n",
    "    o2_in = Input(shape=(1, ))\n",
    "\n",
    "    t1, t2, o1, o2 = t1_in, t2_in, o1_in, o2_in\n",
    "    \n",
    "    ## Char information\n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'))(t1)\n",
    "    word_embed = embed(t1)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    word_embed = Bidirectional(LSTM(cfg[\"unit1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    \n",
    "    t = bert_model([t1, t2])\n",
    "    t = Concatenate(axis=-1)([t, word_embed])\n",
    "    t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask]) \n",
    "    t = Bidirectional(LSTM(cfg[\"unit3\"], return_sequences=True), merge_mode=\"concat\")(t)\n",
    "    # t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask]) \n",
    "    # t = Conv1D(cfg[\"conv_num\"], kernel_size=3, padding=\"same\")(t) \n",
    "    t = Lambda(lambda x: x[:, 0, :], name=\"extract_layer\")(t)\n",
    "    if cfg.get(\"num_class\", 1) == 2:\n",
    "        po1_logit = Dense(1, name=\"po1_logit\")(t)\n",
    "        po1 = Activation('sigmoid', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])        \n",
    "        o1_loss = K.binary_crossentropy(o1, po1)\n",
    "        loss = K.mean(o1_loss)\n",
    "    else:\n",
    "        po1_logit = Dense(cfg[\"num_class\"], name=\"po1_logit\")(t)\n",
    "        po1 = Activation('softmax', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])\n",
    "        loss = K.categorical_crossentropy(o1, po1, axis=-1)\n",
    "        loss = K.mean(loss)\n",
    "\n",
    "    train_model.add_loss(loss)\n",
    "    opt = _get_opt(num_example=cfg[\"num_example\"], lr=cfg[\"lr\"], min_lr=cfg['min_lr'])\n",
    "    train_model.compile(optimizer=opt)\n",
    "    if summary:\n",
    "        train_model.summary()\n",
    "    return train_model\n",
    "\n",
    "\n",
    "# print(\"----------------build model ---------------\")\n",
    "# model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2id_X(x, x_dict, x2=None, maxlen=None, maxlen1=None):\n",
    "    if x2:\n",
    "        x1 = x\n",
    "        del x\n",
    "        maxlen -= 3\n",
    "        maxlen1 -= 2\n",
    "        assert maxlen > maxlen1\n",
    "        maxlen2 = maxlen - maxlen1 - 1\n",
    "        x1 = [\"[CLS]\"] + list(x1)[: maxlen1] + [\"[SEP]\"] \n",
    "        x1 = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x1]\n",
    "        seg1= [0 for _ in x1]\n",
    "        \n",
    "        x2 = list(x2)[: maxlen2] + [\"[SEP]\"] \n",
    "        x2= [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x2]\n",
    "        seg2 = [1 for _ in x2]\n",
    "        x = x1 + x2\n",
    "        seg = seg1 + seg2\n",
    "        \n",
    "    else:\n",
    "        maxlen -= 2\n",
    "        x = [\"[CLS]\"] + list(x)[: maxlen] + [\"[SEP]\"] \n",
    "        x = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x]\n",
    "        seg = [0 for _ in x]        \n",
    "    return x, seg\n",
    "\n",
    "\n",
    "def seq_padding(X, maxlen=None, padding_value=None, debug=False):\n",
    "    L = [len(x) for x in X]\n",
    "    if maxlen is None:\n",
    "        maxlen = max(L)\n",
    "\n",
    "    pad_X = np.array([\n",
    "        np.concatenate([x, [padding_value] * (maxlen - len(x))]) if len(x) < maxlen else x for x in X\n",
    "    ])\n",
    "    if debug:\n",
    "        print(\"[!] before pading {}\\n\".format(X))\n",
    "        print(\"[!] after pading {}\\n\".format(pad_X))\n",
    "    return pad_X\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    \n",
    "    def __init__(self, data, batch_size=B_SIZE, shuffle=SHUFFLE, augm_frac=0.75):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = cfg[\"num_example\"] // self.batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_dict = get_data(data)\n",
    "        self.augm_frac = augm_frac\n",
    "        if cfg[\"num_example\"] % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "            T1, T2, O1, O2 = [], [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data.iloc[i]\n",
    "                text = d[\"query1\"]\n",
    "                label_text = d[\"query2\"]\n",
    "                o1 = d[\"label\"]\n",
    "                \n",
    "                if random() > self.augm_frac:\n",
    "                    data_d = self.data_dict[text]\n",
    "                    pos_data = data_d[\"pos\"]\n",
    "                    neg_data = data_d[\"neg\"]\n",
    "                    if pos_data and neg_data:\n",
    "                        if random() > 0.5:\n",
    "                            o1 = 1\n",
    "                            label_text = choice(pos_data)\n",
    "                            if len(pos_data) >= 2:\n",
    "                                _pos_data = [e for e in pos_data if e != label_text]\n",
    "                                text = choice(_pos_data)\n",
    "                        else:\n",
    "                            o1 = 0\n",
    "                            text = choice(pos_data)\n",
    "                            label_text = choice(neg_data)   \n",
    "                \n",
    "                if random() > 0.5:\n",
    "                    text, label_text = label_text, text\n",
    "                \n",
    "                if o1 == \"\":\n",
    "                    continue\n",
    "                o1 = float(o1)\n",
    "                assert 0 <= o1 <= 1\n",
    "                \n",
    "                O1.append(o1)                \n",
    "                t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "                assert len(t1) == len(t2)\n",
    "                \n",
    "                T1.append(t1)\n",
    "                T2.append(t2)\n",
    "\n",
    "                if len(T1) == self.batch_size or i == idxs[-1]:\n",
    "                    O1 = np.array(O1).reshape(-1, 1)\n",
    "                    T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "                    T2 = seq_padding(T2, padding_value=0)\n",
    "                    assert T1.shape == T2.shape and T1.shape[0] == O1.shape[0]\n",
    "\n",
    "                    yield [T1, T2, O1], None\n",
    "                    T1, T2, O1, = [], [], []\n",
    "                    \n",
    "                        \n",
    "# gen = data_generator(train_data)\n",
    "# for i, e in enumerate(gen):\n",
    "#     if i > 400:\n",
    "#         break\n",
    "#     print(\"i = {}\".format(i), \"-\" * 81)\n",
    "#     # print(e[0])\n",
    "#     for _e in e[0]:\n",
    "#         print(_e.shape, _e.sum(axis=0).sum(axis=0))\n",
    "# del gen\n",
    "# print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_):\n",
    "    model_inp_ind = [0, 1]\n",
    "    inputs = [model_.inputs[e] for e in model_inp_ind]\n",
    "    sub_model = Model(inputs=inputs, outputs=[model_.get_layer(\"po1\").output])\n",
    "    return sub_model\n",
    "\n",
    "\n",
    "def evaluate(sub_model, data, bs=32):\n",
    "    idxs = list(range(len(data)))\n",
    "    T1, T2, O1, O2 = [], [], [], []\n",
    "    preds = []\n",
    "    for i in idxs:\n",
    "        d = data.iloc[i]\n",
    "        text = d[\"query1\"]\n",
    "        label_text = d[\"query2\"]\n",
    "\n",
    "        t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "        assert len(t1) == len(t2)\n",
    "\n",
    "        T1.append(t1)\n",
    "        T2.append(t2)\n",
    "\n",
    "        o1 = float(d[\"label\"])\n",
    "        O1.append(o1)\n",
    "        if len(T1) == bs or i == idxs[-1]:\n",
    "            T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "            T2 = seq_padding(T2, padding_value=0)\n",
    "            assert T1.shape == T2.shape\n",
    "            pred = sub_model.predict([T1, T2])\n",
    "            preds.append(pred)\n",
    "            T1, T2 = [], []\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    O1 = np.array(O1).reshape(-1)\n",
    "    O1 = O1.astype(\"int32\")\n",
    "    auc = roc_auc_score(O1, preds)\n",
    "    acc = accuracy_score(O1, np.array(preds > 0.5, \"int32\"))\n",
    "    return auc, acc\n",
    "    \n",
    "\n",
    "class Evaluate(Callback):\n",
    "    def __init__(self, data, filename=None):\n",
    "        self.F1 = []\n",
    "        self.best = 0.\n",
    "        self.filename = filename\n",
    "        self.data = data\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch ==  0:\n",
    "            print(\"[!] test load&save model\")\n",
    "            f = self.filename + \".h5\"\n",
    "            self.model.save(f, include_optimizer=False, overwrite=False)\n",
    "            if \"albert\" in cfg[\"verbose\"]:\n",
    "                model_ = load_model(f) \n",
    "            else:\n",
    "                model_ = load_model(f, custom_objects=get_custom_objects()) \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch + 1 < 1:\n",
    "            return\n",
    "        if epoch + 1 in [3, 6, 9, 10, 12, 15, 18, 20]:\n",
    "            self.model.save(self.filename + \"_{}.h5\".format(epoch + 1), include_optimizer=False)\n",
    "            \n",
    "        sub_model = get_model(self.model)\n",
    "        f1, class_f1 = evaluate(sub_model, data=self.data)\n",
    "        self.F1.append(f1)\n",
    "        if f1 > self.best:\n",
    "            self.model.save(self.filename + \".h5\", include_optimizer=False)\n",
    "            \n",
    "        if f1 > self.best:\n",
    "            self.best = f1\n",
    "            print(\"[!] epoch = {}, new best_auc = {}\".format(epoch + 1,  f1))\n",
    "        print('[!] epoch = {}, auc = {}, best auc {}'.format(epoch + 1, f1, self.best))\n",
    "        print('[!] epoch = {}, acc = {}\\n'.format(epoch + 1, class_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_layer(inputs, name, exclude_from=None):\n",
    "    \"\"\"根据inputs和name来搜索层\n",
    "    说明：inputs为某个层或某个层的输出；name为目标层的名字。\n",
    "    实现：根据inputs一直往上递归搜索，直到发现名字为name的层为止；\n",
    "         如果找不到，那就返回None。\n",
    "    \"\"\"\n",
    "    if exclude_from is None:\n",
    "        exclude_from = set()\n",
    "\n",
    "    if isinstance(inputs, keras.layers.Layer):\n",
    "        layer = inputs\n",
    "    else:\n",
    "        layer = inputs._keras_history[0]\n",
    "\n",
    "    if layer.name == name:\n",
    "        return layer\n",
    "    elif layer in exclude_from:\n",
    "        return None\n",
    "    else:\n",
    "        exclude_from.add(layer)\n",
    "        if isinstance(layer, keras.models.Model):\n",
    "            model = layer\n",
    "            for layer in model.layers:\n",
    "                if layer.name == name:\n",
    "                    return layer\n",
    "        inbound_layers = layer._inbound_nodes[0].inbound_layers\n",
    "        if not isinstance(inbound_layers, list):\n",
    "            inbound_layers = [inbound_layers]\n",
    "        if len(inbound_layers) > 0:\n",
    "            for layer in inbound_layers:\n",
    "                layer = search_layer(layer, name, exclude_from)\n",
    "                if layer is not None:\n",
    "                    return layer\n",
    "                \n",
    "def adversarial_training(model, embedding_names, epsilon=1):\n",
    "    \"\"\"给模型添加对抗训练\n",
    "    其中model是需要添加对抗训练的keras模型，embedding_names\n",
    "    则是model里边Embedding层的名字。要在模型compile之后使用。\n",
    "    \"\"\"\n",
    "    if model.train_function is None:  # 如果还没有训练函数\n",
    "        model._make_train_function()  # 手动make\n",
    "    old_train_function = model.train_function  # 备份旧的训练函数\n",
    "\n",
    "    # 查找Embedding层\n",
    "    embedding_layers = []\n",
    "    for embedding_name in embedding_names:\n",
    "        for output in model.outputs:\n",
    "            embedding_layer = search_layer(output, embedding_name)\n",
    "            if embedding_layer is not None:\n",
    "                embedding_layers.append(embedding_layer)\n",
    "                break\n",
    "    for embedding_layer in embedding_layers:\n",
    "        if embedding_layer is None:\n",
    "            raise Exception('Embedding layer not found')\n",
    "\n",
    "    # 求Embedding梯度\n",
    "    embeddings = [embedding_layer.embeddings for embedding_layer in embedding_layers] # Embedding矩阵\n",
    "    gradients = K.gradients(model.total_loss, embeddings)  # Embedding梯度\n",
    "    # gradients = K.zeros_like(embeddings) + gradients[0]  # 转为dense tensor\n",
    "    gradients = [K.zeros_like(embedding) + gradient for embedding, gradient in zip(embeddings, gradients)]\n",
    "\n",
    "    # 封装为函数\n",
    "    inputs = (model._feed_inputs +\n",
    "              model._feed_targets +\n",
    "              model._feed_sample_weights)  # 所有输入层\n",
    "    embedding_gradients = K.function(\n",
    "        inputs=inputs,\n",
    "        outputs=gradients,\n",
    "        name='embedding_gradients',\n",
    "    )  # 封装为函数\n",
    "\n",
    "    def train_function(inputs):  # 重新定义训练函数\n",
    "#         grads = embedding_gradients(inputs)[0]  # Embedding梯度\n",
    "#         delta = epsilon * grads / (np.sqrt((grads**2).sum()) + 1e-8)  # 计算扰动\n",
    "        grads = embedding_gradients(inputs)  # Embedding梯度\n",
    "        deltas = [epsilon * grad / (np.sqrt((grad**2).sum()) + 1e-8) for grad in grads]  # 计算扰动\n",
    "        # 注入扰动\n",
    "        # K.set_value(embeddings, K.eval(embeddings) + delta)  \n",
    "        for embedding, delta in zip(embeddings, deltas):\n",
    "            K.set_value(embedding, K.eval(embedding) + delta)\n",
    "            \n",
    "        outputs = old_train_function(inputs)  # 梯度下降\n",
    "        # 删除扰动\n",
    "        # K.set_value(embeddings, K.eval(embeddings) - delta)  # 删除扰动\n",
    "        for embedding, delta in zip(embeddings, deltas):\n",
    "            K.set_value(embedding, K.eval(embedding) - delta)       \n",
    "        return outputs\n",
    "\n",
    "    model.train_function = train_function  # 覆盖原训练函数\n",
    "\n",
    "\n",
    "# 写好函数后，启用对抗训练只需要一行代码\n",
    "# adversarial_training(model, 'Embedding-Token', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "[!] start fold_id = -1 (8747, 5) (2002, 5)\n",
      "{'verbose': 'roberta_large_v2_augm_seed2020', 'base_dir': '../../../chinese_bert/RoBERTa-large-pair/', 'span_mode': True, 'lr': 2e-05, 'min_lr': 6e-08, 'ch_type': 'tx_ft', 'trainable': True, 'bert_trainable': True, 'accum_step': 1, 'cls_num': 4, 'unit1': 128, 'unit2': 128, 'unit3': 512, 'conv_num': 128, 'maxlen': 60, 'adv_training': False, 'x_pad': 0, 'num_class': 2, 'filename': 'roberta_large_v2_augm_seed2020_tx_ft_25_2e-05', 'num_example': 8747}\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-efa56d8eaaf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfold_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfold_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_embedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"adv_training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[!] using adv_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b70eab2e5204>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(cfg, summary, word_embedding_matrix)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"base_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword_embedding_matrix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b70eab2e5204>\u001b[0m in \u001b[0;36m_get_model\u001b[0;34m(base_dir, cfg_)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                        \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bert_trainable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                                        \u001b[0moutput_layer_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cls_num\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                                        seq_len=cfg_['maxlen'])\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# model = Model(inputs=model.inputs[: 2], outputs=model.layers[-7].output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras_bert/loader.py\u001b[0m in \u001b[0;36mload_trained_model_from_checkpoint\u001b[0;34m(config_file, checkpoint_file, training, trainable, output_layer_num, seq_len, use_adapter, adapter_units)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0madapter_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madapter_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mload_model_weights_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras_bert/loader.py\u001b[0m in \u001b[0;36mload_model_weights_from_checkpoint\u001b[0;34m(model, config, checkpoint_file, training)\u001b[0m\n\u001b[1;32m    126\u001b[0m         model.get_layer(name='Encoder-%d-FeedForward-Norm' % (i + 1)).set_weights([\n\u001b[1;32m    127\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert/encoder/layer_%d/output/LayerNorm/gamma'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert/encoder/layer_%d/output/LayerNorm/beta'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         ])\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1057\u001b[0m                                  'provided weight shape ' + str(w.shape))\n\u001b[1;32m   1058\u001b[0m             \u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2468\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2470\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adv_layer_names = ['Embedding-Token', 'char_embed']\n",
    "\n",
    "if -1 in FOLD_ID:\n",
    "    fold_id = -1\n",
    "    cfg[\"num_example\"] = len(train_data)\n",
    "    print(\"-\" * 81)\n",
    "    print(\"[!] start fold_id =\", fold_id, train_data.shape, dev_data.shape)\n",
    "    print(cfg)\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    train_D = data_generator(train_data)\n",
    "    seed(SEED + fold_id)\n",
    "    np.random.seed(SEED + fold_id)\n",
    "    tf.random.set_random_seed(SEED + fold_id)\n",
    "    model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "    if cfg[\"adv_training\"]:\n",
    "        print(\"[!] using adv_training\")\n",
    "        adversarial_training(model, adv_layer_names, 0.5)\n",
    "    evaluator = Evaluate(filename=cfg[\"filename\"] + \"_fold{}\".format(fold_id), data=dev_data)\n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                              steps_per_epoch=len(train_D),\n",
    "                              epochs=MAX_EPOCH,\n",
    "                              callbacks=[evaluator],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    del model, train_data, dev_data\n",
    "    gc.collect()\n",
    "    print(\"[!] finish fold_id =\", fold_id)\n",
    "    print(\"-\" * 81)\n",
    "\n",
    "\n",
    "skf = SKF(FOLD_NUM, shuffle=True, random_state=SEED)\n",
    "print(all_data.shape)\n",
    "for fold_id, (trn_ind, val_ind) in enumerate(skf.split(range(len(all_data)), all_data[\"label\"])):\n",
    "    if fold_id not in FOLD_ID:\n",
    "        continue\n",
    "    \n",
    "    dev_data = all_data.iloc[val_ind].reset_index(drop=True)\n",
    "    train_data = all_data.iloc[trn_ind].reset_index(drop=True)\n",
    "    cfg[\"num_example\"] = len(train_data)\n",
    "    print(\"-\" * 81)\n",
    "    print(\"[!] start fold_id =\", fold_id, train_data.shape, dev_data.shape)\n",
    "    print(cfg)\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    train_D = data_generator(train_data)\n",
    "    seed(SEED + fold_id)\n",
    "    np.random.seed(SEED + fold_id)\n",
    "    tf.random.set_random_seed(SEED + fold_id)\n",
    "    model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "    if cfg[\"adv_training\"]:\n",
    "        print(\"[!] using adv_training\")\n",
    "        adversarial_training(model, adv_layer_names, 0.5)\n",
    "    evaluator = Evaluate(filename=cfg[\"filename\"] + \"_fold{}\".format(fold_id), data=dev_data)\n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                              steps_per_epoch=len(train_D),\n",
    "                              epochs=MAX_EPOCH,\n",
    "                              callbacks=[evaluator],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    del model, train_data, dev_data\n",
    "    gc.collect()\n",
    "    print(\"[!] finish fold_id =\", fold_id)\n",
    "    print(\"-\" * 81)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.F1, max(evaluator.F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(sub_model, data, bs=32):\n",
    "    idxs = list(range(len(data)))\n",
    "    T1, T2, O1, O2 = [], [], [], []\n",
    "    preds = []\n",
    "    for i in idxs:\n",
    "        d = data.iloc[i]\n",
    "        text = d[\"query1\"]\n",
    "        label_text = d[\"query2\"]\n",
    "\n",
    "        t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "        assert len(t1) == len(t2)\n",
    "\n",
    "        T1.append(t1)\n",
    "        T2.append(t2)\n",
    "\n",
    "        if len(T1) == bs or i == idxs[-1]:\n",
    "            T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "            T2 = seq_padding(T2, padding_value=0)\n",
    "            assert T1.shape == T2.shape\n",
    "            pred = sub_model.predict([T1, T2])\n",
    "            preds.append(pred)\n",
    "            T1, T2 = [], []\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def ensemble_predictions(predictions, weights=None, type_=\"linear\"):\n",
    "    if not weights:\n",
    "        # print(\"[!] AVE_WGT\")\n",
    "        weights = [1./ len(predictions) for _ in range(len(predictions))]\n",
    "    assert len(predictions) == len(weights)\n",
    "    if np.sum(weights) != 1.0:\n",
    "        weights = [w / np.sum(weights) for w in weights]\n",
    "    # print(\"[!] weights = {}\".format(weights))\n",
    "    assert np.isclose(np.sum(weights), 1.0)\n",
    "    if type_ == \"linear\":\n",
    "        res = np.average(predictions, weights=weights, axis=0)\n",
    "    elif type_ == \"harmonic\":\n",
    "        res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n",
    "        return 1 / res\n",
    "    elif type_ == \"geometric\":\n",
    "        numerator = np.average(\n",
    "            [np.log(p) for p in predictions], weights=weights, axis=0\n",
    "        )\n",
    "        res = np.exp(numerator / sum(weights))\n",
    "        return res\n",
    "    elif type_ == \"rank\":\n",
    "        from scipy.stats import rankdata\n",
    "        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n",
    "        return res / (len(res) + 1)\n",
    "    return res\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "\n",
    "model_files =[\n",
    "            \"albert_xxlarge_tx_ft_5fold-1_2e-05.h5\",\n",
    "            \"albert_xxlarge_tx_ft_5fold-1_2e-05_6.h5\",\n",
    "            \"roberta_base_tx_ft_5fold-1_2e-05.h5\", \n",
    "            \"roberta_base_tx_ft_5fold-1_2e-05_10.h5\", \n",
    "            \"roberta_large_tx_ft_5fold-1_2e-05.h5\",\n",
    "            \"roberta_large_tx_ft_5fold-1_2e-05_10.h5\",\n",
    "            \"UER_large_tx_ft_5fold-1_6e-06.h5\",\n",
    "            \"UER_large_tx_ft_5fold-1_6e-06_10.h5\",\n",
    "            \"roberta_large_v2_tx_ft_5fold-1_2e-05.h5\",    \n",
    "            \"roberta_large_v2_tx_ft_5fold-1_2e-05_10.h5\", \n",
    "             ]\n",
    "\n",
    "for f in model_files:\n",
    "    print(f, os.path.exists(f))\n",
    "assert len(model_files) == len(set(model_files)) \n",
    "assert all([os.path.exists(f) for f in model_files]) \n",
    "preds = []\n",
    "O1 = dev_data[\"label\"].values.reshape(-1)\n",
    "for f in model_files:\n",
    "    print(\"-\" * 80)\n",
    "    K.clear_session()\n",
    "    t0 = time()\n",
    "    print(\"[!]\", f)\n",
    "    if \"albert\" in f:\n",
    "        model = load_model(f)\n",
    "    else:\n",
    "        model = load_model(f, custom_objects=get_custom_objects())\n",
    "    sub_model = get_model(model)\n",
    "    pred = test(sub_model, dev_data)\n",
    "    auc = roc_auc_score(O1, pred)\n",
    "    acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "    print(\"[{}]\".format(time() - t0), auc, acc)\n",
    "    print(\"-\" * 80)\n",
    "    preds.append(pred)\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "pred = ensemble_predictions(preds)\n",
    "print(pred.shape)\n",
    "auc = roc_auc_score(O1, pred)\n",
    "acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "print(auc, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred1 = ensemble_predictions(preds[0::2])\n",
    "pred2 = ensemble_predictions(preds[1::2])\n",
    "print(pred2[: 3], pred1[: 3])\n",
    "for i in range(1, 100):\n",
    "    wgt = i / 100\n",
    "    pred = ensemble_predictions([pred1, pred2], weights=[wgt, 1 - wgt], type_=\"geometric\")\n",
    "    auc = roc_auc_score(O1, pred)\n",
    "    acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "    print(wgt, auc, acc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "def get_cnt(data, col1, col2, cate):\n",
    "    data = data[data[\"category\"] == cate]\n",
    "    if col2:\n",
    "        data = data[col1].tolist() + data[col2].tolist() \n",
    "    else:\n",
    "        data = data[col1].tolist()\n",
    "    data = [list(jieba.cut(e)) for e in data]\n",
    "    cnt1 = Counter([w for sent in data for w in sent])\n",
    "    return cnt1\n",
    "    \n",
    "    \n",
    "stop_words = ['？', '吗', '了', '，', '的', '?', '有', '得', '地', '是', '什么',\n",
    "              '怎么办', '哪些', '怎么回事', '怎么', '要', '能', '呢', '会']\n",
    "for cate in train_data[\"category\"].value_counts().index:\n",
    "    print(\"-\" * 40, cate, \"-\" * 40)\n",
    "    cnt1 = get_cnt(train_data, col1=\"query1\", col2=\"query2\", cate=cate)\n",
    "    cnt1 = [(k, cnt) for k, cnt in cnt1.most_common() if k not in stop_words]\n",
    "    print(cnt1[: 20])\n",
    "    print(\"-\" * 81)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in dev_data[\"category\"].value_counts().index:\n",
    "    print(\"------------------------\")\n",
    "    print(cate)\n",
    "    cnt1 = get_cnt(dev_data, col=\"query1\", cate=cate)\n",
    "    cnt2 = get_cnt(dev_data, col=\"query2\", cate=cate)\n",
    "    print(cnt1.most_common(10))\n",
    "    print(cnt2.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _foo(x, col):\n",
    "    cate = x[\"category\"]\n",
    "    q = x[col]\n",
    "    if cate == \"咳血\":\n",
    "        return q.count(\"咯血\") + q.count(\"咳血\")\n",
    "    \n",
    "    \n",
    "    return q.count(cate)\n",
    "\n",
    "\n",
    "    \n",
    "train_data[\"q1_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query1\"), axis=1)\n",
    "train_data[\"q2_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query2\"), axis=1)\n",
    "train_data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2000 * 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
