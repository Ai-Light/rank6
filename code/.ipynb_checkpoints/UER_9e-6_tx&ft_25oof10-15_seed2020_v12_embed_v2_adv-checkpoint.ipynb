{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from random import choice, seed, randint, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, CuDNNGRU as GRU, CuDNNLSTM as LSTM, Dropout, BatchNormalization\n",
    "from keras.layers import Dense, Concatenate, Activation, Embedding, SpatialDropout1D, Bidirectional, Lambda, Conv1D\n",
    "from keras.layers import Add, Average\n",
    "from keras.optimizers import Nadam, Adam, Adamax\n",
    "from keras.activations import absolute_import\n",
    "from keras.legacy import interfaces\n",
    "# from keras.preprocessing.sequence import pad_sequaencesget_\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold as KF\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from keras_bert.loader import load_trained_model_from_checkpoint\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.objectives import sparse_categorical_crossentropy\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from special_tokens import CHINESE_MAP\n",
    "from metric_utils import compute_f1, compute_exact\n",
    "from collections import OrderedDict, Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = \"../../../chinese_bert/UER-py/UER-large/\"\n",
    "TRN_FILENAME = \"../data/train_20200228.csv\"\n",
    "DEV_FILENAME = \"../data/dev_20200228.csv\"\n",
    "SAVE_DIR = \"../save_models/\"\n",
    "PREFIX = \"USE_v12_augm\"\n",
    "if \"large-clue\" in BERT_PRETRAINED_DIR or \"large-pair\" in BERT_PRETRAINED_DIR:\n",
    "    W2V_FILE = \"./word_embedding_matrix_v2\"\n",
    "else:\n",
    "    W2V_FILE = \"./word_embedding_matrix\"\n",
    "MAX_EPOCH = 15\n",
    "RUN_EPOCH = 10\n",
    "MAX_LEN = 60\n",
    "MAX_DOC_LEN = MAX_LEN // 2\n",
    "THRE = 0.5\n",
    "B_SIZE = 32\n",
    "ACCUM_STEP = int(32 // B_SIZE)\n",
    "FOLD_ID = list(range(10, 15))\n",
    "FOLD_NUM = 25\n",
    "SEED = 2020\n",
    "\n",
    "SHUFFLE = True\n",
    "DOC_STRIDE = 128\n",
    "cfg = {}\n",
    "\n",
    "cfg[\"base_dir\"] = BERT_PRETRAINED_DIR\n",
    "cfg[\"span_mode\"] = True\n",
    "cfg[\"lr\"] = 9e-6\n",
    "cfg['min_lr'] = 6e-8 \n",
    "cfg[\"ch_type\"] = \"tx_ft\"\n",
    "cfg[\"trainable\"] = True\n",
    "cfg[\"bert_trainable\"] = True\n",
    "cfg[\"accum_step\"] = ACCUM_STEP\n",
    "cfg[\"cls_num\"] = 4\n",
    "cfg[\"unit1\"] = 128\n",
    "cfg[\"unit2\"] = 128\n",
    "cfg[\"unit3\"] = 512\n",
    "cfg[\"conv_num\"] = 128\n",
    "cfg['maxlen'] = MAX_LEN\n",
    "cfg[\"adv_training\"] = True\n",
    "cfg[\"W2V_FILE\"] = W2V_FILE\n",
    "cfg[\"use_embed\"] = True\n",
    "cfg[\"use_embed_v2\"] = True\n",
    "PREFIX += \"_seed\" + str(SEED)\n",
    "cfg[\"verbose\"] = PREFIX\n",
    "PREFIX = PREFIX + \"_embed_v2\" if cfg[\"use_embed_v2\"] else PREFIX\n",
    "\n",
    "train_data = pd.read_csv(TRN_FILENAME)\n",
    "train_data.fillna(\"\", inplace=True)\n",
    "dev_data = pd.read_csv(DEV_FILENAME)\n",
    "dev_data.fillna(\"\", inplace=True)\n",
    "all_data = pd.concat([train_data, dev_data], axis=0, ignore_index=True)\n",
    "\n",
    "def get_data(df_data):\n",
    "\n",
    "    df_gb = df_data.groupby('query1')\n",
    "    res = {}\n",
    "    for index, data in df_gb:\n",
    "        query2s = data[\"query2\"]\n",
    "        lables = data[\"label\"]\n",
    "        ele = {}\n",
    "        pos_qs = []\n",
    "        neg_qs = []\n",
    "        for q, lable in zip(query2s, lables):\n",
    "            if lable == 1:\n",
    "                pos_qs.append(q)\n",
    "            elif lable == 0:\n",
    "                neg_qs.append(q)\n",
    "            else:\n",
    "                print(\"wrong data\", index, q, lable)\n",
    "        ele[\"pos\"] = pos_qs\n",
    "        ele[\"neg\"] = neg_qs\n",
    "        res[index] = ele\n",
    "    return res\n",
    "\n",
    "# train_data_dict = get_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1} {0: '0', 1: '1'} ['0', '1'] 21059 USE_v12_augm_seed2020_embed_v2_tx_ft_25_9e-06_adv_training_embed_v2\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(base_dir=BERT_PRETRAINED_DIR, albert=False):\n",
    "    if albert or \"albert\"in cfg[\"verbose\"].lower():\n",
    "        dict_path = os.path.join(base_dir, 'vocab_chinese.txt')\n",
    "    else:\n",
    "        dict_path = os.path.join(base_dir, 'vocab.txt')\n",
    "    with open(dict_path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "\n",
    "    word_index = {v: k  for k, v in enumerate(lines)}\n",
    "    for k, v in CHINESE_MAP.items():\n",
    "        assert v in word_index\n",
    "        if k in word_index:\n",
    "            print(\"[!] CHINESE_MAP k = {} is in word_index, DON'T using `{}` to replace\".format(k, v))\n",
    "            continue\n",
    "        # word_index[k] = word_index[v]\n",
    "        del word_index[v]\n",
    "    return word_index\n",
    "\n",
    "\n",
    "def get_label():\n",
    "    labels = [\"0\", \"1\"]\n",
    "    label2id = {k: v for v, k in enumerate(labels)}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    return label2id, id2label, labels\n",
    "    \n",
    "    \n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=np.float16)\n",
    "\n",
    "\n",
    "def load_embed(path, dim=300, word_index=None):\n",
    "    embedding_index = {}\n",
    "    with open(path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.strip().split()\n",
    "            word, arr = l[0], l[1:]\n",
    "            if len(arr) != dim:\n",
    "                print(\"[!] l = {}\".format(l))\n",
    "                continue\n",
    "            if word_index and word not in word_index:\n",
    "                continue\n",
    "            word, arr = get_coefs(word, arr)\n",
    "            embedding_index[word] = arr\n",
    "    return embedding_index\n",
    "\n",
    "\n",
    "def build_matrix(path, word_index=None, max_features=None, dim=300):\n",
    "    embedding_index = load_embed(path, dim=dim, word_index=word_index)\n",
    "    max_features = len(word_index) + 1 if max_features is None else max_features \n",
    "    embedding_matrix = np.zeros((max_features + 1, dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_word_embed(word_embed_f1=\"../../../chinese_embedding/Tencent_AILab_ChineseEmbedding.txt\", \n",
    "               word_embed_f2=\"../../../chinese_embedding/cc.zh.300.vec\", \n",
    "               save_filename=W2V_FILE,\n",
    "               word_index=None):\n",
    "    if os.path.exists(save_filename + \".npy\"):\n",
    "        word_embedding_matrix = np.load(save_filename + \".npy\").astype(\"float32\")\n",
    "    else:\n",
    "        if \"tx\" in cfg[\"ch_type\"]:\n",
    "            tx_embed, tx_unk = build_matrix(word_embed_f1, word_index=word_index, dim=200)\n",
    "        else:\n",
    "            tx_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            tx_unk = []\n",
    "        if \"ft\" in cfg[\"ch_type\"]:\n",
    "            ft_embed, ft_unk = build_matrix(word_embed_f2, word_index=word_index, dim=300)\n",
    "        else:\n",
    "            ft_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            ft_unk = []    \n",
    "\n",
    "        word_embedding_matrix = np.concatenate([tx_embed, ft_embed], axis=-1).astype(\"float32\")\n",
    "        print(word_embedding_matrix.shape, len(tx_unk), len(ft_unk))\n",
    "        np.save(save_filename, word_embedding_matrix )\n",
    "    return word_embedding_matrix\n",
    "    \n",
    "    \n",
    "word_index = get_vocab()\n",
    "label2id, id2label, labels = get_label()\n",
    "word_embedding_matrix = load_word_embed(word_index=word_index)\n",
    "\n",
    "NUM_CLASS = len(label2id)\n",
    "cfg[\"x_pad\"] = word_index[\"[PAD]\"]\n",
    "cfg[\"num_class\"] = NUM_CLASS\n",
    "cfg[\"filename\"] = \"{}_{}_{}_{}\".format(PREFIX, cfg[\"ch_type\"], FOLD_NUM, cfg[\"lr\"])\n",
    "cfg[\"filename\"] = cfg[\"filename\"] + \"_adv_training\" if cfg[\"adv_training\"] else cfg[\"filename\"]\n",
    "cfg[\"filename\"] = cfg[\"filename\"] + \"_embed\" if cfg[\"use_embed\"] else cfg[\"filename\"]\n",
    "cfg[\"filename\"] = cfg[\"filename\"] + \"_v2\" if cfg[\"use_embed_v2\"]and cfg[\"use_embed\"] else cfg[\"filename\"]\n",
    "print(label2id, id2label, labels, len(word_index), cfg[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(cfg, summary=False, word_embedding_matrix=None):\n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        if \"albert\"in cfg[\"verbose\"].lower():\n",
    "            from bert4keras.bert import build_bert_model\n",
    "            config_file = os.path.join(base_dir, 'albert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'model.ckpt-best')\n",
    "            model = build_bert_model(\n",
    "                    config_path=config_file,\n",
    "                    checkpoint_path=checkpoint_file,\n",
    "                    model='albert',\n",
    "                    return_keras_model=True\n",
    "            )\n",
    "            if cfg_[\"cls_num\"] > 1:\n",
    "                output = Concatenate(axis=-1)([model.get_layer(\"Encoder-1-FeedForward-Norm\").get_output_at(-i) for i in range(1, cfg[\"cls_num\"] + 1)])\n",
    "                model = Model(model.inputs[: 2], outputs=output)\n",
    "            model.trainable = cfg_[\"bert_trainable\"]\n",
    "        else:\n",
    "            config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "            if not os.path.exists(config_file):\n",
    "                config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "                checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')            \n",
    "            model = load_trained_model_from_checkpoint(config_file, \n",
    "                                                       checkpoint_file, \n",
    "                                                       training=False, \n",
    "                                                       trainable=cfg_[\"bert_trainable\"], \n",
    "                                                       output_layer_num=cfg_[\"cls_num\"],\n",
    "                                                       seq_len=cfg_['maxlen'])\n",
    "            \n",
    "            # model = Model(inputs=model.inputs[: 2], outputs=model.layers[-7].output)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def _get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        total_steps, warmup_steps = calc_train_steps(\n",
    "            num_example=num_example,\n",
    "            batch_size=B_SIZE,\n",
    "            epochs=MAX_EPOCH,\n",
    "            warmup_proportion=warmup_proportion,\n",
    "        )\n",
    "        opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "        if cfg.get(\"accum_step\", None) and cfg[\"accum_step\"] > 1:\n",
    "            print(\"[!] using accum_step = {}\".format(cfg[\"accum_step\"]))\n",
    "            from accum_optimizer import AccumOptimizer\n",
    "            opt = AccumOptimizer(opt, steps_per_update=cfg[\"accum_step\"])\n",
    "        \n",
    "        return opt\n",
    "\n",
    "    bert_model = _get_model(cfg[\"base_dir\"], cfg)\n",
    "\n",
    "    if word_embedding_matrix is not None:\n",
    "        embed = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                          output_dim=word_embedding_matrix.shape[1],\n",
    "                          weights=[word_embedding_matrix],\n",
    "                          trainable=cfg[\"trainable\"],\n",
    "                          name=\"char_embed\"\n",
    "                         )\n",
    "    \n",
    "    t1_in = Input(shape=(None, ))\n",
    "    t2_in = Input(shape=(None, ))\n",
    "    o1_in = Input(shape=(1, ))\n",
    "    o2_in = Input(shape=(1, ))\n",
    "\n",
    "    t1, t2, o1, o2 = t1_in, t2_in, o1_in, o2_in\n",
    "    \n",
    "    t = bert_model([t1, t2])\n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'))(t1)\n",
    "    ## Char information\n",
    "    if word_embedding_matrix is not None:\n",
    "        word_embed = embed(t1)\n",
    "        if cfg.get(\"use_embed_v2\", False):\n",
    "            _t2 = Lambda(lambda x: K.expand_dims(x, axis=-1))(t2)\n",
    "            word_embed = Concatenate(axis=-1)([word_embed, _t2])\n",
    "        word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "        word_embed = Bidirectional(LSTM(cfg[\"unit1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "        word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "        t = Concatenate(axis=-1)([t, word_embed])\n",
    "    \n",
    "    t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask])     \n",
    "    t = Bidirectional(LSTM(cfg[\"unit3\"], return_sequences=True), merge_mode=\"concat\")(t)\n",
    "    # t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask]) \n",
    "    # t = Conv1D(cfg[\"conv_num\"], kernel_size=3, padding=\"same\")(t) \n",
    "    t = Lambda(lambda x: x[:, 0, :], name=\"extract_layer\")(t)\n",
    "    if cfg.get(\"num_class\", 1) == 2:\n",
    "        po1_logit = Dense(1, name=\"po1_logit\")(t)\n",
    "        po1 = Activation('sigmoid', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])        \n",
    "        o1_loss = K.binary_crossentropy(o1, po1)\n",
    "        loss = K.mean(o1_loss)\n",
    "    else:\n",
    "        po1_logit = Dense(cfg[\"num_class\"], name=\"po1_logit\")(t)\n",
    "        po1 = Activation('softmax', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])\n",
    "        loss = K.categorical_crossentropy(o1, po1, axis=-1)\n",
    "        loss = K.mean(loss)\n",
    "\n",
    "    train_model.add_loss(loss)\n",
    "    opt = _get_opt(num_example=cfg[\"num_example\"], lr=cfg[\"lr\"], min_lr=cfg['min_lr'])\n",
    "    train_model.compile(optimizer=opt)\n",
    "    if summary:\n",
    "        train_model.summary()\n",
    "    return train_model\n",
    "\n",
    "\n",
    "# print(\"----------------build model ---------------\")\n",
    "# model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix if cfg[\"use_embed\"] else None)\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2id_X(x, x_dict, x2=None, maxlen=None, maxlen1=None):\n",
    "    if x2:\n",
    "        x1 = x\n",
    "        del x\n",
    "        maxlen -= 3\n",
    "        maxlen1 -= 2\n",
    "        assert maxlen > maxlen1\n",
    "        maxlen2 = maxlen - maxlen1 - 1\n",
    "        x1 = [\"[CLS]\"] + list(x1)[: maxlen1] + [\"[SEP]\"] \n",
    "        x1 = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x1]\n",
    "        seg1= [0 for _ in x1]\n",
    "        \n",
    "        x2 = list(x2)[: maxlen2] + [\"[SEP]\"] \n",
    "        x2= [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x2]\n",
    "        seg2 = [1 for _ in x2]\n",
    "        x = x1 + x2\n",
    "        seg = seg1 + seg2\n",
    "        \n",
    "    else:\n",
    "        maxlen -= 2\n",
    "        x = [\"[CLS]\"] + list(x)[: maxlen] + [\"[SEP]\"] \n",
    "        x = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x]\n",
    "        seg = [0 for _ in x]        \n",
    "    return x, seg\n",
    "\n",
    "\n",
    "def seq_padding(X, maxlen=None, padding_value=None, debug=False):\n",
    "    L = [len(x) for x in X]\n",
    "    if maxlen is None:\n",
    "        maxlen = max(L)\n",
    "\n",
    "    pad_X = np.array([\n",
    "        np.concatenate([x, [padding_value] * (maxlen - len(x))]) if len(x) < maxlen else x for x in X\n",
    "    ])\n",
    "    if debug:\n",
    "        print(\"[!] before pading {}\\n\".format(X))\n",
    "        print(\"[!] after pading {}\\n\".format(pad_X))\n",
    "    return pad_X\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    \n",
    "    def __init__(self, data, batch_size=B_SIZE, shuffle=SHUFFLE, augm_frac=0.75):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = cfg[\"num_example\"] // self.batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_dict = get_data(data)\n",
    "        self.augm_frac = augm_frac\n",
    "        if cfg[\"num_example\"] % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "            T1, T2, O1, O2 = [], [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data.iloc[i]\n",
    "                text = d[\"query1\"]\n",
    "                label_text = d[\"query2\"]\n",
    "                o1 = d[\"label\"]\n",
    "                \n",
    "                if random() > self.augm_frac:\n",
    "                    data_d = self.data_dict[text]\n",
    "                    pos_data = data_d[\"pos\"]\n",
    "                    neg_data = data_d[\"neg\"]\n",
    "                    if pos_data and neg_data:\n",
    "                        if random() > 0.5:\n",
    "                            o1 = 1\n",
    "                            label_text = choice(pos_data)\n",
    "                            if len(pos_data) >= 2:\n",
    "                                _pos_data = [e for e in pos_data if e != label_text]\n",
    "                                text = choice(_pos_data)\n",
    "                        else:\n",
    "                            o1 = 0\n",
    "                            text = choice(pos_data)\n",
    "                            label_text = choice(neg_data)   \n",
    "                \n",
    "                if random() > 0.5:\n",
    "                    text, label_text = label_text, text\n",
    "                \n",
    "                if o1 == \"\":\n",
    "                    continue\n",
    "                o1 = float(o1)\n",
    "                assert 0 <= o1 <= 1\n",
    "                \n",
    "                O1.append(o1)                \n",
    "                t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "                assert len(t1) == len(t2)\n",
    "                \n",
    "                T1.append(t1)\n",
    "                T2.append(t2)\n",
    "\n",
    "                if len(T1) == self.batch_size or i == idxs[-1]:\n",
    "                    O1 = np.array(O1).reshape(-1, 1)\n",
    "                    T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "                    T2 = seq_padding(T2, padding_value=0)\n",
    "                    assert T1.shape == T2.shape and T1.shape[0] == O1.shape[0]\n",
    "\n",
    "                    yield [T1, T2, O1], None\n",
    "                    T1, T2, O1, = [], [], []\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_):\n",
    "    model_inp_ind = [0, 1]\n",
    "    inputs = [model_.inputs[e] for e in model_inp_ind]\n",
    "    sub_model = Model(inputs=inputs, outputs=[model_.get_layer(\"po1\").output])\n",
    "    return sub_model\n",
    "\n",
    "\n",
    "def find_best_acc_score(y_pred, y_true, use_plt=True, bins=1000):\n",
    "    thres = [i / bins for i in range(1, bins)]\n",
    "    scores = [accuracy_score(y_true, np.array(y_pred > thre, \"int32\")) for thre in thres]\n",
    "#     if use_plt:\n",
    "#         import matplotlib\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         %matplotlib inline\n",
    "#         plt.plot(scores)\n",
    "#         plt.show()\n",
    "    ind = np.argmax(scores)\n",
    "    max_score = np.max(scores)\n",
    "    assert abs(scores[ind] - max_score) < 1e-15\n",
    "    return max_score, thres[ind]\n",
    "            \n",
    "\n",
    "def evaluate(sub_model, data, bs=32):\n",
    "    idxs = list(range(len(data)))\n",
    "    T1, T2, O1, O2 = [], [], [], []\n",
    "    preds = []\n",
    "    for i in idxs:\n",
    "        d = data.iloc[i]\n",
    "        text = d[\"query1\"]\n",
    "        label_text = d[\"query2\"]\n",
    "\n",
    "        t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "        assert len(t1) == len(t2)\n",
    "\n",
    "        T1.append(t1)\n",
    "        T2.append(t2)\n",
    "\n",
    "        o1 = float(d[\"label\"])\n",
    "        O1.append(o1)\n",
    "        if len(T1) == bs or i == idxs[-1]:\n",
    "            T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "            T2 = seq_padding(T2, padding_value=0)\n",
    "            assert T1.shape == T2.shape\n",
    "            pred = sub_model.predict([T1, T2])\n",
    "            preds.append(pred)\n",
    "            T1, T2 = [], []\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    O1 = np.array(O1).reshape(-1)\n",
    "    O1 = O1.astype(\"int32\")\n",
    "    auc = roc_auc_score(O1, preds)\n",
    "    best_res = find_best_acc_score(preds, O1)\n",
    "    print(\"[!] best accurary&threshold = {}\".format(best_res))\n",
    "    print(\"[!] best threshold classification_report\")\n",
    "    print(classification_report(O1,  np.array(preds > best_res[1], \"int32\"), digits=6))    \n",
    "    print(\"-\" * 80)\n",
    "    print(\"[!] np.mean(preds) = {}\".format(np.mean(preds)))\n",
    "    print(\"[!] classification_report\")\n",
    "    print(classification_report(O1,  np.array(preds > 0.5, \"int32\"), digits=6))\n",
    "    acc = accuracy_score(O1, np.array(preds > 0.5, \"int32\"))\n",
    "    return auc, acc\n",
    "    \n",
    "\n",
    "class Evaluate(Callback):\n",
    "    def __init__(self, data, filename=None):\n",
    "        self.F1 = []\n",
    "        self.best = 0.\n",
    "        self.filename = filename\n",
    "        self.data = data\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch ==  0:\n",
    "            print(\"[!] test load&save model\")\n",
    "            f = self.filename + \".h5\"\n",
    "            f = os.path.join(SAVE_DIR, f)\n",
    "            self.model.save(f, include_optimizer=False, overwrite=False)\n",
    "            if \"albert\" in cfg[\"verbose\"]:\n",
    "                model_ = load_model(f) \n",
    "            else:\n",
    "                model_ = load_model(f, custom_objects=get_custom_objects()) \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch + 1 < 1:\n",
    "            return\n",
    "#         if epoch + 1 in [3, 6, 9, 10, 12, 15, 18, 20]:\n",
    "#             f = self.filename + \"_{}.h5\".format(epoch + 1)\n",
    "#             f = os.path.join(SAVE_DIR, f)\n",
    "#             self.model.save(f, include_optimizer=False)\n",
    "            \n",
    "        sub_model = get_model(self.model)\n",
    "        f1, class_f1 = evaluate(sub_model, data=self.data)\n",
    "        self.F1.append(f1)\n",
    "        if f1 > self.best:\n",
    "            f = self.filename + \".h5\"\n",
    "            f = os.path.join(SAVE_DIR, f)\n",
    "            self.model.save(f, include_optimizer=False)\n",
    "            \n",
    "        if f1 > self.best:\n",
    "            self.best = f1\n",
    "            print(\"[!] epoch = {}, new best_auc = {}\".format(epoch + 1,  f1))\n",
    "        print('[!] epoch = {}, auc = {}, best auc {}'.format(epoch + 1, f1, self.best))\n",
    "        print('[!] epoch = {}, acc = {}\\n'.format(epoch + 1, class_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_layer(inputs, name, exclude_from=None):\n",
    "    \"\"\"根据inputs和name来搜索层\n",
    "    说明：inputs为某个层或某个层的输出；name为目标层的名字。\n",
    "    实现：根据inputs一直往上递归搜索，直到发现名字为name的层为止；\n",
    "         如果找不到，那就返回None。\n",
    "    \"\"\"\n",
    "    if exclude_from is None:\n",
    "        exclude_from = set()\n",
    "\n",
    "    if isinstance(inputs, keras.layers.Layer):\n",
    "        layer = inputs\n",
    "    else:\n",
    "        layer = inputs._keras_history[0]\n",
    "\n",
    "    if layer.name == name:\n",
    "        return layer\n",
    "    elif layer in exclude_from:\n",
    "        return None\n",
    "    else:\n",
    "        exclude_from.add(layer)\n",
    "        if isinstance(layer, keras.models.Model):\n",
    "            model = layer\n",
    "            for layer in model.layers:\n",
    "                if layer.name == name:\n",
    "                    return layer\n",
    "        inbound_layers = layer._inbound_nodes[0].inbound_layers\n",
    "        if not isinstance(inbound_layers, list):\n",
    "            inbound_layers = [inbound_layers]\n",
    "        if len(inbound_layers) > 0:\n",
    "            for layer in inbound_layers:\n",
    "                layer = search_layer(layer, name, exclude_from)\n",
    "                if layer is not None:\n",
    "                    return layer\n",
    "                \n",
    "def adversarial_training(model, embedding_names, epsilon=1):\n",
    "    \"\"\"给模型添加对抗训练\n",
    "    其中model是需要添加对抗训练的keras模型，embedding_names\n",
    "    则是model里边Embedding层的名字。要在模型compile之后使用。\n",
    "    \"\"\"\n",
    "    if model.train_function is None:  # 如果还没有训练函数\n",
    "        model._make_train_function()  # 手动make\n",
    "    old_train_function = model.train_function  # 备份旧的训练函数\n",
    "\n",
    "    # 查找Embedding层\n",
    "    embedding_layers = []\n",
    "    for embedding_name in embedding_names:\n",
    "        for output in model.outputs:\n",
    "            embedding_layer = search_layer(output, embedding_name)\n",
    "            if embedding_layer is not None:\n",
    "                embedding_layers.append(embedding_layer)\n",
    "                break\n",
    "    for embedding_layer in embedding_layers:\n",
    "        if embedding_layer is None:\n",
    "            raise Exception('Embedding layer not found')\n",
    "\n",
    "    # 求Embedding梯度\n",
    "    embeddings = [embedding_layer.embeddings for embedding_layer in embedding_layers] # Embedding矩阵\n",
    "    gradients = K.gradients(model.total_loss, embeddings)  # Embedding梯度\n",
    "    # gradients = K.zeros_like(embeddings) + gradients[0]  # 转为dense tensor\n",
    "    gradients = [K.zeros_like(embedding) + gradient for embedding, gradient in zip(embeddings, gradients)]\n",
    "\n",
    "    # 封装为函数\n",
    "    inputs = (model._feed_inputs +\n",
    "              model._feed_targets +\n",
    "              model._feed_sample_weights)  # 所有输入层\n",
    "    embedding_gradients = K.function(\n",
    "        inputs=inputs,\n",
    "        outputs=gradients,\n",
    "        name='embedding_gradients',\n",
    "    )  # 封装为函数\n",
    "\n",
    "    def train_function(inputs):  # 重新定义训练函数\n",
    "#         grads = embedding_gradients(inputs)[0]  # Embedding梯度\n",
    "#         delta = epsilon * grads / (np.sqrt((grads**2).sum()) + 1e-8)  # 计算扰动\n",
    "        grads = embedding_gradients(inputs)  # Embedding梯度\n",
    "        deltas = [epsilon * grad / (np.sqrt((grad**2).sum()) + 1e-8) for grad in grads]  # 计算扰动\n",
    "        # 注入扰动\n",
    "        # K.set_value(embeddings, K.eval(embeddings) + delta)  \n",
    "        for embedding, delta in zip(embeddings, deltas):\n",
    "            K.set_value(embedding, K.eval(embedding) + delta)\n",
    "            \n",
    "        outputs = old_train_function(inputs)  # 梯度下降\n",
    "        # 删除扰动\n",
    "        # K.set_value(embeddings, K.eval(embeddings) - delta)  # 删除扰动\n",
    "        for embedding, delta in zip(embeddings, deltas):\n",
    "            K.set_value(embedding, K.eval(embedding) - delta)       \n",
    "        return outputs\n",
    "\n",
    "    model.train_function = train_function  # 覆盖原训练函数\n",
    "\n",
    "\n",
    "# 写好函数后，启用对抗训练只需要一行代码\n",
    "# adversarial_training(model, 'Embedding-Token', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10749, 5)\n",
      "---------------------------------------------------------------------------------\n",
      "[!] start fold_id = 10 (10319, 5) (430, 5)\n",
      "{'base_dir': '../../../chinese_bert/UER-py/UER-large/', 'span_mode': True, 'lr': 9e-06, 'min_lr': 6e-08, 'ch_type': 'tx_ft', 'trainable': True, 'bert_trainable': True, 'accum_step': 1, 'cls_num': 4, 'unit1': 128, 'unit2': 128, 'unit3': 512, 'conv_num': 128, 'maxlen': 60, 'adv_training': True, 'W2V_FILE': './word_embedding_matrix', 'use_embed': True, 'use_embed_v2': True, 'verbose': 'USE_v12_augm_seed2020', 'x_pad': 0, 'num_class': 2, 'filename': 'USE_v12_augm_seed2020_embed_v2_tx_ft_25_9e-06_adv_training_embed_v2', 'num_example': 10319}\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, None, 500)    10530500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 1)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 501)    0           char_embed[0][0]                 \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 501)    0           concatenate_1[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 128)    646144      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             324009984   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 128)    0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 4224)   0           model_2[1][0]                    \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 4224)   0           concatenate_2[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 1024)   19406848    lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "extract_layer (Lambda)          (None, 1024)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "po1_logit (Dense)               (None, 1)            1025        extract_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "po1 (Activation)                (None, 1)            0           po1_logit[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 354,594,501\n",
      "Trainable params: 354,594,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[!] using adv_training\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10\n",
      "[!] test load&save model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 539s 2s/step - loss: 0.4571\n",
      "[!] best accurary&threshold = (0.9697674418604652, 0.784)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.976654  0.972868  0.974757       258\n",
      "           1   0.959538  0.965116  0.962319       172\n",
      "\n",
      "    accuracy                       0.969767       430\n",
      "   macro avg   0.968096  0.968992  0.968538       430\n",
      "weighted avg   0.969807  0.969767  0.969782       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.4333880841732025\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.979592  0.930233  0.954274       258\n",
      "           1   0.902703  0.970930  0.935574       172\n",
      "\n",
      "    accuracy                       0.946512       430\n",
      "   macro avg   0.941147  0.950581  0.944924       430\n",
      "weighted avg   0.948836  0.946512  0.946794       430\n",
      "\n",
      "[!] epoch = 1, new best_auc = 0.9859383450513791\n",
      "[!] epoch = 1, auc = 0.9859383450513791, best auc 0.9859383450513791\n",
      "[!] epoch = 1, acc = 0.9465116279069767\n",
      "\n",
      "Epoch 2/10\n",
      "323/323 [==============================] - 460s 1s/step - loss: 0.2136\n",
      "[!] best accurary&threshold = (0.9674418604651163, 0.589)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.965649  0.980620  0.973077       258\n",
      "           1   0.970238  0.947674  0.958824       172\n",
      "\n",
      "    accuracy                       0.967442       430\n",
      "   macro avg   0.967943  0.964147  0.965950       430\n",
      "weighted avg   0.967485  0.967442  0.967376       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3838936686515808\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.965385  0.972868  0.969112       258\n",
      "           1   0.958824  0.947674  0.953216       172\n",
      "\n",
      "    accuracy                       0.962791       430\n",
      "   macro avg   0.962104  0.960271  0.961164       430\n",
      "weighted avg   0.962760  0.962791  0.962754       430\n",
      "\n",
      "[!] epoch = 2, new best_auc = 0.987425635478637\n",
      "[!] epoch = 2, auc = 0.987425635478637, best auc 0.987425635478637\n",
      "[!] epoch = 2, acc = 0.9627906976744186\n",
      "\n",
      "Epoch 3/10\n",
      "323/323 [==============================] - 457s 1s/step - loss: 0.1432\n",
      "[!] best accurary&threshold = (0.9651162790697675, 0.277)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.980237  0.961240  0.970646       258\n",
      "           1   0.943503  0.970930  0.957020       172\n",
      "\n",
      "    accuracy                       0.965116       430\n",
      "   macro avg   0.961870  0.966085  0.963833       430\n",
      "weighted avg   0.965543  0.965116  0.965195       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.39110252261161804\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.961538  0.968992  0.965251       258\n",
      "           1   0.952941  0.941860  0.947368       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.957240  0.955426  0.956310       430\n",
      "weighted avg   0.958100  0.958140  0.958098       430\n",
      "\n",
      "[!] epoch = 3, auc = 0.9850820263205337, best auc 0.987425635478637\n",
      "[!] epoch = 3, acc = 0.958139534883721\n",
      "\n",
      "Epoch 4/10\n",
      "323/323 [==============================] - 455s 1s/step - loss: 0.1078\n",
      "[!] best accurary&threshold = (0.9720930232558139, 0.54)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.976744  0.976744  0.976744       258\n",
      "           1   0.965116  0.965116  0.965116       172\n",
      "\n",
      "    accuracy                       0.972093       430\n",
      "   macro avg   0.970930  0.970930  0.970930       430\n",
      "weighted avg   0.972093  0.972093  0.972093       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.39829009771347046\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.980392  0.968992  0.974659       258\n",
      "           1   0.954286  0.970930  0.962536       172\n",
      "\n",
      "    accuracy                       0.969767       430\n",
      "   macro avg   0.967339  0.969961  0.968597       430\n",
      "weighted avg   0.969950  0.969767  0.969810       430\n",
      "\n",
      "[!] epoch = 4, new best_auc = 0.9879213989543898\n",
      "[!] epoch = 4, auc = 0.9879213989543898, best auc 0.9879213989543898\n",
      "[!] epoch = 4, acc = 0.9697674418604652\n",
      "\n",
      "Epoch 5/10\n",
      "323/323 [==============================] - 453s 1s/step - loss: 0.0793\n",
      "[!] best accurary&threshold = (0.9674418604651163, 0.227)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.980315  0.965116  0.972656       258\n",
      "           1   0.948864  0.970930  0.959770       172\n",
      "\n",
      "    accuracy                       0.967442       430\n",
      "   macro avg   0.964589  0.968023  0.966213       430\n",
      "weighted avg   0.967734  0.967442  0.967502       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3816753923892975\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.961538  0.968992  0.965251       258\n",
      "           1   0.952941  0.941860  0.947368       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.957240  0.955426  0.956310       430\n",
      "weighted avg   0.958100  0.958140  0.958098       430\n",
      "\n",
      "[!] epoch = 5, auc = 0.9865693167477916, best auc 0.9879213989543898\n",
      "[!] epoch = 5, acc = 0.958139534883721\n",
      "\n",
      "Epoch 6/10\n",
      "323/323 [==============================] - 451s 1s/step - loss: 0.0599\n",
      "[!] best accurary&threshold = (0.9651162790697675, 0.335)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.972763  0.968992  0.970874       258\n",
      "           1   0.953757  0.959302  0.956522       172\n",
      "\n",
      "    accuracy                       0.965116       430\n",
      "   macro avg   0.963260  0.964147  0.963698       430\n",
      "weighted avg   0.965160  0.965116  0.965133       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3892327547073364\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.954545  0.976744  0.965517       258\n",
      "           1   0.963855  0.930233  0.946746       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.959200  0.953488  0.956131       430\n",
      "weighted avg   0.958269  0.958140  0.958009       430\n",
      "\n",
      "[!] epoch = 6, auc = 0.9875383089958537, best auc 0.9879213989543898\n",
      "[!] epoch = 6, acc = 0.958139534883721\n",
      "\n",
      "Epoch 7/10\n",
      "323/323 [==============================] - 453s 1s/step - loss: 0.0488\n",
      "[!] best accurary&threshold = (0.9651162790697675, 0.205)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.980237  0.961240  0.970646       258\n",
      "           1   0.943503  0.970930  0.957020       172\n",
      "\n",
      "    accuracy                       0.965116       430\n",
      "   macro avg   0.961870  0.966085  0.963833       430\n",
      "weighted avg   0.965543  0.965116  0.965195       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.39558348059654236\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.961538  0.968992  0.965251       258\n",
      "           1   0.952941  0.941860  0.947368       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.957240  0.955426  0.956310       430\n",
      "weighted avg   0.958100  0.958140  0.958098       430\n",
      "\n",
      "[!] epoch = 7, new best_auc = 0.9888453217955652\n",
      "[!] epoch = 7, auc = 0.9888453217955652, best auc 0.9888453217955652\n",
      "[!] epoch = 7, acc = 0.958139534883721\n",
      "\n",
      "Epoch 8/10\n",
      "323/323 [==============================] - 451s 1s/step - loss: 0.0392\n",
      "[!] best accurary&threshold = (0.9604651162790697, 0.313)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.965251  0.968992  0.967118       258\n",
      "           1   0.953216  0.947674  0.950437       172\n",
      "\n",
      "    accuracy                       0.960465       430\n",
      "   macro avg   0.959234  0.958333  0.958778       430\n",
      "weighted avg   0.960437  0.960465  0.960446       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.39441749453544617\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.961538  0.968992  0.965251       258\n",
      "           1   0.952941  0.941860  0.947368       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.957240  0.955426  0.956310       430\n",
      "weighted avg   0.958100  0.958140  0.958098       430\n",
      "\n",
      "[!] epoch = 8, auc = 0.9868848025959978, best auc 0.9888453217955652\n",
      "[!] epoch = 8, acc = 0.958139534883721\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 458s 1s/step - loss: 0.0283\n",
      "[!] best accurary&threshold = (0.9651162790697675, 0.805)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.961977  0.980620  0.971209       258\n",
      "           1   0.970060  0.941860  0.955752       172\n",
      "\n",
      "    accuracy                       0.965116       430\n",
      "   macro avg   0.966019  0.961240  0.963481       430\n",
      "weighted avg   0.965210  0.965116  0.965026       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3988229036331177\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.965116  0.965116  0.965116       258\n",
      "           1   0.947674  0.947674  0.947674       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.956395  0.956395  0.956395       430\n",
      "weighted avg   0.958140  0.958140  0.958140       430\n",
      "\n",
      "[!] epoch = 9, auc = 0.9865918514512348, best auc 0.9888453217955652\n",
      "[!] epoch = 9, acc = 0.958139534883721\n",
      "\n",
      "Epoch 10/10\n",
      "323/323 [==============================] - 457s 1s/step - loss: 0.0298\n",
      "[!] best accurary&threshold = (0.9651162790697675, 0.109)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.976471  0.965116  0.970760       258\n",
      "           1   0.948571  0.965116  0.956772       172\n",
      "\n",
      "    accuracy                       0.965116       430\n",
      "   macro avg   0.962521  0.965116  0.963766       430\n",
      "weighted avg   0.965311  0.965116  0.965165       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3903803527355194\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.957854  0.968992  0.963391       258\n",
      "           1   0.952663  0.936047  0.944282       172\n",
      "\n",
      "    accuracy                       0.955814       430\n",
      "   macro avg   0.955259  0.952519  0.953836       430\n",
      "weighted avg   0.955778  0.955814  0.955747       430\n",
      "\n",
      "[!] epoch = 10, auc = 0.9868848025959978, best auc 0.9888453217955652\n",
      "[!] epoch = 10, acc = 0.9558139534883721\n",
      "\n",
      "[0.9859383450513791, 0.987425635478637, 0.9850820263205337, 0.9879213989543898, 0.9865693167477916, 0.9875383089958537, 0.9888453217955652, 0.9868848025959978, 0.9865918514512348, 0.9868848025959978] 0.9888453217955652\n",
      "[4866.096004247665] finish fold_id = 10\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "[!] start fold_id = 11 (10319, 5) (430, 5)\n",
      "{'base_dir': '../../../chinese_bert/UER-py/UER-large/', 'span_mode': True, 'lr': 9e-06, 'min_lr': 6e-08, 'ch_type': 'tx_ft', 'trainable': True, 'bert_trainable': True, 'accum_step': 1, 'cls_num': 4, 'unit1': 128, 'unit2': 128, 'unit3': 512, 'conv_num': 128, 'maxlen': 60, 'adv_training': True, 'W2V_FILE': './word_embedding_matrix', 'use_embed': True, 'use_embed_v2': True, 'verbose': 'USE_v12_augm_seed2020', 'x_pad': 0, 'num_class': 2, 'filename': 'USE_v12_augm_seed2020_embed_v2_tx_ft_25_9e-06_adv_training_embed_v2', 'num_example': 10319}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, None, 500)    10530500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 1)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 501)    0           char_embed[0][0]                 \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 501)    0           concatenate_1[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 128)    646144      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             324009984   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 128)    0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 4224)   0           model_2[1][0]                    \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 4224)   0           concatenate_2[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 1024)   19406848    lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "extract_layer (Lambda)          (None, 1024)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "po1_logit (Dense)               (None, 1)            1025        extract_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "po1 (Activation)                (None, 1)            0           po1_logit[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 354,594,501\n",
      "Trainable params: 354,594,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[!] using adv_training\n",
      "Epoch 1/10\n",
      "[!] test load&save model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 538s 2s/step - loss: 0.4547\n",
      "[!] best accurary&threshold = (0.9116279069767442, 0.853)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.943548  0.906977  0.924901       258\n",
      "           1   0.868132  0.918605  0.892655       172\n",
      "\n",
      "    accuracy                       0.911628       430\n",
      "   macro avg   0.905840  0.912791  0.908778       430\n",
      "weighted avg   0.913382  0.911628  0.912003       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.536414623260498\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.985222  0.775194  0.867679       258\n",
      "           1   0.744493  0.982558  0.847118       172\n",
      "\n",
      "    accuracy                       0.858140       430\n",
      "   macro avg   0.864858  0.878876  0.857398       430\n",
      "weighted avg   0.888930  0.858140  0.859454       430\n",
      "\n",
      "[!] epoch = 1, new best_auc = 0.9617811429601585\n",
      "[!] epoch = 1, auc = 0.9617811429601585, best auc 0.9617811429601585\n",
      "[!] epoch = 1, acc = 0.858139534883721\n",
      "\n",
      "Epoch 2/10\n",
      "323/323 [==============================] - 469s 1s/step - loss: 0.2160\n",
      "[!] best accurary&threshold = (0.9209302325581395, 0.79)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.921053  0.949612  0.935115       258\n",
      "           1   0.920732  0.877907  0.898810       172\n",
      "\n",
      "    accuracy                       0.920930       430\n",
      "   macro avg   0.920892  0.913760  0.916962       430\n",
      "weighted avg   0.920924  0.920930  0.920593       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.45050060749053955\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.950000  0.883721  0.915663       258\n",
      "           1   0.842105  0.930233  0.883978       172\n",
      "\n",
      "    accuracy                       0.902326       430\n",
      "   macro avg   0.896053  0.906977  0.899820       430\n",
      "weighted avg   0.906842  0.902326  0.902989       430\n",
      "\n",
      "[!] epoch = 2, new best_auc = 0.9676852352623039\n",
      "[!] epoch = 2, auc = 0.9676852352623039, best auc 0.9676852352623039\n",
      "[!] epoch = 2, acc = 0.9023255813953488\n",
      "\n",
      "Epoch 3/10\n",
      "323/323 [==============================] - 458s 1s/step - loss: 0.1503\n",
      "[!] best accurary&threshold = (0.9372093023255814, 0.82)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.926199  0.972868  0.948960       258\n",
      "           1   0.955975  0.883721  0.918429       172\n",
      "\n",
      "    accuracy                       0.937209       430\n",
      "   macro avg   0.941087  0.928295  0.933695       430\n",
      "weighted avg   0.938109  0.937209  0.936748       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.4467076361179352\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.965957  0.879845  0.920892       258\n",
      "           1   0.841026  0.953488  0.893733       172\n",
      "\n",
      "    accuracy                       0.909302       430\n",
      "   macro avg   0.903492  0.916667  0.907313       430\n",
      "weighted avg   0.915985  0.909302  0.910029       430\n",
      "\n",
      "[!] epoch = 3, new best_auc = 0.9742653686677484\n",
      "[!] epoch = 3, auc = 0.9742653686677484, best auc 0.9742653686677484\n",
      "[!] epoch = 3, acc = 0.9093023255813953\n",
      "\n",
      "Epoch 4/10\n",
      "323/323 [==============================] - 452s 1s/step - loss: 0.1053\n",
      "[!] best accurary&threshold = (0.9232558139534883, 0.944)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.918216  0.957364  0.937381       258\n",
      "           1   0.931677  0.872093  0.900901       172\n",
      "\n",
      "    accuracy                       0.923256       430\n",
      "   macro avg   0.924946  0.914729  0.919141       430\n",
      "weighted avg   0.923600  0.923256  0.922789       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.4975925385951996\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.977376  0.837209  0.901879       258\n",
      "           1   0.799043  0.970930  0.876640       172\n",
      "\n",
      "    accuracy                       0.890698       430\n",
      "   macro avg   0.888209  0.904070  0.889260       430\n",
      "weighted avg   0.906043  0.890698  0.891784       430\n",
      "\n",
      "[!] epoch = 4, auc = 0.9718992248062016, best auc 0.9742653686677484\n",
      "[!] epoch = 4, acc = 0.8906976744186047\n",
      "\n",
      "Epoch 5/10\n",
      "323/323 [==============================] - 452s 1s/step - loss: 0.0874\n",
      "[!] best accurary&threshold = (0.9372093023255814, 0.891)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.926199  0.972868  0.948960       258\n",
      "           1   0.955975  0.883721  0.918429       172\n",
      "\n",
      "    accuracy                       0.937209       430\n",
      "   macro avg   0.941087  0.928295  0.933695       430\n",
      "weighted avg   0.938109  0.937209  0.936748       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.45020076632499695\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.962185  0.887597  0.923387       258\n",
      "           1   0.848958  0.947674  0.895604       172\n",
      "\n",
      "    accuracy                       0.911628       430\n",
      "   macro avg   0.905572  0.917636  0.909496       430\n",
      "weighted avg   0.916894  0.911628  0.912274       430\n",
      "\n",
      "[!] epoch = 5, new best_auc = 0.9743780421849648\n",
      "[!] epoch = 5, auc = 0.9743780421849648, best auc 0.9743780421849648\n",
      "[!] epoch = 5, acc = 0.9116279069767442\n",
      "\n",
      "Epoch 6/10\n",
      "323/323 [==============================] - 449s 1s/step - loss: 0.0624\n",
      "[!] best accurary&threshold = (0.9302325581395349, 0.856)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.938462  0.945736  0.942085       258\n",
      "           1   0.917647  0.906977  0.912281       172\n",
      "\n",
      "    accuracy                       0.930233       430\n",
      "   macro avg   0.928054  0.926357  0.927183       430\n",
      "weighted avg   0.930136  0.930233  0.930163       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.43170076608657837\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.955466  0.914729  0.934653       258\n",
      "           1   0.879781  0.936047  0.907042       172\n",
      "\n",
      "    accuracy                       0.923256       430\n",
      "   macro avg   0.917624  0.925388  0.920848       430\n",
      "weighted avg   0.925192  0.923256  0.923609       430\n",
      "\n",
      "[!] epoch = 6, new best_auc = 0.9759329367225527\n",
      "[!] epoch = 6, auc = 0.9759329367225527, best auc 0.9759329367225527\n",
      "[!] epoch = 6, acc = 0.9232558139534883\n",
      "\n",
      "Epoch 7/10\n",
      "323/323 [==============================] - 452s 1s/step - loss: 0.0505\n",
      "[!] best accurary&threshold = (0.9325581395348838, 0.792)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.935361  0.953488  0.944338       258\n",
      "           1   0.928144  0.901163  0.914454       172\n",
      "\n",
      "    accuracy                       0.932558       430\n",
      "   macro avg   0.931752  0.927326  0.929396       430\n",
      "weighted avg   0.932474  0.932558  0.932384       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.43015220761299133\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.958506  0.895349  0.925852       258\n",
      "           1   0.857143  0.941860  0.897507       172\n",
      "\n",
      "    accuracy                       0.913953       430\n",
      "   macro avg   0.907825  0.918605  0.911679       430\n",
      "weighted avg   0.917961  0.913953  0.914514       430\n",
      "\n",
      "[!] epoch = 7, auc = 0.9730259599783666, best auc 0.9759329367225527\n",
      "[!] epoch = 7, acc = 0.913953488372093\n",
      "\n",
      "Epoch 8/10\n",
      "323/323 [==============================] - 452s 1s/step - loss: 0.0404\n",
      "[!] best accurary&threshold = (0.9279069767441861, 0.739)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.921933  0.961240  0.941176       258\n",
      "           1   0.937888  0.877907  0.906907       172\n",
      "\n",
      "    accuracy                       0.927907       430\n",
      "   macro avg   0.929911  0.919574  0.924042       430\n",
      "weighted avg   0.928315  0.927907  0.927469       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.4046299457550049\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.940476  0.918605  0.929412       258\n",
      "           1   0.882022  0.912791  0.897143       172\n",
      "\n",
      "    accuracy                       0.916279       430\n",
      "   macro avg   0.911249  0.915698  0.913277       430\n",
      "weighted avg   0.917095  0.916279  0.916504       430\n",
      "\n",
      "[!] epoch = 8, auc = 0.9754146385433567, best auc 0.9759329367225527\n",
      "[!] epoch = 8, acc = 0.9162790697674419\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 464s 1s/step - loss: 0.0330\n",
      "[!] best accurary&threshold = (0.9395348837209302, 0.705)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.942748  0.957364  0.950000       258\n",
      "           1   0.934524  0.912791  0.923529       172\n",
      "\n",
      "    accuracy                       0.939535       430\n",
      "   macro avg   0.938636  0.935078  0.936765       430\n",
      "weighted avg   0.939458  0.939535  0.939412       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.41239237785339355\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.944223  0.918605  0.931238       258\n",
      "           1   0.882682  0.918605  0.900285       172\n",
      "\n",
      "    accuracy                       0.918605       430\n",
      "   macro avg   0.913452  0.918605  0.915761       430\n",
      "weighted avg   0.919606  0.918605  0.918857       430\n",
      "\n",
      "[!] epoch = 9, auc = 0.9732851090679646, best auc 0.9759329367225527\n",
      "[!] epoch = 9, acc = 0.9186046511627907\n",
      "\n",
      "Epoch 10/10\n",
      "323/323 [==============================] - 455s 1s/step - loss: 0.0296\n",
      "[!] best accurary&threshold = (0.9279069767441861, 0.939)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.925094  0.957364  0.940952       258\n",
      "           1   0.932515  0.883721  0.907463       172\n",
      "\n",
      "    accuracy                       0.927907       430\n",
      "   macro avg   0.928804  0.920543  0.924208       430\n",
      "weighted avg   0.928062  0.927907  0.927557       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.43825507164001465\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.966527  0.895349  0.929577       258\n",
      "           1   0.858639  0.953488  0.903581       172\n",
      "\n",
      "    accuracy                       0.918605       430\n",
      "   macro avg   0.912583  0.924419  0.916579       430\n",
      "weighted avg   0.923372  0.918605  0.919179       430\n",
      "\n",
      "[!] epoch = 10, auc = 0.9702091220479538, best auc 0.9759329367225527\n",
      "[!] epoch = 10, acc = 0.9186046511627907\n",
      "\n",
      "[0.9617811429601585, 0.9676852352623039, 0.9742653686677484, 0.9718992248062016, 0.9743780421849648, 0.9759329367225527, 0.9730259599783666, 0.9754146385433567, 0.9732851090679646, 0.9702091220479538] 0.9759329367225527\n",
      "[4867.183194637299] finish fold_id = 11\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "[!] start fold_id = 12 (10319, 5) (430, 5)\n",
      "{'base_dir': '../../../chinese_bert/UER-py/UER-large/', 'span_mode': True, 'lr': 9e-06, 'min_lr': 6e-08, 'ch_type': 'tx_ft', 'trainable': True, 'bert_trainable': True, 'accum_step': 1, 'cls_num': 4, 'unit1': 128, 'unit2': 128, 'unit3': 512, 'conv_num': 128, 'maxlen': 60, 'adv_training': True, 'W2V_FILE': './word_embedding_matrix', 'use_embed': True, 'use_embed_v2': True, 'verbose': 'USE_v12_augm_seed2020', 'x_pad': 0, 'num_class': 2, 'filename': 'USE_v12_augm_seed2020_embed_v2_tx_ft_25_9e-06_adv_training_embed_v2', 'num_example': 10319}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, None, 500)    10530500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 1)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 501)    0           char_embed[0][0]                 \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 501)    0           concatenate_1[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 128)    646144      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             324009984   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 128)    0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 4224)   0           model_2[1][0]                    \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 4224)   0           concatenate_2[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 1024)   19406848    lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "extract_layer (Lambda)          (None, 1024)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "po1_logit (Dense)               (None, 1)            1025        extract_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "po1 (Activation)                (None, 1)            0           po1_logit[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 354,594,501\n",
      "Trainable params: 354,594,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[!] using adv_training\n",
      "Epoch 1/10\n",
      "[!] test load&save model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 558s 2s/step - loss: 0.4524\n",
      "[!] best accurary&threshold = (0.9395348837209302, 0.698)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.946154  0.953488  0.949807       258\n",
      "           1   0.929412  0.918605  0.923977       172\n",
      "\n",
      "    accuracy                       0.939535       430\n",
      "   macro avg   0.937783  0.936047  0.936892       430\n",
      "weighted avg   0.939457  0.939535  0.939475       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.44537460803985596\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.963415  0.918605  0.940476       258\n",
      "           1   0.885870  0.947674  0.915730       172\n",
      "\n",
      "    accuracy                       0.930233       430\n",
      "   macro avg   0.924642  0.933140  0.928103       430\n",
      "weighted avg   0.932397  0.930233  0.930578       430\n",
      "\n",
      "[!] epoch = 1, new best_auc = 0.9731386334955832\n",
      "[!] epoch = 1, auc = 0.9731386334955832, best auc 0.9731386334955832\n",
      "[!] epoch = 1, acc = 0.9302325581395349\n",
      "\n",
      "Epoch 2/10\n",
      "323/323 [==============================] - 452s 1s/step - loss: 0.2202\n",
      "[!] best accurary&threshold = (0.9488372093023256, 0.354)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.957364  0.957364  0.957364       258\n",
      "           1   0.936047  0.936047  0.936047       172\n",
      "\n",
      "    accuracy                       0.948837       430\n",
      "   macro avg   0.946705  0.946705  0.946705       430\n",
      "weighted avg   0.948837  0.948837  0.948837       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.37545573711395264\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.933086  0.972868  0.952562       258\n",
      "           1   0.956522  0.895349  0.924925       172\n",
      "\n",
      "    accuracy                       0.941860       430\n",
      "   macro avg   0.944804  0.934109  0.938743       430\n",
      "weighted avg   0.942460  0.941860  0.941507       430\n",
      "\n",
      "[!] epoch = 2, new best_auc = 0.9797413016044709\n",
      "[!] epoch = 2, auc = 0.9797413016044709, best auc 0.9797413016044709\n",
      "[!] epoch = 2, acc = 0.9418604651162791\n",
      "\n",
      "Epoch 3/10\n",
      "323/323 [==============================] - 455s 1s/step - loss: 0.1504\n",
      "[!] best accurary&threshold = (0.9465116279069767, 0.295)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.950192  0.961240  0.955684       258\n",
      "           1   0.940828  0.924419  0.932551       172\n",
      "\n",
      "    accuracy                       0.946512       430\n",
      "   macro avg   0.945510  0.942829  0.944118       430\n",
      "weighted avg   0.946446  0.946512  0.946431       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.35968494415283203\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.919708  0.976744  0.947368       258\n",
      "           1   0.961538  0.872093  0.914634       172\n",
      "\n",
      "    accuracy                       0.934884       430\n",
      "   macro avg   0.940623  0.924419  0.931001       430\n",
      "weighted avg   0.936440  0.934884  0.934275       430\n",
      "\n",
      "[!] epoch = 3, new best_auc = 0.983549666486389\n",
      "[!] epoch = 3, auc = 0.983549666486389, best auc 0.983549666486389\n",
      "[!] epoch = 3, acc = 0.9348837209302325\n",
      "\n",
      "Epoch 4/10\n",
      "323/323 [==============================] - 451s 1s/step - loss: 0.1087\n",
      "[!] best accurary&threshold = (0.9418604651162791, 0.308)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.946360  0.957364  0.951830       258\n",
      "           1   0.934911  0.918605  0.926686       172\n",
      "\n",
      "    accuracy                       0.941860       430\n",
      "   macro avg   0.940636  0.937984  0.939258       430\n",
      "weighted avg   0.941781  0.941860  0.941773       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.360787034034729\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.903226  0.976744  0.938547       258\n",
      "           1   0.960265  0.843023  0.897833       172\n",
      "\n",
      "    accuracy                       0.923256       430\n",
      "   macro avg   0.931745  0.909884  0.918190       430\n",
      "weighted avg   0.926041  0.923256  0.922262       430\n",
      "\n",
      "[!] epoch = 4, auc = 0.9808004326663061, best auc 0.983549666486389\n",
      "[!] epoch = 4, acc = 0.9232558139534883\n",
      "\n",
      "Epoch 5/10\n",
      "323/323 [==============================] - 455s 1s/step - loss: 0.0833\n",
      "[!] best accurary&threshold = (0.9465116279069767, 0.239)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.957198  0.953488  0.955340       258\n",
      "           1   0.930636  0.936047  0.933333       172\n",
      "\n",
      "    accuracy                       0.946512       430\n",
      "   macro avg   0.943917  0.944767  0.944337       430\n",
      "weighted avg   0.946573  0.946512  0.946537       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3824908137321472\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.932331  0.961240  0.946565       258\n",
      "           1   0.939024  0.895349  0.916667       172\n",
      "\n",
      "    accuracy                       0.934884       430\n",
      "   macro avg   0.935678  0.928295  0.931616       430\n",
      "weighted avg   0.935008  0.934884  0.934606       430\n",
      "\n",
      "[!] epoch = 5, new best_auc = 0.9869298720028844\n",
      "[!] epoch = 5, auc = 0.9869298720028844, best auc 0.9869298720028844\n",
      "[!] epoch = 5, acc = 0.9348837209302325\n",
      "\n",
      "Epoch 6/10\n",
      "323/323 [==============================] - 455s 1s/step - loss: 0.0595\n",
      "[!] best accurary&threshold = (0.9372093023255814, 0.077)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.967611  0.926357  0.946535       258\n",
      "           1   0.896175  0.953488  0.923944       172\n",
      "\n",
      "    accuracy                       0.937209       430\n",
      "   macro avg   0.931893  0.939922  0.935239       430\n",
      "weighted avg   0.939037  0.937209  0.937498       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3566511869430542\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.906810  0.980620  0.942272       258\n",
      "           1   0.966887  0.848837  0.904025       172\n",
      "\n",
      "    accuracy                       0.927907       430\n",
      "   macro avg   0.936849  0.914729  0.923148       430\n",
      "weighted avg   0.930841  0.927907  0.926973       430\n",
      "\n",
      "[!] epoch = 6, auc = 0.9856003244997296, best auc 0.9869298720028844\n",
      "[!] epoch = 6, acc = 0.9279069767441861\n",
      "\n",
      "Epoch 7/10\n",
      "323/323 [==============================] - 453s 1s/step - loss: 0.0474\n",
      "[!] best accurary&threshold = (0.9372093023255814, 0.075)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.971429  0.922481  0.946322       258\n",
      "           1   0.891892  0.959302  0.924370       172\n",
      "\n",
      "    accuracy                       0.937209       430\n",
      "   macro avg   0.931660  0.940891  0.935346       430\n",
      "weighted avg   0.939614  0.937209  0.937541       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.37581852078437805\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.918519  0.961240  0.939394       258\n",
      "           1   0.937500  0.872093  0.903614       172\n",
      "\n",
      "    accuracy                       0.925581       430\n",
      "   macro avg   0.928009  0.916667  0.921504       430\n",
      "weighted avg   0.926111  0.925581  0.925082       430\n",
      "\n",
      "[!] epoch = 7, auc = 0.9867045249684514, best auc 0.9869298720028844\n",
      "[!] epoch = 7, acc = 0.9255813953488372\n",
      "\n",
      "Epoch 8/10\n",
      "323/323 [==============================] - 454s 1s/step - loss: 0.0392\n",
      "[!] best accurary&threshold = (0.9418604651162791, 0.139)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.960474  0.941860  0.951076       258\n",
      "           1   0.915254  0.941860  0.928367       172\n",
      "\n",
      "    accuracy                       0.941860       430\n",
      "   macro avg   0.937864  0.941860  0.939722       430\n",
      "weighted avg   0.942386  0.941860  0.941992       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.38695117831230164\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.939163  0.957364  0.948177       258\n",
      "           1   0.934132  0.906977  0.920354       172\n",
      "\n",
      "    accuracy                       0.937209       430\n",
      "   macro avg   0.936648  0.932171  0.934265       430\n",
      "weighted avg   0.937151  0.937209  0.937048       430\n",
      "\n",
      "[!] epoch = 8, auc = 0.9845637281413376, best auc 0.9869298720028844\n",
      "[!] epoch = 8, acc = 0.9372093023255814\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 453s 1s/step - loss: 0.0337\n",
      "[!] best accurary&threshold = (0.9465116279069767, 0.231)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.957198  0.953488  0.955340       258\n",
      "           1   0.930636  0.936047  0.933333       172\n",
      "\n",
      "    accuracy                       0.946512       430\n",
      "   macro avg   0.943917  0.944767  0.944337       430\n",
      "weighted avg   0.946573  0.946512  0.946537       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.38479819893836975\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.932584  0.965116  0.948571       258\n",
      "           1   0.944785  0.895349  0.919403       172\n",
      "\n",
      "    accuracy                       0.937209       430\n",
      "   macro avg   0.938685  0.930233  0.933987       430\n",
      "weighted avg   0.937465  0.937209  0.936904       430\n",
      "\n",
      "[!] epoch = 9, new best_auc = 0.987267892554534\n",
      "[!] epoch = 9, auc = 0.987267892554534, best auc 0.987267892554534\n",
      "[!] epoch = 9, acc = 0.9372093023255814\n",
      "\n",
      "Epoch 10/10\n",
      "323/323 [==============================] - 457s 1s/step - loss: 0.0265\n",
      "[!] best accurary&threshold = (0.9465116279069767, 0.432)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.953668  0.957364  0.955513       258\n",
      "           1   0.935673  0.930233  0.932945       172\n",
      "\n",
      "    accuracy                       0.946512       430\n",
      "   macro avg   0.944670  0.943798  0.944229       430\n",
      "weighted avg   0.946470  0.946512  0.946485       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.39279061555862427\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.950000  0.957364  0.953668       258\n",
      "           1   0.935294  0.924419  0.929825       172\n",
      "\n",
      "    accuracy                       0.944186       430\n",
      "   macro avg   0.942647  0.940891  0.941746       430\n",
      "weighted avg   0.944118  0.944186  0.944131       430\n",
      "\n",
      "[!] epoch = 10, auc = 0.9852397692446366, best auc 0.987267892554534\n",
      "[!] epoch = 10, acc = 0.9441860465116279\n",
      "\n",
      "[0.9731386334955832, 0.9797413016044709, 0.983549666486389, 0.9808004326663061, 0.9869298720028844, 0.9856003244997296, 0.9867045249684514, 0.9845637281413376, 0.987267892554534, 0.9852397692446366] 0.987267892554534\n",
      "[4874.385992050171] finish fold_id = 12\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "[!] start fold_id = 13 (10319, 5) (430, 5)\n",
      "{'base_dir': '../../../chinese_bert/UER-py/UER-large/', 'span_mode': True, 'lr': 9e-06, 'min_lr': 6e-08, 'ch_type': 'tx_ft', 'trainable': True, 'bert_trainable': True, 'accum_step': 1, 'cls_num': 4, 'unit1': 128, 'unit2': 128, 'unit3': 512, 'conv_num': 128, 'maxlen': 60, 'adv_training': True, 'W2V_FILE': './word_embedding_matrix', 'use_embed': True, 'use_embed_v2': True, 'verbose': 'USE_v12_augm_seed2020', 'x_pad': 0, 'num_class': 2, 'filename': 'USE_v12_augm_seed2020_embed_v2_tx_ft_25_9e-06_adv_training_embed_v2', 'num_example': 10319}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, None, 500)    10530500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 1)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 501)    0           char_embed[0][0]                 \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 501)    0           concatenate_1[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 128)    646144      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             324009984   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 128)    0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 4224)   0           model_2[1][0]                    \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 4224)   0           concatenate_2[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 1024)   19406848    lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "extract_layer (Lambda)          (None, 1024)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "po1_logit (Dense)               (None, 1)            1025        extract_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "po1 (Activation)                (None, 1)            0           po1_logit[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 354,594,501\n",
      "Trainable params: 354,594,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[!] using adv_training\n",
      "Epoch 1/10\n",
      "[!] test load&save model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 544s 2s/step - loss: 0.4673\n",
      "[!] best accurary&threshold = (0.9720930232558139, 0.217)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.969466  0.984496  0.976923       258\n",
      "           1   0.976190  0.953488  0.964706       172\n",
      "\n",
      "    accuracy                       0.972093       430\n",
      "   macro avg   0.972828  0.968992  0.970814       430\n",
      "weighted avg   0.972156  0.972093  0.972036       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.30581146478652954\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.888112  0.984496  0.933824       258\n",
      "           1   0.972222  0.813953  0.886076       172\n",
      "\n",
      "    accuracy                       0.916279       430\n",
      "   macro avg   0.930167  0.899225  0.909950       430\n",
      "weighted avg   0.921756  0.916279  0.914724       430\n",
      "\n",
      "[!] epoch = 1, new best_auc = 0.9875833784027404\n",
      "[!] epoch = 1, auc = 0.9875833784027404, best auc 0.9875833784027404\n",
      "[!] epoch = 1, acc = 0.9162790697674419\n",
      "\n",
      "Epoch 2/10\n",
      "323/323 [==============================] - 464s 1s/step - loss: 0.2219\n",
      "[!] best accurary&threshold = (0.9558139534883721, 0.468)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.947566  0.980620  0.963810       258\n",
      "           1   0.969325  0.918605  0.943284       172\n",
      "\n",
      "    accuracy                       0.955814       430\n",
      "   macro avg   0.958445  0.949612  0.953547       430\n",
      "weighted avg   0.956269  0.955814  0.955599       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3938497006893158\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.947566  0.980620  0.963810       258\n",
      "           1   0.969325  0.918605  0.943284       172\n",
      "\n",
      "    accuracy                       0.955814       430\n",
      "   macro avg   0.958445  0.949612  0.953547       430\n",
      "weighted avg   0.956269  0.955814  0.955599       430\n",
      "\n",
      "[!] epoch = 2, auc = 0.9867270596718948, best auc 0.9875833784027404\n",
      "[!] epoch = 2, acc = 0.9558139534883721\n",
      "\n",
      "Epoch 3/10\n",
      "323/323 [==============================] - 462s 1s/step - loss: 0.1665\n",
      "[!] best accurary&threshold = (0.9651162790697675, 0.516)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.951673  0.992248  0.971537       258\n",
      "           1   0.987578  0.924419  0.954955       172\n",
      "\n",
      "    accuracy                       0.965116       430\n",
      "   macro avg   0.969625  0.958333  0.963246       430\n",
      "weighted avg   0.966035  0.965116  0.964904       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.39600056409835815\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.951311  0.984496  0.967619       258\n",
      "           1   0.975460  0.924419  0.949254       172\n",
      "\n",
      "    accuracy                       0.960465       430\n",
      "   macro avg   0.963385  0.954457  0.958436       430\n",
      "weighted avg   0.960971  0.960465  0.960273       430\n",
      "\n",
      "[!] epoch = 3, new best_auc = 0.9906480980710294\n",
      "[!] epoch = 3, auc = 0.9906480980710294, best auc 0.9906480980710294\n",
      "[!] epoch = 3, acc = 0.9604651162790697\n",
      "\n",
      "Epoch 4/10\n",
      "323/323 [==============================] - 456s 1s/step - loss: 0.1173\n",
      "[!] best accurary&threshold = (0.9558139534883721, 0.3)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.968627  0.957364  0.962963       258\n",
      "           1   0.937143  0.953488  0.945245       172\n",
      "\n",
      "    accuracy                       0.955814       430\n",
      "   macro avg   0.952885  0.955426  0.954104       430\n",
      "weighted avg   0.956034  0.955814  0.955876       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3982413411140442\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.954198  0.968992  0.961538       258\n",
      "           1   0.952381  0.930233  0.941176       172\n",
      "\n",
      "    accuracy                       0.953488       430\n",
      "   macro avg   0.953290  0.949612  0.951357       430\n",
      "weighted avg   0.953471  0.953488  0.953394       430\n",
      "\n",
      "[!] epoch = 4, auc = 0.9895213628988643, best auc 0.9906480980710294\n",
      "[!] epoch = 4, acc = 0.9534883720930233\n",
      "\n",
      "Epoch 5/10\n",
      "323/323 [==============================] - 452s 1s/step - loss: 0.0888\n",
      "[!] best accurary&threshold = (0.9651162790697675, 0.39)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.958491  0.984496  0.971319       258\n",
      "           1   0.975758  0.936047  0.955490       172\n",
      "\n",
      "    accuracy                       0.965116       430\n",
      "   macro avg   0.967124  0.960271  0.963404       430\n",
      "weighted avg   0.965397  0.965116  0.964987       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3734491169452667\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.944444  0.988372  0.965909       258\n",
      "           1   0.981250  0.912791  0.945783       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.962847  0.950581  0.955846       430\n",
      "weighted avg   0.959167  0.958140  0.957859       430\n",
      "\n",
      "[!] epoch = 5, auc = 0.9903776816297098, best auc 0.9906480980710294\n",
      "[!] epoch = 5, acc = 0.958139534883721\n",
      "\n",
      "Epoch 6/10\n",
      "323/323 [==============================] - 451s 1s/step - loss: 0.0683\n",
      "[!] best accurary&threshold = (0.9627906976744186, 0.573)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.958333  0.980620  0.969349       258\n",
      "           1   0.969880  0.936047  0.952663       172\n",
      "\n",
      "    accuracy                       0.962791       430\n",
      "   macro avg   0.964106  0.958333  0.961006       430\n",
      "weighted avg   0.962952  0.962791  0.962674       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.40416762232780457\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.965251  0.968992  0.967118       258\n",
      "           1   0.953216  0.947674  0.950437       172\n",
      "\n",
      "    accuracy                       0.960465       430\n",
      "   macro avg   0.959234  0.958333  0.958778       430\n",
      "weighted avg   0.960437  0.960465  0.960446       430\n",
      "\n",
      "[!] epoch = 6, auc = 0.9886650441680187, best auc 0.9906480980710294\n",
      "[!] epoch = 6, acc = 0.9604651162790697\n",
      "\n",
      "Epoch 7/10\n",
      "323/323 [==============================] - 453s 1s/step - loss: 0.0578\n",
      "[!] best accurary&threshold = (0.9627906976744186, 0.259)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.968992  0.968992  0.968992       258\n",
      "           1   0.953488  0.953488  0.953488       172\n",
      "\n",
      "    accuracy                       0.962791       430\n",
      "   macro avg   0.961240  0.961240  0.961240       430\n",
      "weighted avg   0.962791  0.962791  0.962791       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3819100260734558\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.954545  0.976744  0.965517       258\n",
      "           1   0.963855  0.930233  0.946746       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.959200  0.953488  0.956131       430\n",
      "weighted avg   0.958269  0.958140  0.958009       430\n",
      "\n",
      "[!] epoch = 7, new best_auc = 0.9920452496845141\n",
      "[!] epoch = 7, auc = 0.9920452496845141, best auc 0.9920452496845141\n",
      "[!] epoch = 7, acc = 0.958139534883721\n",
      "\n",
      "Epoch 8/10\n",
      "323/323 [==============================] - 455s 1s/step - loss: 0.0443\n",
      "[!] best accurary&threshold = (0.9604651162790697, 0.139)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.965251  0.968992  0.967118       258\n",
      "           1   0.953216  0.947674  0.950437       172\n",
      "\n",
      "    accuracy                       0.960465       430\n",
      "   macro avg   0.959234  0.958333  0.958778       430\n",
      "weighted avg   0.960437  0.960465  0.960446       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.36107540130615234\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.923636  0.984496  0.953096       258\n",
      "           1   0.974194  0.877907  0.923547       172\n",
      "\n",
      "    accuracy                       0.941860       430\n",
      "   macro avg   0.948915  0.931202  0.938322       430\n",
      "weighted avg   0.943859  0.941860  0.941276       430\n",
      "\n",
      "[!] epoch = 8, auc = 0.989656571119524, best auc 0.9920452496845141\n",
      "[!] epoch = 8, acc = 0.9418604651162791\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 454s 1s/step - loss: 0.0373\n",
      "[!] best accurary&threshold = (0.958139534883721, 0.445)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.954545  0.976744  0.965517       258\n",
      "           1   0.963855  0.930233  0.946746       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.959200  0.953488  0.956131       430\n",
      "weighted avg   0.958269  0.958140  0.958009       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3862456977367401\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.954545  0.976744  0.965517       258\n",
      "           1   0.963855  0.930233  0.946746       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.959200  0.953488  0.956131       430\n",
      "weighted avg   0.958269  0.958140  0.958009       430\n",
      "\n",
      "[!] epoch = 9, auc = 0.9882819542094826, best auc 0.9920452496845141\n",
      "[!] epoch = 9, acc = 0.958139534883721\n",
      "\n",
      "Epoch 10/10\n",
      "323/323 [==============================] - 455s 1s/step - loss: 0.0290\n",
      "[!] best accurary&threshold = (0.958139534883721, 0.343)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.954545  0.976744  0.965517       258\n",
      "           1   0.963855  0.930233  0.946746       172\n",
      "\n",
      "    accuracy                       0.958140       430\n",
      "   macro avg   0.959200  0.953488  0.956131       430\n",
      "weighted avg   0.958269  0.958140  0.958009       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3796706199645996\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.947566  0.980620  0.963810       258\n",
      "           1   0.969325  0.918605  0.943284       172\n",
      "\n",
      "    accuracy                       0.955814       430\n",
      "   macro avg   0.958445  0.949612  0.953547       430\n",
      "weighted avg   0.956269  0.955814  0.955599       430\n",
      "\n",
      "[!] epoch = 10, auc = 0.9898819181539571, best auc 0.9920452496845141\n",
      "[!] epoch = 10, acc = 0.9558139534883721\n",
      "\n",
      "[0.9875833784027404, 0.9867270596718948, 0.9906480980710294, 0.9895213628988643, 0.9903776816297098, 0.9886650441680187, 0.9920452496845141, 0.989656571119524, 0.9882819542094826, 0.9898819181539571] 0.9920452496845141\n",
      "[4867.630635261536] finish fold_id = 13\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "[!] start fold_id = 14 (10319, 5) (430, 5)\n",
      "{'base_dir': '../../../chinese_bert/UER-py/UER-large/', 'span_mode': True, 'lr': 9e-06, 'min_lr': 6e-08, 'ch_type': 'tx_ft', 'trainable': True, 'bert_trainable': True, 'accum_step': 1, 'cls_num': 4, 'unit1': 128, 'unit2': 128, 'unit3': 512, 'conv_num': 128, 'maxlen': 60, 'adv_training': True, 'W2V_FILE': './word_embedding_matrix', 'use_embed': True, 'use_embed_v2': True, 'verbose': 'USE_v12_augm_seed2020', 'x_pad': 0, 'num_class': 2, 'filename': 'USE_v12_augm_seed2020_embed_v2_tx_ft_25_9e-06_adv_training_embed_v2', 'num_example': 10319}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, None, 500)    10530500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 1)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 501)    0           char_embed[0][0]                 \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 501)    0           concatenate_1[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 128)    646144      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             324009984   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 128)    0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 4224)   0           model_2[1][0]                    \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 4224)   0           concatenate_2[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 1024)   19406848    lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "extract_layer (Lambda)          (None, 1024)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "po1_logit (Dense)               (None, 1)            1025        extract_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "po1 (Activation)                (None, 1)            0           po1_logit[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 354,594,501\n",
      "Trainable params: 354,594,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[!] using adv_training\n",
      "Epoch 1/10\n",
      "[!] test load&save model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 540s 2s/step - loss: 0.4726\n",
      "[!] best accurary&threshold = (0.9209302325581395, 0.744)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.948000  0.918605  0.933071       258\n",
      "           1   0.883333  0.924419  0.903409       172\n",
      "\n",
      "    accuracy                       0.920930       430\n",
      "   macro avg   0.915667  0.921512  0.918240       430\n",
      "weighted avg   0.922133  0.920930  0.921206       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.4730384349822998\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.982301  0.860465  0.917355       258\n",
      "           1   0.823529  0.976744  0.893617       172\n",
      "\n",
      "    accuracy                       0.906977       430\n",
      "   macro avg   0.902915  0.918605  0.905486       430\n",
      "weighted avg   0.918792  0.906977  0.907860       430\n",
      "\n",
      "[!] epoch = 1, new best_auc = 0.9751892915089238\n",
      "[!] epoch = 1, auc = 0.9751892915089238, best auc 0.9751892915089238\n",
      "[!] epoch = 1, acc = 0.9069767441860465\n",
      "\n",
      "Epoch 2/10\n",
      "323/323 [==============================] - 456s 1s/step - loss: 0.2196\n",
      "[!] best accurary&threshold = (0.9162790697674419, 0.436)\n",
      "[!] best threshold classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.944000  0.914729  0.929134       258\n",
      "           1   0.877778  0.918605  0.897727       172\n",
      "\n",
      "    accuracy                       0.916279       430\n",
      "   macro avg   0.910889  0.916667  0.913431       430\n",
      "weighted avg   0.917511  0.916279  0.916571       430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[!] np.mean(preds) = 0.3955332934856415\n",
      "[!] classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.926357  0.926357  0.926357       258\n",
      "           1   0.889535  0.889535  0.889535       172\n",
      "\n",
      "    accuracy                       0.911628       430\n",
      "   macro avg   0.907946  0.907946  0.907946       430\n",
      "weighted avg   0.911628  0.911628  0.911628       430\n",
      "\n",
      "[!] epoch = 2, new best_auc = 0.9773976924463674\n",
      "[!] epoch = 2, auc = 0.9773976924463674, best auc 0.9773976924463674\n",
      "[!] epoch = 2, acc = 0.9116279069767442\n",
      "\n",
      "Epoch 3/10\n",
      "111/323 [=========>....................] - ETA: 4:57 - loss: 0.1623"
     ]
    }
   ],
   "source": [
    "adv_layer_names = ['Embedding-Token', 'char_embed']\n",
    "\n",
    "if -1 in FOLD_ID:\n",
    "    fold_id = -1\n",
    "    cfg[\"num_example\"] = len(train_data)\n",
    "    print(\"-\" * 81)\n",
    "    print(\"[!] start fold_id =\", fold_id, train_data.shape, dev_data.shape)\n",
    "    print(cfg)\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    train_D = data_generator(train_data)\n",
    "    seed(SEED + fold_id)\n",
    "    np.random.seed(SEED + fold_id)\n",
    "    tf.random.set_random_seed(SEED + fold_id)\n",
    "    model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix if cfg[\"use_embed\"] else None)\n",
    "    if cfg[\"adv_training\"]:\n",
    "        print(\"[!] using adv_training\")\n",
    "        adversarial_training(model, adv_layer_names, 0.5)\n",
    "    evaluator = Evaluate(filename=cfg[\"filename\"] + \"_fold{}\".format(fold_id), data=dev_data)\n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                              steps_per_epoch=len(train_D),\n",
    "                              epochs=RUN_EPOCH,\n",
    "                              callbacks=[evaluator],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    del model, train_data, dev_data\n",
    "    gc.collect()\n",
    "    print(\"[!] finish fold_id =\", fold_id)\n",
    "    print(\"-\" * 81)\n",
    "    \n",
    "\n",
    "skf = SKF(FOLD_NUM, shuffle=False, random_state=SEED)\n",
    "\n",
    "print(all_data.shape)\n",
    "_t0 = time()\n",
    "for fold_id, (trn_ind, val_ind) in enumerate(skf.split(range(len(all_data)), all_data[\"label\"])):\n",
    "    if fold_id not in FOLD_ID:\n",
    "        continue\n",
    "    t0 = time()\n",
    "    dev_data = all_data.iloc[val_ind].reset_index(drop=True)\n",
    "    train_data = all_data.iloc[trn_ind].reset_index(drop=True)\n",
    "    cfg[\"num_example\"] = len(train_data)\n",
    "    print(\"-\" * 81)\n",
    "    print(\"[!] start fold_id =\", fold_id, train_data.shape, dev_data.shape)\n",
    "    print(cfg)\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    train_D = data_generator(train_data)\n",
    "    seed(SEED + fold_id)\n",
    "    np.random.seed(SEED + fold_id)\n",
    "    tf.random.set_random_seed(SEED + fold_id)\n",
    "    model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix if cfg[\"use_embed\"] else None)\n",
    "    if cfg[\"adv_training\"]:\n",
    "        print(\"[!] using adv_training\")\n",
    "        adversarial_training(model, adv_layer_names, 0.5)\n",
    "    evaluator = Evaluate(filename=cfg[\"filename\"] + \"_fold{}\".format(fold_id), data=dev_data)\n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                              steps_per_epoch=len(train_D),\n",
    "                              epochs=RUN_EPOCH,\n",
    "                              callbacks=[evaluator],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    print(evaluator.F1, max(evaluator.F1))    \n",
    "    print(\"[{}] finish fold_id =\".format(time() - t0), fold_id)\n",
    "    print(\"-\" * 81)\n",
    "    del model, train_data, dev_data, evaluator\n",
    "    gc.collect()    \n",
    "print(\"[{}] finish =\".format(time() - _t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_model = get_model(model)\n",
    "evaluate(sub_model=sub_model, data=dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "def get_cnt(data, col1, col2, cate):\n",
    "    data = data[data[\"category\"] == cate]\n",
    "    if col2:\n",
    "        data = data[col1].tolist() + data[col2].tolist() \n",
    "    else:\n",
    "        data = data[col1].tolist()\n",
    "    data = [list(jieba.cut(e)) for e in data]\n",
    "    cnt1 = Counter([w for sent in data for w in sent])\n",
    "    return cnt1\n",
    "    \n",
    "    \n",
    "stop_words = ['？', '吗', '了', '，', '的', '?', '有', '得', '地', '是', '什么',\n",
    "              '怎么办', '哪些', '怎么回事', '怎么', '要', '能', '呢', '会']\n",
    "for cate in train_data[\"category\"].value_counts().index:\n",
    "    print(\"-\" * 40, cate, \"-\" * 40)\n",
    "    cnt1 = get_cnt(train_data, col1=\"query1\", col2=\"query2\", cate=cate)\n",
    "    cnt1 = [(k, cnt) for k, cnt in cnt1.most_common() if k not in stop_words]\n",
    "    print(cnt1[: 20])\n",
    "    print(\"-\" * 81)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in dev_data[\"category\"].value_counts().index:\n",
    "    print(\"------------------------\")\n",
    "    print(cate)\n",
    "    cnt1 = get_cnt(dev_data, col=\"query1\", cate=cate)\n",
    "    cnt2 = get_cnt(dev_data, col=\"query2\", cate=cate)\n",
    "    print(cnt1.most_common(10))\n",
    "    print(cnt2.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _foo(x, col):\n",
    "    cate = x[\"category\"]\n",
    "    q = x[col]\n",
    "    if cate == \"咳血\":\n",
    "        return q.count(\"咯血\") + q.count(\"咳血\")\n",
    "    \n",
    "    \n",
    "    return q.count(cate)\n",
    "\n",
    "    \n",
    "train_data[\"q1_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query1\"), axis=1)\n",
    "train_data[\"q2_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query2\"), axis=1)\n",
    "train_data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
