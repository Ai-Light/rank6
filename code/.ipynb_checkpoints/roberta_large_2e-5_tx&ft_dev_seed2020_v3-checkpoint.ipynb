{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from random import choice, seed, randint, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, CuDNNGRU as GRU, CuDNNLSTM as LSTM, Dropout, BatchNormalization\n",
    "from keras.layers import Dense, Concatenate, Activation, Embedding, SpatialDropout1D, Bidirectional, Lambda, Conv1D\n",
    "from keras.layers import Add, Average\n",
    "from keras.optimizers import Nadam, Adam, Adamax\n",
    "from keras.activations import absolute_import\n",
    "from keras.legacy import interfaces\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold as KF\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from keras_bert.loader import load_trained_model_from_checkpoint\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.objectives import sparse_categorical_crossentropy\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from special_tokens import CHINESE_MAP\n",
    "from metric_utils import compute_f1, compute_exact\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = \"../../../chinese_bert/chinese_roberta_wwm_large_ext_L-24_H-1024_A-16/\"\n",
    "TRN_FILENAME = \"../data/train_20200228.csv\"\n",
    "DEV_FILENAME = \"../data/dev_20200228.csv\"\n",
    "PREFIX = \"roberta_large_v2_augm\"\n",
    "MAX_EPOCH = 20\n",
    "MAX_LEN = 60\n",
    "MAX_DOC_LEN = MAX_LEN // 2\n",
    "THRE = 0.5\n",
    "B_SIZE = 32\n",
    "ACCUM_STEP = int(32 // B_SIZE)\n",
    "FOLD_ID = [-1]\n",
    "FOLD_NUM = 25\n",
    "SEED = 2020\n",
    "PREFIX += \"_seed\" + str(SEED)\n",
    "SHUFFLE = True\n",
    "DOC_STRIDE = 128\n",
    "cfg = {}\n",
    "cfg[\"verbose\"] = PREFIX\n",
    "cfg[\"base_dir\"] = BERT_PRETRAINED_DIR\n",
    "cfg[\"span_mode\"] = True\n",
    "cfg[\"lr\"] = 2e-5\n",
    "cfg['min_lr'] = 6e-8 \n",
    "cfg[\"ch_type\"] = \"tx_ft\"\n",
    "cfg[\"trainable\"] = True\n",
    "cfg[\"bert_trainable\"] = True\n",
    "cfg[\"accum_step\"] = ACCUM_STEP\n",
    "cfg[\"cls_num\"] = 4\n",
    "cfg[\"unit1\"] = 128\n",
    "cfg[\"unit2\"] = 128\n",
    "cfg[\"unit3\"] = 512\n",
    "cfg[\"conv_num\"] = 128\n",
    "cfg['maxlen'] = MAX_LEN\n",
    "cfg[\"adv_training\"] = False\n",
    "\n",
    "train_data = pd.read_csv(TRN_FILENAME)\n",
    "train_data.fillna(\"\", inplace=True)\n",
    "dev_data = pd.read_csv(DEV_FILENAME)\n",
    "dev_data.fillna(\"\", inplace=True)\n",
    "all_data = pd.concat([train_data, dev_data], axis=0, ignore_index=True)\n",
    "\n",
    "def get_data(df_data):\n",
    "\n",
    "    df_gb = df_data.groupby('query1')\n",
    "    res = {}\n",
    "    for index, data in df_gb:\n",
    "        query2s = data[\"query2\"]\n",
    "        lables = data[\"label\"]\n",
    "        ele = {}\n",
    "        pos_qs = []\n",
    "        neg_qs = []\n",
    "        for q, lable in zip(query2s, lables):\n",
    "            if lable == 1:\n",
    "                pos_qs.append(q)\n",
    "            elif lable == 0:\n",
    "                neg_qs.append(q)\n",
    "            else:\n",
    "                print(\"wrong data\", index, q, lable)\n",
    "        ele[\"pos\"] = pos_qs\n",
    "        ele[\"neg\"] = neg_qs\n",
    "        res[index] = ele\n",
    "    return res\n",
    "\n",
    "# train_data_dict = get_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1} {0: '0', 1: '1'} ['0', '1']\n"
     ]
    }
   ],
   "source": [
    "def get_vocab():\n",
    "    if \"albert\"in cfg[\"verbose\"].lower():\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab_chinese.txt')\n",
    "    else:\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "    with open(dict_path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "\n",
    "    word_index = {v: k  for k, v in enumerate(lines)}\n",
    "    for k, v in CHINESE_MAP.items():\n",
    "        assert v in word_index\n",
    "        del word_index[v]\n",
    "    return word_index\n",
    "\n",
    "\n",
    "def get_label():\n",
    "    labels = [\"0\", \"1\"]\n",
    "    label2id = {k: v for v, k in enumerate(labels)}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    return label2id, id2label, labels\n",
    "    \n",
    "    \n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=np.float16)\n",
    "\n",
    "\n",
    "def load_embed(path, dim=300, word_index=None):\n",
    "    embedding_index = {}\n",
    "    with open(path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.strip().split()\n",
    "            word, arr = l[0], l[1:]\n",
    "            if len(arr) != dim:\n",
    "                print(\"[!] l = {}\".format(l))\n",
    "                continue\n",
    "            if word_index and word not in word_index:\n",
    "                continue\n",
    "            word, arr = get_coefs(word, arr)\n",
    "            embedding_index[word] = arr\n",
    "    return embedding_index\n",
    "\n",
    "\n",
    "def build_matrix(path, word_index=None, max_features=None, dim=300):\n",
    "    embedding_index = load_embed(path, dim=dim, word_index=word_index)\n",
    "    max_features = len(word_index) + 1 if max_features is None else max_features \n",
    "    embedding_matrix = np.zeros((max_features + 1, dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_word_embed(word_embed_f1=\"../../../chinese_embedding/Tencent_AILab_ChineseEmbedding.txt\", \n",
    "               word_embed_f2=\"../../../chinese_embedding/cc.zh.300.vec\", \n",
    "               save_filename=\"./word_embedding_matrix\",\n",
    "               word_index=None):\n",
    "    if os.path.exists(save_filename + \".npy\"):\n",
    "        word_embedding_matrix = np.load(save_filename + \".npy\").astype(\"float32\")\n",
    "    else:\n",
    "        if \"tx\" in cfg[\"ch_type\"]:\n",
    "            tx_embed, tx_unk = build_matrix(word_embed_f1, word_index=word_index, dim=200)\n",
    "        else:\n",
    "            tx_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            tx_unk = []\n",
    "        if \"ft\" in cfg[\"ch_type\"]:\n",
    "            ft_embed, ft_unk = build_matrix(word_embed_f2, word_index=word_index, dim=300)\n",
    "        else:\n",
    "            ft_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            ft_unk = []    \n",
    "\n",
    "        word_embedding_matrix = np.concatenate([tx_embed, ft_embed], axis=-1).astype(\"float32\")\n",
    "        print(word_embedding_matrix.shape, len(tx_unk), len(ft_unk))\n",
    "        np.save(save_filename, word_embedding_matrix )\n",
    "    return word_embedding_matrix\n",
    "    \n",
    "    \n",
    "word_index = get_vocab()\n",
    "label2id, id2label, labels = get_label()\n",
    "word_embedding_matrix = load_word_embed(word_index=word_index)\n",
    "\n",
    "NUM_CLASS = len(label2id)\n",
    "cfg[\"x_pad\"] = word_index[\"[PAD]\"]\n",
    "cfg[\"num_class\"] = NUM_CLASS\n",
    "cfg[\"filename\"] = \"{}_{}_{}_{}\".format(PREFIX, cfg[\"ch_type\"], FOLD_NUM, cfg[\"lr\"])\n",
    "cfg[\"filename\"] = cfg[\"filename\"] + \"_adv_training\" if cfg[\"adv_training\"] else cfg[\"filename\"]\n",
    "print(label2id, id2label, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(cfg, summary=False, word_embedding_matrix=None):\n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        if \"albert\"in cfg[\"verbose\"].lower():\n",
    "            from bert4keras.bert import build_bert_model\n",
    "            config_file = os.path.join(base_dir, 'albert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'model.ckpt-best')\n",
    "            model = build_bert_model(\n",
    "                    config_path=config_file,\n",
    "                    checkpoint_path=checkpoint_file,\n",
    "                    model='albert',\n",
    "                    return_keras_model=True\n",
    "            )\n",
    "            if cfg_[\"cls_num\"] > 1:\n",
    "                output = Concatenate(axis=-1)([model.get_layer(\"Encoder-1-FeedForward-Norm\").get_output_at(-i) for i in range(1, cfg[\"cls_num\"] + 1)])\n",
    "                model = Model(model.inputs[: 2], outputs=output)\n",
    "            model.trainable = cfg_[\"bert_trainable\"]\n",
    "        else:\n",
    "            config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "            if not os.path.exists(config_file):\n",
    "                config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "                checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')            \n",
    "            model = load_trained_model_from_checkpoint(config_file, \n",
    "                                                       checkpoint_file, \n",
    "                                                       training=False, \n",
    "                                                       trainable=cfg_[\"bert_trainable\"], \n",
    "                                                       output_layer_num=cfg_[\"cls_num\"],\n",
    "                                                       seq_len=cfg_['maxlen'])\n",
    "            \n",
    "            # model = Model(inputs=model.inputs[: 2], outputs=model.layers[-7].output)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def _get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        total_steps, warmup_steps = calc_train_steps(\n",
    "            num_example=num_example,\n",
    "            batch_size=B_SIZE,\n",
    "            epochs=MAX_EPOCH,\n",
    "            warmup_proportion=warmup_proportion,\n",
    "        )\n",
    "        opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "        if cfg.get(\"accum_step\", None) and cfg[\"accum_step\"] > 1:\n",
    "            print(\"[!] using accum_step = {}\".format(cfg[\"accum_step\"]))\n",
    "            from accum_optimizer import AccumOptimizer\n",
    "            opt = AccumOptimizer(opt, steps_per_update=cfg[\"accum_step\"])\n",
    "        \n",
    "        return opt\n",
    "\n",
    "    bert_model = _get_model(cfg[\"base_dir\"], cfg)\n",
    "\n",
    "    if word_embedding_matrix is not None:\n",
    "        embed = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                          output_dim=word_embedding_matrix.shape[1],\n",
    "                          weights=[word_embedding_matrix],\n",
    "                          trainable=cfg[\"trainable\"],\n",
    "                          name=\"char_embed\"\n",
    "                         )\n",
    "    \n",
    "    t1_in = Input(shape=(None, ))\n",
    "    t2_in = Input(shape=(None, ))\n",
    "    o1_in = Input(shape=(1, ))\n",
    "    o2_in = Input(shape=(1, ))\n",
    "\n",
    "    t1, t2, o1, o2 = t1_in, t2_in, o1_in, o2_in\n",
    "    \n",
    "    ## Char information\n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'))(t1)\n",
    "    word_embed = embed(t1)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    word_embed = Bidirectional(LSTM(cfg[\"unit1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    \n",
    "    t = bert_model([t1, t2])\n",
    "    t = Concatenate(axis=-1)([t, word_embed])\n",
    "    t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask]) \n",
    "    t = Bidirectional(LSTM(cfg[\"unit3\"], return_sequences=True), merge_mode=\"concat\")(t)\n",
    "    # t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask]) \n",
    "    # t = Conv1D(cfg[\"conv_num\"], kernel_size=3, padding=\"same\")(t) \n",
    "    t = Lambda(lambda x: x[:, 0, :], name=\"extract_layer\")(t)\n",
    "    if cfg.get(\"num_class\", 1) == 2:\n",
    "        po1_logit = Dense(1, name=\"po1_logit\")(t)\n",
    "        po1 = Activation('sigmoid', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])        \n",
    "        o1_loss = K.binary_crossentropy(o1, po1)\n",
    "        loss = K.mean(o1_loss)\n",
    "    else:\n",
    "        po1_logit = Dense(cfg[\"num_class\"], name=\"po1_logit\")(t)\n",
    "        po1 = Activation('softmax', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])\n",
    "        loss = K.categorical_crossentropy(o1, po1, axis=-1)\n",
    "        loss = K.mean(loss)\n",
    "\n",
    "    train_model.add_loss(loss)\n",
    "    opt = _get_opt(num_example=cfg[\"num_example\"], lr=cfg[\"lr\"], min_lr=cfg['min_lr'])\n",
    "    train_model.compile(optimizer=opt)\n",
    "    if summary:\n",
    "        train_model.summary()\n",
    "    return train_model\n",
    "\n",
    "\n",
    "# print(\"----------------build model ---------------\")\n",
    "# model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2id_X(x, x_dict, x2=None, maxlen=None, maxlen1=None):\n",
    "    if x2:\n",
    "        x1 = x\n",
    "        del x\n",
    "        maxlen -= 3\n",
    "        maxlen1 -= 2\n",
    "        assert maxlen > maxlen1\n",
    "        maxlen2 = maxlen - maxlen1 - 1\n",
    "        x1 = [\"[CLS]\"] + list(x1)[: maxlen1] + [\"[SEP]\"] \n",
    "        x1 = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x1]\n",
    "        seg1= [0 for _ in x1]\n",
    "        \n",
    "        x2 = list(x2)[: maxlen2] + [\"[SEP]\"] \n",
    "        x2= [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x2]\n",
    "        seg2 = [1 for _ in x2]\n",
    "        x = x1 + x2\n",
    "        seg = seg1 + seg2\n",
    "        \n",
    "    else:\n",
    "        maxlen -= 2\n",
    "        x = [\"[CLS]\"] + list(x)[: maxlen] + [\"[SEP]\"] \n",
    "        x = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x]\n",
    "        seg = [0 for _ in x]        \n",
    "    return x, seg\n",
    "\n",
    "\n",
    "def seq_padding(X, maxlen=None, padding_value=None, debug=False):\n",
    "    L = [len(x) for x in X]\n",
    "    if maxlen is None:\n",
    "        maxlen = max(L)\n",
    "\n",
    "    pad_X = np.array([\n",
    "        np.concatenate([x, [padding_value] * (maxlen - len(x))]) if len(x) < maxlen else x for x in X\n",
    "    ])\n",
    "    if debug:\n",
    "        print(\"[!] before pading {}\\n\".format(X))\n",
    "        print(\"[!] after pading {}\\n\".format(pad_X))\n",
    "    return pad_X\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    \n",
    "    def __init__(self, data, batch_size=B_SIZE, shuffle=SHUFFLE, augm_frac=0.75):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = cfg[\"num_example\"] // self.batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_dict = get_data(data)\n",
    "        self.augm_frac = augm_frac\n",
    "        if cfg[\"num_example\"] % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "            T1, T2, O1, O2 = [], [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data.iloc[i]\n",
    "                text = d[\"query1\"]\n",
    "                label_text = d[\"query2\"]\n",
    "                o1 = d[\"label\"]\n",
    "                \n",
    "                if random() > self.augm_frac:\n",
    "                    data_d = self.data_dict[text]\n",
    "                    pos_data = data_d[\"pos\"]\n",
    "                    neg_data = data_d[\"neg\"]\n",
    "                    if pos_data and neg_data:\n",
    "                        if random() > 0.5:\n",
    "                            o1 = 1\n",
    "                            label_text = choice(pos_data)\n",
    "                            if len(pos_data) >= 2:\n",
    "                                _pos_data = [e for e in pos_data if e != label_text]\n",
    "                                text = choice(_pos_data)\n",
    "                        else:\n",
    "                            o1 = 0\n",
    "                            text = choice(pos_data)\n",
    "                            label_text = choice(neg_data)   \n",
    "                \n",
    "                if random() > 0.5:\n",
    "                    text, label_text = label_text, text\n",
    "                \n",
    "                if o1 == \"\":\n",
    "                    continue\n",
    "                o1 = float(o1)\n",
    "                assert 0 <= o1 <= 1\n",
    "                \n",
    "                O1.append(o1)                \n",
    "                t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "                assert len(t1) == len(t2)\n",
    "                \n",
    "                T1.append(t1)\n",
    "                T2.append(t2)\n",
    "\n",
    "                if len(T1) == self.batch_size or i == idxs[-1]:\n",
    "                    O1 = np.array(O1).reshape(-1, 1)\n",
    "                    T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "                    T2 = seq_padding(T2, padding_value=0)\n",
    "                    assert T1.shape == T2.shape and T1.shape[0] == O1.shape[0]\n",
    "\n",
    "                    yield [T1, T2, O1], None\n",
    "                    T1, T2, O1, = [], [], []\n",
    "                    \n",
    "                        \n",
    "# gen = data_generator(train_data)\n",
    "# for i, e in enumerate(gen):\n",
    "#     if i > 400:\n",
    "#         break\n",
    "#     print(\"i = {}\".format(i), \"-\" * 81)\n",
    "#     # print(e[0])\n",
    "#     for _e in e[0]:\n",
    "#         print(_e.shape, _e.sum(axis=0).sum(axis=0))\n",
    "# del gen\n",
    "# print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_):\n",
    "    model_inp_ind = [0, 1]\n",
    "    inputs = [model_.inputs[e] for e in model_inp_ind]\n",
    "    sub_model = Model(inputs=inputs, outputs=[model_.get_layer(\"po1\").output])\n",
    "    return sub_model\n",
    "\n",
    "\n",
    "def evaluate(sub_model, data, bs=32):\n",
    "    idxs = list(range(len(data)))\n",
    "    T1, T2, O1, O2 = [], [], [], []\n",
    "    preds = []\n",
    "    for i in idxs:\n",
    "        d = data.iloc[i]\n",
    "        text = d[\"query1\"]\n",
    "        label_text = d[\"query2\"]\n",
    "\n",
    "        t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "        assert len(t1) == len(t2)\n",
    "\n",
    "        T1.append(t1)\n",
    "        T2.append(t2)\n",
    "\n",
    "        o1 = float(d[\"label\"])\n",
    "        O1.append(o1)\n",
    "        if len(T1) == bs or i == idxs[-1]:\n",
    "            T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "            T2 = seq_padding(T2, padding_value=0)\n",
    "            assert T1.shape == T2.shape\n",
    "            pred = sub_model.predict([T1, T2])\n",
    "            preds.append(pred)\n",
    "            T1, T2 = [], []\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    O1 = np.array(O1).reshape(-1)\n",
    "    O1 = O1.astype(\"int32\")\n",
    "    auc = roc_auc_score(O1, preds)\n",
    "    acc = accuracy_score(O1, np.array(preds > 0.5, \"int32\"))\n",
    "    return auc, acc\n",
    "    \n",
    "\n",
    "class Evaluate(Callback):\n",
    "    def __init__(self, data, filename=None):\n",
    "        self.F1 = []\n",
    "        self.best = 0.\n",
    "        self.filename = filename\n",
    "        self.data = data\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch ==  0:\n",
    "            print(\"[!] test load&save model\")\n",
    "            f = self.filename + \".h5\"\n",
    "            self.model.save(f, include_optimizer=False, overwrite=False)\n",
    "            if \"albert\" in cfg[\"verbose\"]:\n",
    "                model_ = load_model(f) \n",
    "            else:\n",
    "                model_ = load_model(f, custom_objects=get_custom_objects()) \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch + 1 < 1:\n",
    "            return\n",
    "        if epoch + 1 in [3, 6, 9, 10, 12, 15, 18, 20]:\n",
    "            self.model.save(self.filename + \"_{}.h5\".format(epoch + 1), include_optimizer=False)\n",
    "            \n",
    "        sub_model = get_model(self.model)\n",
    "        f1, class_f1 = evaluate(sub_model, data=self.data)\n",
    "        self.F1.append(f1)\n",
    "        if f1 > self.best:\n",
    "            self.model.save(self.filename + \".h5\", include_optimizer=False)\n",
    "            \n",
    "        if f1 > self.best:\n",
    "            self.best = f1\n",
    "            print(\"[!] epoch = {}, new best_auc = {}\".format(epoch + 1,  f1))\n",
    "        print('[!] epoch = {}, auc = {}, best auc {}'.format(epoch + 1, f1, self.best))\n",
    "        print('[!] epoch = {}, acc = {}\\n'.format(epoch + 1, class_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_layer(inputs, name, exclude_from=None):\n",
    "    \"\"\"根据inputs和name来搜索层\n",
    "    说明：inputs为某个层或某个层的输出；name为目标层的名字。\n",
    "    实现：根据inputs一直往上递归搜索，直到发现名字为name的层为止；\n",
    "         如果找不到，那就返回None。\n",
    "    \"\"\"\n",
    "    if exclude_from is None:\n",
    "        exclude_from = set()\n",
    "\n",
    "    if isinstance(inputs, keras.layers.Layer):\n",
    "        layer = inputs\n",
    "    else:\n",
    "        layer = inputs._keras_history[0]\n",
    "\n",
    "    if layer.name == name:\n",
    "        return layer\n",
    "    elif layer in exclude_from:\n",
    "        return None\n",
    "    else:\n",
    "        exclude_from.add(layer)\n",
    "        if isinstance(layer, keras.models.Model):\n",
    "            model = layer\n",
    "            for layer in model.layers:\n",
    "                if layer.name == name:\n",
    "                    return layer\n",
    "        inbound_layers = layer._inbound_nodes[0].inbound_layers\n",
    "        if not isinstance(inbound_layers, list):\n",
    "            inbound_layers = [inbound_layers]\n",
    "        if len(inbound_layers) > 0:\n",
    "            for layer in inbound_layers:\n",
    "                layer = search_layer(layer, name, exclude_from)\n",
    "                if layer is not None:\n",
    "                    return layer\n",
    "                \n",
    "def adversarial_training(model, embedding_names, epsilon=1):\n",
    "    \"\"\"给模型添加对抗训练\n",
    "    其中model是需要添加对抗训练的keras模型，embedding_names\n",
    "    则是model里边Embedding层的名字。要在模型compile之后使用。\n",
    "    \"\"\"\n",
    "    if model.train_function is None:  # 如果还没有训练函数\n",
    "        model._make_train_function()  # 手动make\n",
    "    old_train_function = model.train_function  # 备份旧的训练函数\n",
    "\n",
    "    # 查找Embedding层\n",
    "    embedding_layers = []\n",
    "    for embedding_name in embedding_names:\n",
    "        for output in model.outputs:\n",
    "            embedding_layer = search_layer(output, embedding_name)\n",
    "            if embedding_layer is not None:\n",
    "                embedding_layers.append(embedding_layer)\n",
    "                break\n",
    "    for embedding_layer in embedding_layers:\n",
    "        if embedding_layer is None:\n",
    "            raise Exception('Embedding layer not found')\n",
    "\n",
    "    # 求Embedding梯度\n",
    "    embeddings = [embedding_layer.embeddings for embedding_layer in embedding_layers] # Embedding矩阵\n",
    "    gradients = K.gradients(model.total_loss, embeddings)  # Embedding梯度\n",
    "    # gradients = K.zeros_like(embeddings) + gradients[0]  # 转为dense tensor\n",
    "    gradients = [K.zeros_like(embedding) + gradient for embedding, gradient in zip(embeddings, gradients)]\n",
    "\n",
    "    # 封装为函数\n",
    "    inputs = (model._feed_inputs +\n",
    "              model._feed_targets +\n",
    "              model._feed_sample_weights)  # 所有输入层\n",
    "    embedding_gradients = K.function(\n",
    "        inputs=inputs,\n",
    "        outputs=gradients,\n",
    "        name='embedding_gradients',\n",
    "    )  # 封装为函数\n",
    "\n",
    "    def train_function(inputs):  # 重新定义训练函数\n",
    "#         grads = embedding_gradients(inputs)[0]  # Embedding梯度\n",
    "#         delta = epsilon * grads / (np.sqrt((grads**2).sum()) + 1e-8)  # 计算扰动\n",
    "        grads = embedding_gradients(inputs)  # Embedding梯度\n",
    "        deltas = [epsilon * grad / (np.sqrt((grad**2).sum()) + 1e-8) for grad in grads]  # 计算扰动\n",
    "        # 注入扰动\n",
    "        # K.set_value(embeddings, K.eval(embeddings) + delta)  \n",
    "        for embedding, delta in zip(embeddings, deltas):\n",
    "            K.set_value(embedding, K.eval(embedding) + delta)\n",
    "            \n",
    "        outputs = old_train_function(inputs)  # 梯度下降\n",
    "        # 删除扰动\n",
    "        # K.set_value(embeddings, K.eval(embeddings) - delta)  # 删除扰动\n",
    "        for embedding, delta in zip(embeddings, deltas):\n",
    "            K.set_value(embedding, K.eval(embedding) - delta)       \n",
    "        return outputs\n",
    "\n",
    "    model.train_function = train_function  # 覆盖原训练函数\n",
    "\n",
    "\n",
    "# 写好函数后，启用对抗训练只需要一行代码\n",
    "# adversarial_training(model, 'Embedding-Token', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "[!] start fold_id = -1 (8747, 5) (2002, 5)\n",
      "{'verbose': 'roberta_large_v2_augm_seed2020', 'base_dir': '../../../chinese_bert/chinese_roberta_wwm_large_ext_L-24_H-1024_A-16/', 'span_mode': True, 'lr': 2e-05, 'min_lr': 6e-08, 'ch_type': 'tx_ft', 'trainable': True, 'bert_trainable': True, 'accum_step': 1, 'cls_num': 4, 'unit1': 128, 'unit2': 128, 'unit3': 512, 'conv_num': 128, 'maxlen': 60, 'adv_training': False, 'x_pad': 0, 'num_class': 2, 'filename': 'roberta_large_v2_augm_seed2020_tx_ft_25_2e-05', 'num_example': 8747}\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, None, 500)    10530500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 500)    0           char_embed[0][0]                 \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 128)    645120      lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             324009984   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 128)    0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 4224)   0           model_2[1][0]                    \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 4224)   0           concatenate_1[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 1024)   19406848    lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "extract_layer (Lambda)          (None, 1024)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "po1_logit (Dense)               (None, 1)            1025        extract_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "po1 (Activation)                (None, 1)            0           po1_logit[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 354,593,477\n",
      "Trainable params: 354,593,477\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10\n",
      "[!] test load&save model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/274 [==============================] - 279s 1s/step - loss: 0.4087\n",
      "[!] epoch = 1, new best_auc = 0.9790332560901669\n",
      "[!] epoch = 1, auc = 0.9790332560901669, best auc 0.9790332560901669\n",
      "[!] epoch = 1, acc = 0.9180819180819181\n",
      "\n",
      "Epoch 2/10\n",
      "274/274 [==============================] - 177s 645ms/step - loss: 0.1502\n",
      "[!] epoch = 2, new best_auc = 0.981319404646218\n",
      "[!] epoch = 2, auc = 0.981319404646218, best auc 0.981319404646218\n",
      "[!] epoch = 2, acc = 0.9325674325674326\n",
      "\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 178s 649ms/step - loss: 0.0995\n",
      "[!] epoch = 3, auc = 0.9790093771542953, best auc 0.981319404646218\n",
      "[!] epoch = 3, acc = 0.9365634365634365\n",
      "\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 177s 646ms/step - loss: 0.0699\n",
      "[!] epoch = 4, new best_auc = 0.9824531349928156\n",
      "[!] epoch = 4, auc = 0.9824531349928156, best auc 0.9824531349928156\n",
      "[!] epoch = 4, acc = 0.9385614385614386\n",
      "\n",
      "Epoch 5/10\n",
      "274/274 [==============================] - 179s 652ms/step - loss: 0.0483\n",
      "[!] epoch = 5, auc = 0.9793997458450652, best auc 0.9824531349928156\n",
      "[!] epoch = 5, acc = 0.9415584415584416\n",
      "\n",
      "Epoch 6/10\n",
      "274/274 [==============================] - 177s 647ms/step - loss: 0.0384\n",
      "[!] epoch = 6, auc = 0.9802022857332702, best auc 0.9824531349928156\n",
      "[!] epoch = 6, acc = 0.9425574425574426\n",
      "\n",
      "Epoch 7/10\n",
      "274/274 [==============================] - 176s 644ms/step - loss: 0.0260\n",
      "[!] epoch = 7, auc = 0.9790555777041339, best auc 0.9824531349928156\n",
      "[!] epoch = 7, acc = 0.9445554445554446\n",
      "\n",
      "Epoch 8/10\n",
      "274/274 [==============================] - 176s 644ms/step - loss: 0.0192\n",
      "[!] epoch = 8, auc = 0.9809113863071952, best auc 0.9824531349928156\n",
      "[!] epoch = 8, acc = 0.9425574425574426\n",
      "\n",
      "Epoch 9/10\n",
      "274/274 [==============================] - 177s 648ms/step - loss: 0.0199\n",
      "[!] epoch = 9, auc = 0.9795554780355318, best auc 0.9824531349928156\n",
      "[!] epoch = 9, acc = 0.9410589410589411\n",
      "\n",
      "Epoch 10/10\n",
      "274/274 [==============================] - 179s 654ms/step - loss: 0.0095\n",
      "[!] epoch = 10, auc = 0.9800071013878853, best auc 0.9824531349928156\n",
      "[!] epoch = 10, acc = 0.942057942057942\n",
      "\n",
      "[!] finish fold_id = -1\n",
      "---------------------------------------------------------------------------------\n",
      "(10749, 5)\n"
     ]
    }
   ],
   "source": [
    "adv_layer_names = ['Embedding-Token', 'char_embed']\n",
    "\n",
    "if -1 in FOLD_ID:\n",
    "    fold_id = -1\n",
    "    cfg[\"num_example\"] = len(train_data)\n",
    "    print(\"-\" * 81)\n",
    "    print(\"[!] start fold_id =\", fold_id, train_data.shape, dev_data.shape)\n",
    "    print(cfg)\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    train_D = data_generator(train_data)\n",
    "    seed(SEED + fold_id)\n",
    "    np.random.seed(SEED + fold_id)\n",
    "    tf.random.set_random_seed(SEED + fold_id)\n",
    "    model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "    if cfg[\"adv_training\"]:\n",
    "        print(\"[!] using adv_training\")\n",
    "        adversarial_training(model, adv_layer_names, 0.5)\n",
    "    evaluator = Evaluate(filename=cfg[\"filename\"] + \"_fold{}\".format(fold_id), data=dev_data)\n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                              steps_per_epoch=len(train_D),\n",
    "                              epochs=MAX_EPOCH,\n",
    "                              callbacks=[evaluator],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    del model, train_data, dev_data\n",
    "    gc.collect()\n",
    "    print(\"[!] finish fold_id =\", fold_id)\n",
    "    print(\"-\" * 81)\n",
    "\n",
    "\n",
    "skf = SKF(FOLD_NUM, shuffle=True, random_state=SEED)\n",
    "print(all_data.shape)\n",
    "for fold_id, (trn_ind, val_ind) in enumerate(skf.split(range(len(all_data)), all_data[\"label\"])):\n",
    "    if fold_id not in FOLD_ID:\n",
    "        continue\n",
    "    \n",
    "    dev_data = all_data.iloc[val_ind].reset_index(drop=True)\n",
    "    train_data = all_data.iloc[trn_ind].reset_index(drop=True)\n",
    "    cfg[\"num_example\"] = len(train_data)\n",
    "    print(\"-\" * 81)\n",
    "    print(\"[!] start fold_id =\", fold_id, train_data.shape, dev_data.shape)\n",
    "    print(cfg)\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    train_D = data_generator(train_data)\n",
    "    seed(SEED + fold_id)\n",
    "    np.random.seed(SEED + fold_id)\n",
    "    tf.random.set_random_seed(SEED + fold_id)\n",
    "    model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "    if cfg[\"adv_training\"]:\n",
    "        print(\"[!] using adv_training\")\n",
    "        adversarial_training(model, adv_layer_names, 0.5)\n",
    "    evaluator = Evaluate(filename=cfg[\"filename\"] + \"_fold{}\".format(fold_id), data=dev_data)\n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                              steps_per_epoch=len(train_D),\n",
    "                              epochs=MAX_EPOCH,\n",
    "                              callbacks=[evaluator],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    del model, train_data, dev_data\n",
    "    gc.collect()\n",
    "    print(\"[!] finish fold_id =\", fold_id)\n",
    "    print(\"-\" * 81)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9790332560901669,\n",
       "  0.981319404646218,\n",
       "  0.9790093771542953,\n",
       "  0.9824531349928156,\n",
       "  0.9793997458450652,\n",
       "  0.9802022857332702,\n",
       "  0.9790555777041339,\n",
       "  0.9809113863071952,\n",
       "  0.9795554780355318,\n",
       "  0.9800071013878853],\n",
       " 0.9824531349928156)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.F1, max(evaluator.F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert_xxlarge_tx_ft_5fold-1_2e-05.h5 False\n",
      "albert_xxlarge_tx_ft_5fold-1_2e-05_6.h5 False\n",
      "roberta_base_tx_ft_5fold-1_2e-05.h5 False\n",
      "roberta_base_tx_ft_5fold-1_2e-05_10.h5 False\n",
      "roberta_large_tx_ft_5fold-1_2e-05.h5 False\n",
      "roberta_large_tx_ft_5fold-1_2e-05_10.h5 False\n",
      "UER_large_tx_ft_5fold-1_6e-06.h5 False\n",
      "UER_large_tx_ft_5fold-1_6e-06_10.h5 False\n",
      "roberta_large_v2_tx_ft_5fold-1_2e-05.h5 False\n",
      "roberta_large_v2_tx_ft_5fold-1_2e-05_10.h5 False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-01d2d30dbdb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mO1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test(sub_model, data, bs=32):\n",
    "    idxs = list(range(len(data)))\n",
    "    T1, T2, O1, O2 = [], [], [], []\n",
    "    preds = []\n",
    "    for i in idxs:\n",
    "        d = data.iloc[i]\n",
    "        text = d[\"query1\"]\n",
    "        label_text = d[\"query2\"]\n",
    "\n",
    "        t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "        assert len(t1) == len(t2)\n",
    "\n",
    "        T1.append(t1)\n",
    "        T2.append(t2)\n",
    "\n",
    "        if len(T1) == bs or i == idxs[-1]:\n",
    "            T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "            T2 = seq_padding(T2, padding_value=0)\n",
    "            assert T1.shape == T2.shape\n",
    "            pred = sub_model.predict([T1, T2])\n",
    "            preds.append(pred)\n",
    "            T1, T2 = [], []\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def ensemble_predictions(predictions, weights=None, type_=\"linear\"):\n",
    "    if not weights:\n",
    "        # print(\"[!] AVE_WGT\")\n",
    "        weights = [1./ len(predictions) for _ in range(len(predictions))]\n",
    "    assert len(predictions) == len(weights)\n",
    "    if np.sum(weights) != 1.0:\n",
    "        weights = [w / np.sum(weights) for w in weights]\n",
    "    # print(\"[!] weights = {}\".format(weights))\n",
    "    assert np.isclose(np.sum(weights), 1.0)\n",
    "    if type_ == \"linear\":\n",
    "        res = np.average(predictions, weights=weights, axis=0)\n",
    "    elif type_ == \"harmonic\":\n",
    "        res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n",
    "        return 1 / res\n",
    "    elif type_ == \"geometric\":\n",
    "        numerator = np.average(\n",
    "            [np.log(p) for p in predictions], weights=weights, axis=0\n",
    "        )\n",
    "        res = np.exp(numerator / sum(weights))\n",
    "        return res\n",
    "    elif type_ == \"rank\":\n",
    "        from scipy.stats import rankdata\n",
    "        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n",
    "        return res / (len(res) + 1)\n",
    "    return res\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "\n",
    "model_files =[\n",
    "            \"albert_xxlarge_tx_ft_5fold-1_2e-05.h5\",\n",
    "            \"albert_xxlarge_tx_ft_5fold-1_2e-05_6.h5\",\n",
    "            \"roberta_base_tx_ft_5fold-1_2e-05.h5\", \n",
    "            \"roberta_base_tx_ft_5fold-1_2e-05_10.h5\", \n",
    "            \"roberta_large_tx_ft_5fold-1_2e-05.h5\",\n",
    "            \"roberta_large_tx_ft_5fold-1_2e-05_10.h5\",\n",
    "            \"UER_large_tx_ft_5fold-1_6e-06.h5\",\n",
    "            \"UER_large_tx_ft_5fold-1_6e-06_10.h5\",\n",
    "            \"roberta_large_v2_tx_ft_5fold-1_2e-05.h5\",    \n",
    "            \"roberta_large_v2_tx_ft_5fold-1_2e-05_10.h5\", \n",
    "             ]\n",
    "\n",
    "for f in model_files:\n",
    "    print(f, os.path.exists(f))\n",
    "assert len(model_files) == len(set(model_files)) \n",
    "assert all([os.path.exists(f) for f in model_files]) \n",
    "preds = []\n",
    "O1 = dev_data[\"label\"].values.reshape(-1)\n",
    "for f in model_files:\n",
    "    print(\"-\" * 80)\n",
    "    K.clear_session()\n",
    "    t0 = time()\n",
    "    print(\"[!]\", f)\n",
    "    if \"albert\" in f:\n",
    "        model = load_model(f)\n",
    "    else:\n",
    "        model = load_model(f, custom_objects=get_custom_objects())\n",
    "    sub_model = get_model(model)\n",
    "    pred = test(sub_model, dev_data)\n",
    "    auc = roc_auc_score(O1, pred)\n",
    "    acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "    print(\"[{}]\".format(time() - t0), auc, acc)\n",
    "    print(\"-\" * 80)\n",
    "    preds.append(pred)\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "pred = ensemble_predictions(preds)\n",
    "print(pred.shape)\n",
    "auc = roc_auc_score(O1, pred)\n",
    "acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "print(auc, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred1 = ensemble_predictions(preds[0::2])\n",
    "pred2 = ensemble_predictions(preds[1::2])\n",
    "print(pred2[: 3], pred1[: 3])\n",
    "for i in range(1, 100):\n",
    "    wgt = i / 100\n",
    "    pred = ensemble_predictions([pred1, pred2], weights=[wgt, 1 - wgt], type_=\"geometric\")\n",
    "    auc = roc_auc_score(O1, pred)\n",
    "    acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "    print(wgt, auc, acc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "def get_cnt(data, col1, col2, cate):\n",
    "    data = data[data[\"category\"] == cate]\n",
    "    if col2:\n",
    "        data = data[col1].tolist() + data[col2].tolist() \n",
    "    else:\n",
    "        data = data[col1].tolist()\n",
    "    data = [list(jieba.cut(e)) for e in data]\n",
    "    cnt1 = Counter([w for sent in data for w in sent])\n",
    "    return cnt1\n",
    "    \n",
    "    \n",
    "stop_words = ['？', '吗', '了', '，', '的', '?', '有', '得', '地', '是', '什么',\n",
    "              '怎么办', '哪些', '怎么回事', '怎么', '要', '能', '呢', '会']\n",
    "for cate in train_data[\"category\"].value_counts().index:\n",
    "    print(\"-\" * 40, cate, \"-\" * 40)\n",
    "    cnt1 = get_cnt(train_data, col1=\"query1\", col2=\"query2\", cate=cate)\n",
    "    cnt1 = [(k, cnt) for k, cnt in cnt1.most_common() if k not in stop_words]\n",
    "    print(cnt1[: 20])\n",
    "    print(\"-\" * 81)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in dev_data[\"category\"].value_counts().index:\n",
    "    print(\"------------------------\")\n",
    "    print(cate)\n",
    "    cnt1 = get_cnt(dev_data, col=\"query1\", cate=cate)\n",
    "    cnt2 = get_cnt(dev_data, col=\"query2\", cate=cate)\n",
    "    print(cnt1.most_common(10))\n",
    "    print(cnt2.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _foo(x, col):\n",
    "    cate = x[\"category\"]\n",
    "    q = x[col]\n",
    "    if cate == \"咳血\":\n",
    "        return q.count(\"咯血\") + q.count(\"咳血\")\n",
    "    \n",
    "    \n",
    "    return q.count(cate)\n",
    "\n",
    "\n",
    "    \n",
    "train_data[\"q1_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query1\"), axis=1)\n",
    "train_data[\"q2_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query2\"), axis=1)\n",
    "train_data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
