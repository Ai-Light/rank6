{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from random import choice, seed, randint, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, CuDNNGRU as GRU, CuDNNLSTM as LSTM, Dropout, BatchNormalization\n",
    "from keras.layers import Dense, Concatenate, Activation, Embedding, SpatialDropout1D, Bidirectional, Lambda, Conv1D\n",
    "from keras.layers import Add, Average\n",
    "from keras.optimizers import Nadam, Adam, Adamax\n",
    "from keras.activations import absolute_import\n",
    "from keras.legacy import interfaces\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold as KF\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from keras_bert.loader import load_trained_model_from_checkpoint\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.objectives import sparse_categorical_crossentropy\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from special_tokens import CHINESE_MAP\n",
    "from metric_utils import compute_f1, compute_exact\n",
    "from collections import OrderedDict, Counter\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = \"../../../chinese_bert/RoBERTa-large-pair/\"\n",
    "TRN_FILENAME = \"../data/train_20200228.csv\"\n",
    "DEV_FILENAME = \"../data/dev_20200228.csv\"\n",
    "PREFIX = \"roberta_large_v2_augm\"\n",
    "MAX_EPOCH = 20\n",
    "MAX_LEN = 60\n",
    "MAX_DOC_LEN = MAX_LEN // 2\n",
    "THRE = 0.5\n",
    "B_SIZE = 32\n",
    "ACCUM_STEP = int(32 // B_SIZE)\n",
    "FOLD_ID = [-1]\n",
    "FOLD_NUM = 20\n",
    "SEED = 2020\n",
    "PREFIX += \"_seed\" + str(SEED)\n",
    "SHUFFLE = True\n",
    "DOC_STRIDE = 128\n",
    "cfg = {}\n",
    "cfg[\"verbose\"] = PREFIX\n",
    "cfg[\"base_dir\"] = BERT_PRETRAINED_DIR\n",
    "cfg[\"span_mode\"] = True\n",
    "cfg[\"lr\"] = 3e-6\n",
    "cfg['min_lr'] = 6e-8 \n",
    "cfg[\"ch_type\"] = \"tx_ft\"\n",
    "cfg[\"trainable\"] = True\n",
    "cfg[\"bert_trainable\"] = True\n",
    "cfg[\"accum_step\"] = ACCUM_STEP\n",
    "cfg[\"cls_num\"] = 4\n",
    "cfg[\"unit1\"] = 128\n",
    "cfg[\"unit2\"] = 128\n",
    "cfg[\"unit3\"] = 512\n",
    "cfg[\"conv_num\"] = 128\n",
    "cfg['maxlen'] = MAX_LEN\n",
    "cfg[\"adv_training\"] = False\n",
    "\n",
    "train_data = pd.read_csv(TRN_FILENAME)\n",
    "train_data.fillna(\"\", inplace=True)\n",
    "dev_data = pd.read_csv(DEV_FILENAME)\n",
    "dev_data.fillna(\"\", inplace=True)\n",
    "all_data = pd.concat([train_data, dev_data], axis=0, ignore_index=True)\n",
    "\n",
    "def get_data(df_data):\n",
    "\n",
    "    df_gb = df_data.groupby('query1')\n",
    "    res = {}\n",
    "    for index, data in df_gb:\n",
    "        query2s = data[\"query2\"]\n",
    "        lables = data[\"label\"]\n",
    "        ele = {}\n",
    "        pos_qs = []\n",
    "        neg_qs = []\n",
    "        for q, lable in zip(query2s, lables):\n",
    "            if lable == 1:\n",
    "                pos_qs.append(q)\n",
    "            elif lable == 0:\n",
    "                neg_qs.append(q)\n",
    "            else:\n",
    "                print(\"wrong data\", index, q, lable)\n",
    "        ele[\"pos\"] = pos_qs\n",
    "        ele[\"neg\"] = neg_qs\n",
    "        res[index] = ele\n",
    "    return res\n",
    "\n",
    "# train_data_dict = get_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1} {0: '0', 1: '1'} ['0', '1']\n"
     ]
    }
   ],
   "source": [
    "def get_vocab():\n",
    "    if \"albert\"in cfg[\"verbose\"].lower():\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab_chinese.txt')\n",
    "    else:\n",
    "        dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "    with open(dict_path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "\n",
    "    word_index = {v: k  for k, v in enumerate(lines)}\n",
    "    for k, v in CHINESE_MAP.items():\n",
    "        assert v in word_index\n",
    "        del word_index[v]\n",
    "    return word_index\n",
    "\n",
    "\n",
    "def get_label():\n",
    "    labels = [\"0\", \"1\"]\n",
    "    label2id = {k: v for v, k in enumerate(labels)}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    return label2id, id2label, labels\n",
    "    \n",
    "    \n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=np.float16)\n",
    "\n",
    "\n",
    "def load_embed(path, dim=300, word_index=None):\n",
    "    embedding_index = {}\n",
    "    with open(path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.strip().split()\n",
    "            word, arr = l[0], l[1:]\n",
    "            if len(arr) != dim:\n",
    "                print(\"[!] l = {}\".format(l))\n",
    "                continue\n",
    "            if word_index and word not in word_index:\n",
    "                continue\n",
    "            word, arr = get_coefs(word, arr)\n",
    "            embedding_index[word] = arr\n",
    "    return embedding_index\n",
    "\n",
    "\n",
    "def build_matrix(path, word_index=None, max_features=None, dim=300):\n",
    "    embedding_index = load_embed(path, dim=dim, word_index=word_index)\n",
    "    max_features = len(word_index) + 1 if max_features is None else max_features \n",
    "    embedding_matrix = np.zeros((max_features + 1, dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_word_embed(word_embed_f1=\"../../../chinese_embedding/Tencent_AILab_ChineseEmbedding.txt\", \n",
    "               word_embed_f2=\"../../../chinese_embedding/cc.zh.300.vec\", \n",
    "               save_filename=\"./word_embedding_matrix\",\n",
    "               word_index=None):\n",
    "    if os.path.exists(save_filename + \".npy\"):\n",
    "        word_embedding_matrix = np.load(save_filename + \".npy\").astype(\"float32\")\n",
    "    else:\n",
    "        if \"tx\" in cfg[\"ch_type\"]:\n",
    "            tx_embed, tx_unk = build_matrix(word_embed_f1, word_index=word_index, dim=200)\n",
    "        else:\n",
    "            tx_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            tx_unk = []\n",
    "        if \"ft\" in cfg[\"ch_type\"]:\n",
    "            ft_embed, ft_unk = build_matrix(word_embed_f2, word_index=word_index, dim=300)\n",
    "        else:\n",
    "            ft_embed = np.zeros(shape=(len(word_index) + 2, 0))\n",
    "            ft_unk = []    \n",
    "\n",
    "        word_embedding_matrix = np.concatenate([tx_embed, ft_embed], axis=-1).astype(\"float32\")\n",
    "        print(word_embedding_matrix.shape, len(tx_unk), len(ft_unk))\n",
    "        np.save(save_filename, word_embedding_matrix )\n",
    "    return word_embedding_matrix\n",
    "    \n",
    "    \n",
    "word_index = get_vocab()\n",
    "label2id, id2label, labels = get_label()\n",
    "word_embedding_matrix = load_word_embed(word_index=word_index)\n",
    "\n",
    "NUM_CLASS = len(label2id)\n",
    "cfg[\"x_pad\"] = word_index[\"[PAD]\"]\n",
    "cfg[\"num_class\"] = NUM_CLASS\n",
    "cfg[\"filename\"] = \"{}_{}_{}_{}\".format(PREFIX, cfg[\"ch_type\"], FOLD_NUM, cfg[\"lr\"])\n",
    "cfg[\"filename\"] = cfg[\"filename\"] + \"_adv_training\" if cfg[\"adv_training\"] else cfg[\"filename\"]\n",
    "print(label2id, id2label, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(cfg, summary=False, word_embedding_matrix=None):\n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        if \"albert\"in cfg[\"verbose\"].lower():\n",
    "            from bert4keras.bert import build_bert_model\n",
    "            config_file = os.path.join(base_dir, 'albert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'model.ckpt-best')\n",
    "            model = build_bert_model(\n",
    "                    config_path=config_file,\n",
    "                    checkpoint_path=checkpoint_file,\n",
    "                    model='albert',\n",
    "                    return_keras_model=True\n",
    "            )\n",
    "            if cfg_[\"cls_num\"] > 1:\n",
    "                output = Concatenate(axis=-1)([model.get_layer(\"Encoder-1-FeedForward-Norm\").get_output_at(-i) for i in range(1, cfg[\"cls_num\"] + 1)])\n",
    "                model = Model(model.inputs[: 2], outputs=output)\n",
    "            model.trainable = cfg_[\"bert_trainable\"]\n",
    "        else:\n",
    "            config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "            if not os.path.exists(config_file):\n",
    "                config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "                checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')            \n",
    "            model = load_trained_model_from_checkpoint(config_file, \n",
    "                                                       checkpoint_file, \n",
    "                                                       training=False, \n",
    "                                                       trainable=cfg_[\"bert_trainable\"], \n",
    "                                                       output_layer_num=cfg_[\"cls_num\"],\n",
    "                                                       seq_len=cfg_['maxlen'])\n",
    "            \n",
    "            # model = Model(inputs=model.inputs[: 2], outputs=model.layers[-7].output)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def _get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        total_steps, warmup_steps = calc_train_steps(\n",
    "            num_example=num_example,\n",
    "            batch_size=B_SIZE,\n",
    "            epochs=MAX_EPOCH,\n",
    "            warmup_proportion=warmup_proportion,\n",
    "        )\n",
    "        opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "        if cfg.get(\"accum_step\", None) and cfg[\"accum_step\"] > 1:\n",
    "            print(\"[!] using accum_step = {}\".format(cfg[\"accum_step\"]))\n",
    "            from accum_optimizer import AccumOptimizer\n",
    "            opt = AccumOptimizer(opt, steps_per_update=cfg[\"accum_step\"])\n",
    "        \n",
    "        return opt\n",
    "\n",
    "    bert_model = _get_model(cfg[\"base_dir\"], cfg)\n",
    "\n",
    "    if word_embedding_matrix is not None:\n",
    "        embed = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                          output_dim=word_embedding_matrix.shape[1],\n",
    "                          weights=[word_embedding_matrix],\n",
    "                          trainable=cfg[\"trainable\"],\n",
    "                          name=\"char_embed\"\n",
    "                         )\n",
    "    \n",
    "    t1_in = Input(shape=(None, ))\n",
    "    t2_in = Input(shape=(None, ))\n",
    "    o1_in = Input(shape=(1, ))\n",
    "    o2_in = Input(shape=(1, ))\n",
    "\n",
    "    t1, t2, o1, o2 = t1_in, t2_in, o1_in, o2_in\n",
    "    \n",
    "    ## Char information\n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'))(t1)\n",
    "    word_embed = embed(t1)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    word_embed = Bidirectional(LSTM(cfg[\"unit1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    \n",
    "    t = bert_model([t1, t2])\n",
    "    t = Concatenate(axis=-1)([t, word_embed])\n",
    "    t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask]) \n",
    "    t = Bidirectional(LSTM(cfg[\"unit3\"], return_sequences=True), merge_mode=\"concat\")(t)\n",
    "    # t = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([t, mask]) \n",
    "    # t = Conv1D(cfg[\"conv_num\"], kernel_size=3, padding=\"same\")(t) \n",
    "    t = Lambda(lambda x: x[:, 0, :], name=\"extract_layer\")(t)\n",
    "    if cfg.get(\"num_class\", 1) == 2:\n",
    "        po1_logit = Dense(1, name=\"po1_logit\")(t)\n",
    "        po1 = Activation('sigmoid', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])        \n",
    "        o1_loss = K.binary_crossentropy(o1, po1)\n",
    "        loss = K.mean(o1_loss)\n",
    "    else:\n",
    "        po1_logit = Dense(cfg[\"num_class\"], name=\"po1_logit\")(t)\n",
    "        po1 = Activation('softmax', name=\"po1\")(po1_logit)\n",
    "        train_model = Model(inputs=[t1_in, t2_in, o1_in],\n",
    "                            outputs=[po1])\n",
    "        loss = K.categorical_crossentropy(o1, po1, axis=-1)\n",
    "        loss = K.mean(loss)\n",
    "\n",
    "    train_model.add_loss(loss)\n",
    "    opt = _get_opt(num_example=cfg[\"num_example\"], lr=cfg[\"lr\"], min_lr=cfg['min_lr'])\n",
    "    train_model.compile(optimizer=opt)\n",
    "    if summary:\n",
    "        train_model.summary()\n",
    "    return train_model\n",
    "\n",
    "\n",
    "# print(\"----------------build model ---------------\")\n",
    "# model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2id_X(x, x_dict, x2=None, maxlen=None, maxlen1=None):\n",
    "    if x2:\n",
    "        x1 = x\n",
    "        del x\n",
    "        maxlen -= 3\n",
    "        maxlen1 -= 2\n",
    "        assert maxlen > maxlen1\n",
    "        maxlen2 = maxlen - maxlen1 - 1\n",
    "        x1 = [\"[CLS]\"] + list(x1)[: maxlen1] + [\"[SEP]\"] \n",
    "        x1 = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x1]\n",
    "        seg1= [0 for _ in x1]\n",
    "        \n",
    "        x2 = list(x2)[: maxlen2] + [\"[SEP]\"] \n",
    "        x2= [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x2]\n",
    "        seg2 = [1 for _ in x2]\n",
    "        x = x1 + x2\n",
    "        seg = seg1 + seg2\n",
    "        \n",
    "    else:\n",
    "        maxlen -= 2\n",
    "        x = [\"[CLS]\"] + list(x)[: maxlen] + [\"[SEP]\"] \n",
    "        x = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in x]\n",
    "        seg = [0 for _ in x]        \n",
    "    return x, seg\n",
    "\n",
    "\n",
    "def seq_padding(X, maxlen=None, padding_value=None, debug=False):\n",
    "    L = [len(x) for x in X]\n",
    "    if maxlen is None:\n",
    "        maxlen = max(L)\n",
    "\n",
    "    pad_X = np.array([\n",
    "        np.concatenate([x, [padding_value] * (maxlen - len(x))]) if len(x) < maxlen else x for x in X\n",
    "    ])\n",
    "    if debug:\n",
    "        print(\"[!] before pading {}\\n\".format(X))\n",
    "        print(\"[!] after pading {}\\n\".format(pad_X))\n",
    "    return pad_X\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    \n",
    "    def __init__(self, data, batch_size=B_SIZE, shuffle=SHUFFLE, augm_frac=0.75):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = cfg[\"num_example\"] // self.batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_dict = get_data(data)\n",
    "        self.augm_frac = augm_frac\n",
    "        if cfg[\"num_example\"] % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "            T1, T2, O1, O2 = [], [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data.iloc[i]\n",
    "                text = d[\"query1\"]\n",
    "                label_text = d[\"query2\"]\n",
    "                o1 = d[\"label\"]\n",
    "                \n",
    "                if random() > self.augm_frac:\n",
    "                    data_d = self.data_dict[text]\n",
    "                    pos_data = data_d[\"pos\"]\n",
    "                    neg_data = data_d[\"neg\"]\n",
    "                    if pos_data and neg_data:\n",
    "                        if random() > 0.5:\n",
    "                            o1 = 1\n",
    "                            label_text = choice(pos_data)\n",
    "                            if len(pos_data) >= 2:\n",
    "                                _pos_data = [e for e in pos_data if e != label_text]\n",
    "                                text = choice(_pos_data)\n",
    "                        else:\n",
    "                            o1 = 0\n",
    "                            text = choice(pos_data)\n",
    "                            label_text = choice(neg_data)   \n",
    "                \n",
    "                if random() > 0.5:\n",
    "                    text, label_text = label_text, text\n",
    "                \n",
    "                if o1 == \"\":\n",
    "                    continue\n",
    "                o1 = float(o1)\n",
    "                assert 0 <= o1 <= 1\n",
    "                \n",
    "                O1.append(o1)                \n",
    "                t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "                assert len(t1) == len(t2)\n",
    "                \n",
    "                T1.append(t1)\n",
    "                T2.append(t2)\n",
    "\n",
    "                if len(T1) == self.batch_size or i == idxs[-1]:\n",
    "                    O1 = np.array(O1).reshape(-1, 1)\n",
    "                    T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "                    T2 = seq_padding(T2, padding_value=0)\n",
    "                    assert T1.shape == T2.shape and T1.shape[0] == O1.shape[0]\n",
    "\n",
    "                    yield [T1, T2, O1], None\n",
    "                    T1, T2, O1, = [], [], []\n",
    "                    \n",
    "                        \n",
    "# gen = data_generator(train_data)\n",
    "# for i, e in enumerate(gen):\n",
    "#     if i > 400:\n",
    "#         break\n",
    "#     print(\"i = {}\".format(i), \"-\" * 81)\n",
    "#     # print(e[0])\n",
    "#     for _e in e[0]:\n",
    "#         print(_e.shape, _e.sum(axis=0).sum(axis=0))\n",
    "# del gen\n",
    "# print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_):\n",
    "    model_inp_ind = [0, 1]\n",
    "    inputs = [model_.inputs[e] for e in model_inp_ind]\n",
    "    sub_model = Model(inputs=inputs, outputs=[model_.get_layer(\"po1\").output])\n",
    "    return sub_model\n",
    "\n",
    "\n",
    "def find_best_acc_score(y_pred, y_true):\n",
    "    thres = [i / 100 for i in range(1, 100)]\n",
    "    scores = [accuracy_score(y_pred, np.array(preds > thre, \"int32\")) for thre in thres]\n",
    "    ind = np.argmax(scores)\n",
    "    max_score = np.max(scores)\n",
    "    assert abs(scores[ind] - scores) < 1e-15\n",
    "    return max_score, thres[ind]\n",
    "            \n",
    "\n",
    "def evaluate(sub_model, data, bs=32):\n",
    "    idxs = list(range(len(data)))\n",
    "    T1, T2, O1, O2 = [], [], [], []\n",
    "    preds = []\n",
    "    for i in idxs:\n",
    "        d = data.iloc[i]\n",
    "        text = d[\"query1\"]\n",
    "        label_text = d[\"query2\"]\n",
    "\n",
    "        t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "        assert len(t1) == len(t2)\n",
    "\n",
    "        T1.append(t1)\n",
    "        T2.append(t2)\n",
    "\n",
    "        o1 = float(d[\"label\"])\n",
    "        O1.append(o1)\n",
    "        if len(T1) == bs or i == idxs[-1]:\n",
    "            T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "            T2 = seq_padding(T2, padding_value=0)\n",
    "            assert T1.shape == T2.shape\n",
    "            pred = sub_model.predict([T1, T2])\n",
    "            preds.append(pred)\n",
    "            T1, T2 = [], []\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    O1 = np.array(O1).reshape(-1)\n",
    "    O1 = O1.astype(\"int32\")\n",
    "    auc = roc_auc_score(O1, preds)\n",
    "    print(\"[!] best accurary&threshold = {}\",  find_best_acc_score(preds, O1))\n",
    "    print(\"[!] np.mean(preds) = {}\".format(np.mean(preds)))\n",
    "    print(\"[!] classification_report\")\n",
    "    print(classification_report(O1,  np.array(preds > 0.5, \"int32\"), digits=6))\n",
    "    acc = accuracy_score(O1, np.array(preds > 0.5, \"int32\"))\n",
    "    return auc, acc\n",
    "    \n",
    "\n",
    "class Evaluate(Callback):\n",
    "    def __init__(self, data, filename=None):\n",
    "        self.F1 = []\n",
    "        self.best = 0.\n",
    "        self.filename = filename\n",
    "        self.data = data\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch ==  0:\n",
    "            print(\"[!] test load&save model\")\n",
    "            f = self.filename + \".h5\"\n",
    "            self.model.save(f, include_optimizer=False, overwrite=False)\n",
    "            if \"albert\" in cfg[\"verbose\"]:\n",
    "                model_ = load_model(f) \n",
    "            else:\n",
    "                model_ = load_model(f, custom_objects=get_custom_objects()) \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch + 1 < 1:\n",
    "            return\n",
    "        if epoch + 1 in [3, 6, 9, 10, 12, 15, 18, 20]:\n",
    "            self.model.save(self.filename + \"_{}.h5\".format(epoch + 1), include_optimizer=False)\n",
    "            \n",
    "        sub_model = get_model(self.model)\n",
    "        f1, class_f1 = evaluate(sub_model, data=self.data)\n",
    "        self.F1.append(f1)\n",
    "        if f1 > self.best:\n",
    "            self.model.save(self.filename + \".h5\", include_optimizer=False)\n",
    "            \n",
    "        if f1 > self.best:\n",
    "            self.best = f1\n",
    "            print(\"[!] epoch = {}, new best_auc = {}\".format(epoch + 1,  f1))\n",
    "        print('[!] epoch = {}, auc = {}, best auc {}'.format(epoch + 1, f1, self.best))\n",
    "        print('[!] epoch = {}, acc = {}\\n'.format(epoch + 1, class_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_layer(inputs, name, exclude_from=None):\n",
    "    \"\"\"根据inputs和name来搜索层\n",
    "    说明：inputs为某个层或某个层的输出；name为目标层的名字。\n",
    "    实现：根据inputs一直往上递归搜索，直到发现名字为name的层为止；\n",
    "         如果找不到，那就返回None。\n",
    "    \"\"\"\n",
    "    if exclude_from is None:\n",
    "        exclude_from = set()\n",
    "\n",
    "    if isinstance(inputs, keras.layers.Layer):\n",
    "        layer = inputs\n",
    "    else:\n",
    "        layer = inputs._keras_history[0]\n",
    "\n",
    "    if layer.name == name:\n",
    "        return layer\n",
    "    elif layer in exclude_from:\n",
    "        return None\n",
    "    else:\n",
    "        exclude_from.add(layer)\n",
    "        if isinstance(layer, keras.models.Model):\n",
    "            model = layer\n",
    "            for layer in model.layers:\n",
    "                if layer.name == name:\n",
    "                    return layer\n",
    "        inbound_layers = layer._inbound_nodes[0].inbound_layers\n",
    "        if not isinstance(inbound_layers, list):\n",
    "            inbound_layers = [inbound_layers]\n",
    "        if len(inbound_layers) > 0:\n",
    "            for layer in inbound_layers:\n",
    "                layer = search_layer(layer, name, exclude_from)\n",
    "                if layer is not None:\n",
    "                    return layer\n",
    "                \n",
    "def adversarial_training(model, embedding_names, epsilon=1):\n",
    "    \"\"\"给模型添加对抗训练\n",
    "    其中model是需要添加对抗训练的keras模型，embedding_names\n",
    "    则是model里边Embedding层的名字。要在模型compile之后使用。\n",
    "    \"\"\"\n",
    "    if model.train_function is None:  # 如果还没有训练函数\n",
    "        model._make_train_function()  # 手动make\n",
    "    old_train_function = model.train_function  # 备份旧的训练函数\n",
    "\n",
    "    # 查找Embedding层\n",
    "    embedding_layers = []\n",
    "    for embedding_name in embedding_names:\n",
    "        for output in model.outputs:\n",
    "            embedding_layer = search_layer(output, embedding_name)\n",
    "            if embedding_layer is not None:\n",
    "                embedding_layers.append(embedding_layer)\n",
    "                break\n",
    "    for embedding_layer in embedding_layers:\n",
    "        if embedding_layer is None:\n",
    "            raise Exception('Embedding layer not found')\n",
    "\n",
    "    # 求Embedding梯度\n",
    "    embeddings = [embedding_layer.embeddings for embedding_layer in embedding_layers] # Embedding矩阵\n",
    "    gradients = K.gradients(model.total_loss, embeddings)  # Embedding梯度\n",
    "    # gradients = K.zeros_like(embeddings) + gradients[0]  # 转为dense tensor\n",
    "    gradients = [K.zeros_like(embedding) + gradient for embedding, gradient in zip(embeddings, gradients)]\n",
    "\n",
    "    # 封装为函数\n",
    "    inputs = (model._feed_inputs +\n",
    "              model._feed_targets +\n",
    "              model._feed_sample_weights)  # 所有输入层\n",
    "    embedding_gradients = K.function(\n",
    "        inputs=inputs,\n",
    "        outputs=gradients,\n",
    "        name='embedding_gradients',\n",
    "    )  # 封装为函数\n",
    "\n",
    "    def train_function(inputs):  # 重新定义训练函数\n",
    "#         grads = embedding_gradients(inputs)[0]  # Embedding梯度\n",
    "#         delta = epsilon * grads / (np.sqrt((grads**2).sum()) + 1e-8)  # 计算扰动\n",
    "        grads = embedding_gradients(inputs)  # Embedding梯度\n",
    "        deltas = [epsilon * grad / (np.sqrt((grad**2).sum()) + 1e-8) for grad in grads]  # 计算扰动\n",
    "        # 注入扰动\n",
    "        # K.set_value(embeddings, K.eval(embeddings) + delta)  \n",
    "        for embedding, delta in zip(embeddings, deltas):\n",
    "            K.set_value(embedding, K.eval(embedding) + delta)\n",
    "            \n",
    "        outputs = old_train_function(inputs)  # 梯度下降\n",
    "        # 删除扰动\n",
    "        # K.set_value(embeddings, K.eval(embeddings) - delta)  # 删除扰动\n",
    "        for embedding, delta in zip(embeddings, deltas):\n",
    "            K.set_value(embedding, K.eval(embedding) - delta)       \n",
    "        return outputs\n",
    "\n",
    "    model.train_function = train_function  # 覆盖原训练函数\n",
    "\n",
    "\n",
    "# 写好函数后，启用对抗训练只需要一行代码\n",
    "# adversarial_training(model, 'Embedding-Token', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "[!] start fold_id = -1 (8747, 5) (2002, 5)\n",
      "{'verbose': 'roberta_large_v2_augm_seed2020', 'base_dir': '../../../chinese_bert/RoBERTa-large-pair/', 'span_mode': True, 'lr': 3e-06, 'min_lr': 6e-08, 'ch_type': 'tx_ft', 'trainable': True, 'bert_trainable': True, 'accum_step': 1, 'cls_num': 4, 'unit1': 128, 'unit2': 128, 'unit3': 512, 'conv_num': 128, 'maxlen': 60, 'adv_training': False, 'x_pad': 0, 'num_class': 2, 'filename': 'roberta_large_v2_augm_seed2020_tx_ft_20_3e-06', 'num_example': 8747}\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embed (Embedding)          (None, None, 500)    10530500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 500)    0           char_embed[0][0]                 \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 128)    645120      lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             324009984   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 128)    0           bidirectional_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 4224)   0           model_2[1][0]                    \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 4224)   0           concatenate_1[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 1024)   19406848    lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "extract_layer (Lambda)          (None, 1024)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "po1_logit (Dense)               (None, 1)            1025        extract_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "po1 (Activation)                (None, 1)            0           po1_logit[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 354,593,477\n",
      "Trainable params: 354,593,477\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/weiqiang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/20\n",
      "[!] test load&save model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiqiang/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/274 [==============================] - 274s 1s/step - loss: 0.4202\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'O1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-efa56d8eaaf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                               )\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f54311635f8c>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0msub_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f54311635f8c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sub_model, data, bs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mO1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mO1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[!] best accurary&threshold = {}\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfind_best_acc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[!] np.mean(preds) = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[!] classification_report\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f54311635f8c>\u001b[0m in \u001b[0;36mfind_best_acc_score\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_best_acc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mthres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthre\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f54311635f8c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_best_acc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mthres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthre\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'O1' is not defined"
     ]
    }
   ],
   "source": [
    "adv_layer_names = ['Embedding-Token', 'char_embed']\n",
    "\n",
    "if -1 in FOLD_ID:\n",
    "    fold_id = -1\n",
    "    cfg[\"num_example\"] = len(train_data)\n",
    "    print(\"-\" * 81)\n",
    "    print(\"[!] start fold_id =\", fold_id, train_data.shape, dev_data.shape)\n",
    "    print(cfg)\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    train_D = data_generator(train_data)\n",
    "    seed(SEED + fold_id)\n",
    "    np.random.seed(SEED + fold_id)\n",
    "    tf.random.set_random_seed(SEED + fold_id)\n",
    "    model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "    if cfg[\"adv_training\"]:\n",
    "        print(\"[!] using adv_training\")\n",
    "        adversarial_training(model, adv_layer_names, 0.5)\n",
    "    evaluator = Evaluate(filename=cfg[\"filename\"] + \"_fold{}\".format(fold_id), data=dev_data)\n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                              steps_per_epoch=len(train_D),\n",
    "                              epochs=MAX_EPOCH,\n",
    "                              callbacks=[evaluator],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    del model, train_data, dev_data\n",
    "    gc.collect()\n",
    "    print(\"[!] finish fold_id =\", fold_id)\n",
    "    print(\"-\" * 81)\n",
    "\n",
    "\n",
    "skf = SKF(FOLD_NUM, shuffle=True, random_state=SEED)\n",
    "print(all_data.shape)\n",
    "for fold_id, (trn_ind, val_ind) in enumerate(skf.split(range(len(all_data)), all_data[\"label\"])):\n",
    "    if fold_id not in FOLD_ID:\n",
    "        continue\n",
    "    \n",
    "    dev_data = all_data.iloc[val_ind].reset_index(drop=True)\n",
    "    train_data = all_data.iloc[trn_ind].reset_index(drop=True)\n",
    "    cfg[\"num_example\"] = len(train_data)\n",
    "    print(\"-\" * 81)\n",
    "    print(\"[!] start fold_id =\", fold_id, train_data.shape, dev_data.shape)\n",
    "    print(cfg)\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    train_D = data_generator(train_data)\n",
    "    seed(SEED + fold_id)\n",
    "    np.random.seed(SEED + fold_id)\n",
    "    tf.random.set_random_seed(SEED + fold_id)\n",
    "    model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "    if cfg[\"adv_training\"]:\n",
    "        print(\"[!] using adv_training\")\n",
    "        adversarial_training(model, adv_layer_names, 0.5)\n",
    "    evaluator = Evaluate(filename=cfg[\"filename\"] + \"_fold{}\".format(fold_id), data=dev_data)\n",
    "    model.fit_generator(train_D.__iter__(),\n",
    "                              steps_per_epoch=len(train_D),\n",
    "                              epochs=MAX_EPOCH,\n",
    "                              callbacks=[evaluator],\n",
    "                              shuffle=True\n",
    "                              )\n",
    "    del model, train_data, dev_data\n",
    "    gc.collect()\n",
    "    print(\"[!] finish fold_id =\", fold_id)\n",
    "    print(\"-\" * 81)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.F1, max(evaluator.F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(sub_model, data, bs=32):\n",
    "    idxs = list(range(len(data)))\n",
    "    T1, T2, O1, O2 = [], [], [], []\n",
    "    preds = []\n",
    "    for i in idxs:\n",
    "        d = data.iloc[i]\n",
    "        text = d[\"query1\"]\n",
    "        label_text = d[\"query2\"]\n",
    "\n",
    "        t1, t2 = token2id_X(text, x2=label_text, x_dict=word_index, maxlen=MAX_LEN, maxlen1=MAX_DOC_LEN)\n",
    "        assert len(t1) == len(t2)\n",
    "\n",
    "        T1.append(t1)\n",
    "        T2.append(t2)\n",
    "\n",
    "        if len(T1) == bs or i == idxs[-1]:\n",
    "            T1 = seq_padding(T1, padding_value=cfg[\"x_pad\"])\n",
    "            T2 = seq_padding(T2, padding_value=0)\n",
    "            assert T1.shape == T2.shape\n",
    "            pred = sub_model.predict([T1, T2])\n",
    "            preds.append(pred)\n",
    "            T1, T2 = [], []\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def ensemble_predictions(predictions, weights=None, type_=\"linear\"):\n",
    "    if not weights:\n",
    "        # print(\"[!] AVE_WGT\")\n",
    "        weights = [1./ len(predictions) for _ in range(len(predictions))]\n",
    "    assert len(predictions) == len(weights)\n",
    "    if np.sum(weights) != 1.0:\n",
    "        weights = [w / np.sum(weights) for w in weights]\n",
    "    # print(\"[!] weights = {}\".format(weights))\n",
    "    assert np.isclose(np.sum(weights), 1.0)\n",
    "    if type_ == \"linear\":\n",
    "        res = np.average(predictions, weights=weights, axis=0)\n",
    "    elif type_ == \"harmonic\":\n",
    "        res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n",
    "        return 1 / res\n",
    "    elif type_ == \"geometric\":\n",
    "        numerator = np.average(\n",
    "            [np.log(p) for p in predictions], weights=weights, axis=0\n",
    "        )\n",
    "        res = np.exp(numerator / sum(weights))\n",
    "        return res\n",
    "    elif type_ == \"rank\":\n",
    "        from scipy.stats import rankdata\n",
    "        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n",
    "        return res / (len(res) + 1)\n",
    "    return res\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "\n",
    "model_files =[\n",
    "            \"albert_xxlarge_tx_ft_5fold-1_2e-05.h5\",\n",
    "            \"albert_xxlarge_tx_ft_5fold-1_2e-05_6.h5\",\n",
    "            \"roberta_base_tx_ft_5fold-1_2e-05.h5\", \n",
    "            \"roberta_base_tx_ft_5fold-1_2e-05_10.h5\", \n",
    "            \"roberta_large_tx_ft_5fold-1_2e-05.h5\",\n",
    "            \"roberta_large_tx_ft_5fold-1_2e-05_10.h5\",\n",
    "            \"UER_large_tx_ft_5fold-1_6e-06.h5\",\n",
    "            \"UER_large_tx_ft_5fold-1_6e-06_10.h5\",\n",
    "            \"roberta_large_v2_tx_ft_5fold-1_2e-05.h5\",    \n",
    "            \"roberta_large_v2_tx_ft_5fold-1_2e-05_10.h5\", \n",
    "             ]\n",
    "\n",
    "for f in model_files:\n",
    "    print(f, os.path.exists(f))\n",
    "assert len(model_files) == len(set(model_files)) \n",
    "assert all([os.path.exists(f) for f in model_files]) \n",
    "preds = []\n",
    "O1 = dev_data[\"label\"].values.reshape(-1)\n",
    "for f in model_files:\n",
    "    print(\"-\" * 80)\n",
    "    K.clear_session()\n",
    "    t0 = time()\n",
    "    print(\"[!]\", f)\n",
    "    if \"albert\" in f:\n",
    "        model = load_model(f)\n",
    "    else:\n",
    "        model = load_model(f, custom_objects=get_custom_objects())\n",
    "    sub_model = get_model(model)\n",
    "    pred = test(sub_model, dev_data)\n",
    "    auc = roc_auc_score(O1, pred)\n",
    "    acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "    print(\"[{}]\".format(time() - t0), auc, acc)\n",
    "    print(\"-\" * 80)\n",
    "    preds.append(pred)\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "pred = ensemble_predictions(preds)\n",
    "print(pred.shape)\n",
    "auc = roc_auc_score(O1, pred)\n",
    "acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "print(auc, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred1 = ensemble_predictions(preds[0::2])\n",
    "pred2 = ensemble_predictions(preds[1::2])\n",
    "print(pred2[: 3], pred1[: 3])\n",
    "for i in range(1, 100):\n",
    "    wgt = i / 100\n",
    "    pred = ensemble_predictions([pred1, pred2], weights=[wgt, 1 - wgt], type_=\"geometric\")\n",
    "    auc = roc_auc_score(O1, pred)\n",
    "    acc = accuracy_score(O1, np.array(pred > 0.5, \"int32\"))    \n",
    "    print(wgt, auc, acc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "def get_cnt(data, col1, col2, cate):\n",
    "    data = data[data[\"category\"] == cate]\n",
    "    if col2:\n",
    "        data = data[col1].tolist() + data[col2].tolist() \n",
    "    else:\n",
    "        data = data[col1].tolist()\n",
    "    data = [list(jieba.cut(e)) for e in data]\n",
    "    cnt1 = Counter([w for sent in data for w in sent])\n",
    "    return cnt1\n",
    "    \n",
    "    \n",
    "stop_words = ['？', '吗', '了', '，', '的', '?', '有', '得', '地', '是', '什么',\n",
    "              '怎么办', '哪些', '怎么回事', '怎么', '要', '能', '呢', '会']\n",
    "for cate in train_data[\"category\"].value_counts().index:\n",
    "    print(\"-\" * 40, cate, \"-\" * 40)\n",
    "    cnt1 = get_cnt(train_data, col1=\"query1\", col2=\"query2\", cate=cate)\n",
    "    cnt1 = [(k, cnt) for k, cnt in cnt1.most_common() if k not in stop_words]\n",
    "    print(cnt1[: 20])\n",
    "    print(\"-\" * 81)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in dev_data[\"category\"].value_counts().index:\n",
    "    print(\"------------------------\")\n",
    "    print(cate)\n",
    "    cnt1 = get_cnt(dev_data, col=\"query1\", cate=cate)\n",
    "    cnt2 = get_cnt(dev_data, col=\"query2\", cate=cate)\n",
    "    print(cnt1.most_common(10))\n",
    "    print(cnt2.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _foo(x, col):\n",
    "    cate = x[\"category\"]\n",
    "    q = x[col]\n",
    "    if cate == \"咳血\":\n",
    "        return q.count(\"咯血\") + q.count(\"咳血\")\n",
    "    \n",
    "    \n",
    "    return q.count(cate)\n",
    "\n",
    "\n",
    "    \n",
    "train_data[\"q1_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query1\"), axis=1)\n",
    "train_data[\"q2_cnt_cate\"] = train_data.apply(lambda x: _foo(x, \"query2\"), axis=1)\n",
    "train_data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2000 * 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
